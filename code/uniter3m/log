[1,0]<stdout>:[Debug] number works 1
[1,0]<stdout>:[Debug] Loading Image db feat_th1.0_max64_min10
[1,0]<stdout>:[Debug] Speech db is not None!!!
[1,0]<stdout>:[Debug] Loading Image db feat_th1.0_max64_min10
[1,0]<stdout>:[Debug] Speech db is not None!!!
[1,0]<stdout>:[Debug] Loading Image db feat_th1.0_max64_min10
[1,0]<stdout>:[Debug] Speech db is not None!!!
[1,0]<stdout>:[Debug] Loading Image db feat_th1.0_max64_min10
[1,0]<stdout>:[Debug] Speech db is not None!!!
[1,0]<stdout>:[Debug] Loading Image db feat_th1.0_max64_min10
[1,0]<stdout>:[Debug] Speech db is not None!!!
[1,0]<stdout>:[Debug] Loading Image db feat_th1.0_max64_min10
[1,0]<stdout>:[Debug] Speech db is not None!!!
[1,0]<stdout>:[Debug] Loading Image db feat_th1.0_max64_min10
[1,0]<stdout>:[Debug] Speech db is not None!!!
[1,0]<stdout>:[Debug] Loading Image db feat_th1.0_max64_min10
[1,0]<stdout>:[Debug] Speech db is not None!!!
[1,0]<stdout>:[Debug] Loading Image db feat_th1.0_max64_min10
[1,0]<stdout>:[Debug] Speech db is not None!!!
[1,0]<stdout>:[Debug] item 34179 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([8])
[1,0]<stdout>:[Debug] item img 34179 is None
[1,0]<stdout>:[Debug] item 34179 speech is not None
[1,0]<stdout>:[Debug] item 34179 speech attn mask torch.Size([31]) and final attn mask torch.Size([39])
[1,0]<stdout>:[Debug] item 48737 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([10])
[1,0]<stdout>:[Debug] item img 48737 is None
[1,0]<stdout>:[Debug] item 48737 speech is not None
[1,0]<stdout>:[Debug] item 48737 speech attn mask torch.Size([29]) and final attn mask torch.Size([39])
[1,0]<stdout>:[Debug] batch indexs [34179, 48737]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 39])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[  101,  2065,  2017, 13743,  2288,  2068,  1010,   102,     0,     0],
[1,0]<stdout>:        [  101,   103,   103,  2009,  1005,   103,   103,  2100,  1012,   102]]) torch.Size([2, 10])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 31, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
[1,0]<stdout>:         20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,
[1,0]<stdout>:         38, 39, 40],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38]]) torch.Size([2, 39])
[1,0]<stdout>:[Debug] item 22681 text attn mask tensor([1, 1, 1, 1, 1, 1, 1]) torch.Size([7])
[1,0]<stdout>:[Debug] item img 22681 is None
[1,0]<stdout>:[Debug] item 22681 speech is not None
[1,0]<stdout>:[Debug] item 22681 speech attn mask torch.Size([55]) and final attn mask torch.Size([62])
[1,0]<stdout>:[Debug] item 5147 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([14])
[1,0]<stdout>:[Debug] item img 5147 is None
[1,0]<stdout>:[Debug] item 5147 speech is not None
[1,0]<stdout>:[Debug] item 5147 speech attn mask torch.Size([48]) and final attn mask torch.Size([62])
[1,0]<stdout>:[Debug] batch indexs [22681, 5147]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 62])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[ 101, 2026, 2132,  103, 5742, 1012,  102,    0,    0,    0,    0,    0,
[1,0]<stdout>:            0,    0],
[1,0]<stdout>:        [ 101, 2073, 1005, 1055, 1996, 3336, 2183, 1029, 2175, 2007, 1996, 3336,
[1,0]<stdout>:          103,  102]]) torch.Size([2, 14])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 55, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,
[1,0]<stdout>:         25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,
[1,0]<stdout>:         43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,
[1,0]<stdout>:         61, 62, 63, 64, 65, 66, 67, 68],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
[1,0]<stdout>:         54, 55, 56, 57, 58, 59, 60, 61]]) torch.Size([2, 62])
[1,0]<stdout>:[Debug] item 25005 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([13])
[1,0]<stdout>:[Debug] item img 25005 is None
[1,0]<stdout>:[Debug] item 25005 speech is not None
[1,0]<stdout>:[Debug] item 25005 speech attn mask torch.Size([64]) and final attn mask torch.Size([77])
[1,0]<stdout>:[Debug] item 11030 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([13])
[1,0]<stdout>:[Debug] item img 11030 is None
[1,0]<stdout>:[Debug] item 11030 speech is not None
[1,0]<stdout>:[Debug] item 11030 speech attn mask torch.Size([64]) and final attn mask torch.Size([77])
[1,0]<stdout>:[Debug] batch indexs [25005, 11030]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1]]) torch.Size([2, 77])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[  101,   103,  1005,  2222,  2156,  2115,  3156,  1998,  5333,  2017,
[1,0]<stdout>:          2456,  1012,   102],
[1,0]<stdout>:        [  101,  3398,  2002,  1005,  1055, 30077,  4845,  1012,  4268,  2024,
[1,0]<stdout>:          5236,  1012,   102]]) torch.Size([2, 13])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 64, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
[1,0]<stdout>:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
[1,0]<stdout>:         72, 73, 74, 75, 76],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
[1,0]<stdout>:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
[1,0]<stdout>:         72, 73, 74, 75, 76]]) torch.Size([2, 77])
[1,0]<stdout>:[Debug] item 787 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([8])
[1,0]<stdout>:[Debug] item img 787 is None
[1,0]<stdout>:[Debug] item 787 speech is not None
[1,0]<stdout>:[Debug] item 787 speech attn mask torch.Size([26]) and final attn mask torch.Size([34])
[1,0]<stdout>:[Debug] item 16837 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([8])
[1,0]<stdout>:[Debug] item img 16837 is None
[1,0]<stdout>:[Debug] item 16837 speech is not None
[1,0]<stdout>:[Debug] item 16837 speech attn mask torch.Size([26]) and final attn mask torch.Size([34])
[1,0]<stdout>:[Debug] batch indexs [787, 16837]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 34])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[  101,   103,  2017,  2101,  1012,  3100,  1012,   102],
[1,0]<stdout>:        [  101,   103,  1996,  6335, 20469, 12818,  1012,   102]]) torch.Size([2, 8])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 26, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]]) torch.Size([2, 34])
[1,0]<stdout>:[Debug] item 60516 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([8])
[1,0]<stdout>:[Debug] item img 60516 is None
[1,0]<stdout>:[Debug] item 60516 speech is not None
[1,0]<stdout>:[Debug] item 60516 speech attn mask torch.Size([28]) and final attn mask torch.Size([36])
[1,0]<stdout>:[Debug] item 20826 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([8])
[1,0]<stdout>:[Debug] item img 20826 is None
[1,0]<stdout>:[Debug] item 20826 speech is not None
[1,0]<stdout>:[Debug] item 20826 speech attn mask torch.Size([28]) and final attn mask torch.Size([36])
[1,0]<stdout>:[Debug] batch indexs [60516, 20826]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 36])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[  101,   103,  2342,  2000,  2203,  2023,   103,   102],
[1,0]<stdout>:        [  101,  3100,   103,  2122, 19712,  2031,  2242,   102]]) torch.Size([2, 8])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 28, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]]) torch.Size([2, 36])
[1,0]<stdout>:[Debug] item 73365 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([15])
[1,0]<stdout>:[Debug] item img 73365 is None
[1,0]<stdout>:[Debug] item 73365 speech is not None
[1,0]<stdout>:[Debug] item 73365 speech attn mask torch.Size([50]) and final attn mask torch.Size([65])
[1,0]<stdout>:[Debug] item 70715 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([14])
[1,0]<stdout>:[Debug] item img 70715 is None
[1,0]<stdout>:[Debug] item 70715 speech is not None
[1,0]<stdout>:[Debug] item 70715 speech attn mask torch.Size([51]) and final attn mask torch.Size([65])
[1,0]<stdout>:[Debug] batch indexs [73365, 70715]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 65])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[  101,  2089,  1010,  1045,  1005,  1049,  3374,  1012,  2026,   103,
[1,0]<stdout>:          1005,  1055, 21305,  1012,   102],
[1,0]<stdout>:        [  101,  1045,  2113,  1037,   103,  1997,  2111,  2040,  2052,  5993,
[1,0]<stdout>:          2007,  2017,  1012,   102,     0]]) torch.Size([2, 15])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 51, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
[1,0]<stdout>:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 15, 16, 17, 18,
[1,0]<stdout>:         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,
[1,0]<stdout>:         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,
[1,0]<stdout>:         55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65]]) torch.Size([2, 65])
[1,0]<stdout>:[Warning] Donot use emo type embeddings add to input
[1,0]<stdout>:Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.
[1,0]<stdout>:
[1,0]<stdout>:Defaults for this optimization level are:
[1,0]<stdout>:enabled                : True
[1,0]<stdout>:opt_level              : O2
[1,0]<stdout>:cast_model_type        : torch.float16
[1,0]<stdout>:patch_torch_functions  : False
[1,0]<stdout>:keep_batchnorm_fp32    : True
[1,0]<stdout>:master_weights         : True
[1,0]<stdout>:loss_scale             : dynamic
[1,0]<stdout>:Processing user overrides (additional kwargs that are not None)...
[1,0]<stdout>:After processing overrides, optimization options are:
[1,0]<stdout>:enabled                : True
[1,0]<stdout>:opt_level              : O2
[1,0]<stdout>:cast_model_type        : torch.float16
[1,0]<stdout>:patch_torch_functions  : False
[1,0]<stdout>:keep_batchnorm_fp32    : True
[1,0]<stdout>:master_weights         : True
[1,0]<stdout>:loss_scale             : dynamic
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([4, 81, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [78, 78, 78,  ..., 78, 78, 78],
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79],
[1,0]<stdout>:         [80, 80, 80,  ..., 80, 80, 80]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [78, 78, 78,  ..., 78, 78, 78],
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79],
[1,0]<stdout>:         [80, 80, 80,  ..., 80, 80, 80]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [78, 78, 78,  ..., 78, 78, 78],
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79],
[1,0]<stdout>:         [80, 80, 80,  ..., 80, 80, 80]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [78, 78, 78,  ..., 78, 78, 78],
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79],
[1,0]<stdout>:         [80, 80, 80,  ..., 80, 80, 80]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([4, 81, 768])
[1,0]<stdout>:emobert2220:6471:6506 [0] NCCL INFO NET : Using interface eth0:172.17.0.3<0>
[1,0]<stdout>:emobert2220:6471:6506 [0] NCCL INFO NET/IB : Using interface eth0 for sideband communication
[1,0]<stdout>:emobert2220:6471:6506 [0] NCCL INFO Using internal Network Socket
[1,0]<stdout>:emobert2220:6471:6506 [0] NCCL INFO NET : Using interface eth0:172.17.0.3<0>
[1,0]<stdout>:emobert2220:6471:6506 [0] NCCL INFO NET/Socket : 1 interfaces found
[1,0]<stdout>:NCCL version 2.3.7+cuda10.0
[1,0]<stdout>:emobert2220:6471:6506 [0] NCCL INFO rank 0 nranks 1
[1,0]<stdout>:emobert2220:6471:6506 [0] NCCL INFO comm 0x7f7d143399a0 rank 0 nranks 1
[1,0]<stdout>:emobert2220:6471:6506 [0] NCCL INFO Could not find real path of /sys/class/net/eth0/device
[1,0]<stdout>:emobert2220:6471:6506 [0] NCCL INFO CUDA Dev 0, IP Interfaces : eth0(SOC) 
[1,0]<stdout>:emobert2220:6471:6506 [0] NCCL INFO Using 256 threads
[1,0]<stdout>:emobert2220:6471:6506 [0] NCCL INFO Min Comp Cap 6
[1,0]<stdout>:emobert2220:6471:6506 [0] NCCL INFO comm 0x7f7d143399a0 rank 0 nranks 1 - COMPLETE
[1,0]<stdout>:Warning: NaN or Inf found in input tensor.
[1,0]<stdout>:Gradient overflow.  Skipping step, loss scaler 4 reducing loss scale to 32768.0
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:[Debug] item 32063 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([10])
[1,0]<stdout>:[Debug] item img 32063 is None
[1,0]<stdout>:[Debug] item 32063 speech is not None
[1,0]<stdout>:before gather embedding input torch.Size([4, 76, 768])
[1,0]<stdout>:[Debug] item 32063 speech attn mask torch.Size([29]) and final attn mask torch.Size([39])
[1,0]<stdout>:[Debug] item 29566 text attn mask tensor([1, 1, 1, 1, 1, 1, 1]) torch.Size([7])
[1,0]<stdout>:[Debug] item img 29566 is None
[1,0]<stdout>:[Debug] item 29566 speech is not None
[1,0]<stdout>:[Debug] item 29566 speech attn mask torch.Size([32]) and final attn mask torch.Size([39])
[1,0]<stdout>:[Debug] batch indexs [32063, 29566]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 39])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[  101,   103,  1005,  1055,  3308,  2007,  2115,  2767,  1029,   102],
[1,0]<stdout>:        [  101,   103,  7396, 14382,  2023,  1012,   102,     0,     0,     0]]) torch.Size([2, 10])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 32, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,
[1,0]<stdout>:         21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
[1,0]<stdout>:         39, 40, 41]]) torch.Size([2, 39])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [73, 73, 73,  ..., 73, 73, 73],
[1,0]<stdout>:         [74, 74, 74,  ..., 74, 74, 74],
[1,0]<stdout>:         [75, 75, 75,  ..., 75, 75, 75]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [72, 72, 72,  ..., 72, 72, 72],
[1,0]<stdout>:         [73, 73, 73,  ..., 73, 73, 73],
[1,0]<stdout>:         [74, 74, 74,  ..., 74, 74, 74]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [73, 73, 73,  ..., 73, 73, 73],
[1,0]<stdout>:         [74, 74, 74,  ..., 74, 74, 74],
[1,0]<stdout>:         [75, 75, 75,  ..., 75, 75, 75]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [72, 72, 72,  ..., 72, 72, 72],
[1,0]<stdout>:         [73, 73, 73,  ..., 73, 73, 73],
[1,0]<stdout>:         [74, 74, 74,  ..., 74, 74, 74]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([4, 75, 768])
[1,0]<stdout>:[Debug] item 25138 text attn mask tensor([1, 1, 1, 1, 1, 1, 1]) torch.Size([7])
[1,0]<stdout>:[Debug] item img 25138 is None
[1,0]<stdout>:[Debug] item 25138 speech is not None
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([2, 36, 768])
[1,0]<stdout>:[Debug] item 25138 speech attn mask torch.Size([22]) and final attn mask torch.Size([29])
[1,0]<stdout>:[Debug] item 6567 text attn mask tensor([1, 1, 1, 1, 1, 1, 1]) torch.Size([7])
[1,0]<stdout>:[Debug] item img 6567 is None
[1,0]<stdout>:[Debug] item 6567 speech is not None
[1,0]<stdout>:[Debug] item 6567 speech attn mask torch.Size([22]) and final attn mask torch.Size([29])
[1,0]<stdout>:[Debug] batch indexs [25138, 6567]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1]]) torch.Size([2, 29])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[ 101,  103, 2359, 1012, 1012, 1012,  102],
[1,0]<stdout>:        [ 101,  103,  103, 1996, 4292, 1012,  102]]) torch.Size([2, 7])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 22, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]]) torch.Size([2, 29])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [33, 33, 33,  ..., 33, 33, 33],
[1,0]<stdout>:         [34, 34, 34,  ..., 34, 34, 34],
[1,0]<stdout>:         [35, 35, 35,  ..., 35, 35, 35]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [33, 33, 33,  ..., 33, 33, 33],
[1,0]<stdout>:         [34, 34, 34,  ..., 34, 34, 34],
[1,0]<stdout>:         [35, 35, 35,  ..., 35, 35, 35]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([2, 36, 768])
[1,0]<stdout>:Warning: NaN or Inf found in input tensor.
[1,0]<stdout>:Gradient overflow.  Skipping step, loss scaler 5 reducing loss scale to 32768.0
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:[Debug] item 11658 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([16])
[1,0]<stdout>:[Debug] item img 11658 is None
[1,0]<stdout>:[Debug] item 11658 speech is not None
[1,0]<stdout>:before gather embedding input torch.Size([2, 66, 768])
[1,0]<stdout>:[Debug] item 11658 speech attn mask torch.Size([59]) and final attn mask torch.Size([75])
[1,0]<stdout>:[Debug] item 11586 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([11])
[1,0]<stdout>:[Debug] item img 11586 is None
[1,0]<stdout>:[Debug] item 11586 speech is not None
[1,0]<stdout>:[Debug] item 11586 speech attn mask torch.Size([64]) and final attn mask torch.Size([75])
[1,0]<stdout>:[Debug] batch indexs [11658, 11586]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1]]) torch.Size([2, 75])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[  101,  2085,  8595,  1999,   103,  2010,   103,  1012,  1045,  2215,
[1,0]<stdout>:          2000,  2156,  2010, 11136,   103,   102],
[1,0]<stdout>:        [  101,   103,  1045,  2245,  2055,  2673,  1045,  1005,  1040, 20268,
[1,0]<stdout>:           102,     0,     0,     0,     0,     0]]) torch.Size([2, 16])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [62, 62, 62,  ..., 62, 62, 62],
[1,0]<stdout>:         [63, 63, 63,  ..., 63, 63, 63],
[1,0]<stdout>:         [64, 64, 64,  ..., 64, 64, 64]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [63, 63, 63,  ..., 63, 63, 63],
[1,0]<stdout>:         [64, 64, 64,  ..., 64, 64, 64],
[1,0]<stdout>:         [65, 65, 65,  ..., 65, 65, 65]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([2, 65, 768])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 64, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
[1,0]<stdout>:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
[1,0]<stdout>:         72, 73, 74],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 16, 17, 18, 19, 20, 21, 22,
[1,0]<stdout>:         23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,
[1,0]<stdout>:         41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58,
[1,0]<stdout>:         59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76,
[1,0]<stdout>:         77, 78, 79]]) torch.Size([2, 75])
[1,0]<stdout>:Warning: NaN or Inf found in input tensor.
[1,0]<stdout>:Gradient overflow.  Skipping step, loss scaler 5 reducing loss scale to 16384.0
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:[Debug] item 21949 text attn mask tensor([1, 1, 1, 1, 1, 1]) torch.Size([6])
[1,0]<stdout>:[Debug] item img 21949 is None
[1,0]<stdout>:[Debug] item 21949 speech is not None
[1,0]<stdout>:before gather embedding input torch.Size([2, 77, 768])
[1,0]<stdout>:[Debug] item 21949 speech attn mask torch.Size([29]) and final attn mask torch.Size([35])
[1,0]<stdout>:[Debug] item 26130 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([9])
[1,0]<stdout>:[Debug] item img 26130 is None
[1,0]<stdout>:[Debug] item 26130 speech is not None
[1,0]<stdout>:[Debug] item 26130 speech attn mask torch.Size([26]) and final attn mask torch.Size([35])
[1,0]<stdout>:[Debug] batch indexs [21949, 26130]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 35])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[ 101, 7632, 1012, 7632,  103,  102,    0,    0,    0],
[1,0]<stdout>:        [ 101, 2043,  103, 2054, 2106, 2016, 2215, 1029,  102]]) torch.Size([2, 9])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 29, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,
[1,0]<stdout>:         21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]]) torch.Size([2, 35])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [74, 74, 74,  ..., 74, 74, 74],
[1,0]<stdout>:         [75, 75, 75,  ..., 75, 75, 75],
[1,0]<stdout>:         [76, 76, 76,  ..., 76, 76, 76]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [74, 74, 74,  ..., 74, 74, 74],
[1,0]<stdout>:         [75, 75, 75,  ..., 75, 75, 75],
[1,0]<stdout>:         [76, 76, 76,  ..., 76, 76, 76]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([2, 77, 768])
[1,0]<stdout>:Warning: NaN or Inf found in input tensor.
[1,0]<stdout>:Gradient overflow.  Skipping step, loss scaler 3 reducing loss scale to 32768.0
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:[Debug] item 15086 text attn mask tensor([1, 1, 1, 1, 1, 1, 1]) torch.Size([7])
[1,0]<stdout>:[Debu[1,0]<stdout>:g] item img 15086 is None
[1,0]<stdout>:[Debug] item 15086 speech is not None
[1,0]<stdout>:before gather embedding input torch.Size([2, 34, 768])
[1,0]<stdout>:[Debug] item 15086 speech attn mask torch.Size([60]) and final attn mask torch.Size([67])
[1,0]<stdout>:[Debug] item 51847 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([12])
[1,0]<stdout>:[Debug] item img 51847 is None
[1,0]<stdout>:[Debug] item 51847 speech is not None
[1,0]<stdout>:[Debug] item 51847 speech attn mask torch.Size([55]) and final attn mask torch.Size([67])
[1,0]<stdout>:[Debug] batch indexs [15086, 51847]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 67])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[ 101,  103, 2017, 2131, 2023, 1012,  102,    0,    0,    0,    0,    0],
[1,0]<stdout>:        [ 101, 2026, 9588, 2052, 2425, 2033, 2000, 2485, 2026,  103, 1012,  102]]) torch.Size([2, 12])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [31, 31, 31,  ..., 31, 31, 31],
[1,0]<stdout>:         [32, 32, 32,  ..., 32, 32, 32],
[1,0]<stdout>:         [33, 33, 33,  ..., 33, 33, 33]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [31, 31, 31,  ..., 31, 31, 31],
[1,0]<stdout>:         [32, 32, 32,  ..., 32, 32, 32],
[1,0]<stdout>:         [33, 33, 33,  ..., 33, 33, 33]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([2, 34, 768])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 60, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,
[1,0]<stdout>:         23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,
[1,0]<stdout>:         41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58,
[1,0]<stdout>:         59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
[1,0]<stdout>:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66]]) torch.Size([2, 67])
[1,0]<stdout>:Warning: NaN or Inf found in input tensor.
[1,0]<stdout>:Gradient overflow.  Skipping step, loss scaler 3 reducing loss scale to 16384.0
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:[Debug] item 4680 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([14])
[1,0]<stdout>:[Debug] item img 4680 is None
[1,0]<stdout>:[Debug] item 4680 speech is not None
[1,0]<stdout>:before gather embedding input torch.Size([2, 41, 768])
[1,0]<stdout>:[Debug] item 4680 speech attn mask torch.Size([56]) and final attn mask torch.Size([70])
[1,0]<stdout>:[Debug] item 25386 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([18])
[1,0]<stdout>:[Debug] item img 25386 is None
[1,0]<stdout>:[Debug] item 25386 speech is not None
[1,0]<stdout>:[Debug] item 25386 speech attn mask torch.Size([52]) and final attn mask torch.Size([70])
[1,0]<stdout>:[Debug] batch indexs [4680, 25386]
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [38, 38, 38,  ..., 38, 38, 38],
[1,0]<stdout>:         [39, 39, 39,  ..., 39, 39, 39],
[1,0]<stdout>:         [40, 40, 40,  ..., 40, 40, 40]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [36, 36, 36,  ..., 36, 36, 36],
[1,0]<stdout>:         [37, 37, 37,  ..., 37, 37, 37],
[1,0]<stdout>:         [38, 38, 38,  ..., 38, 38, 38]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([2, 39, 768])
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 70])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[  101,  2016,  2001,  2025,  1012,  2016,  2001, 15488,  9541,  8450,
[1,0]<stdout>:          2115,  2567,  1012,   102,     0,     0,     0,     0],
[1,0]<stdout>:        [  101,  1045,  1005,  1040,  2156,   103,  2004,  1037,  2613, 14583,
[1,0]<stdout>:          1012,   103,  1005,  1056,   103,  2008,  1012,   102]]) torch.Size([2, 18])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 56, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 18, 19, 20, 21,
[1,0]<stdout>:         22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,
[1,0]<stdout>:         40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57,
[1,0]<stdout>:         58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
[1,0]<stdout>:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]]) torch.Size([2, 70])
[1,0]<stdout>:Warning: NaN or Inf found in input tensor.
[1,0]<stdout>:Gradient overflow.  Skipping step, loss scaler 1 reducing loss scale to 32768.0
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([2, 80, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [72, 72, 72,  ..., 72, 72, 72],
[1,0]<stdout>:         [73, 73, 73,  ..., 73, 73, 73],
[1,0]<stdout>:         [74, 74, 74,  ..., 74, 74, 74]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [77, 77, 77,  ..., 77, 77, 77],
[1,0]<stdout>:         [78, 78, 78,  ..., 78, 78, 78],
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([2, 75, 768])
[1,0]<stdout>:Warning: NaN or Inf found in input tensor.
[1,0]<stdout>:Gradient overflow.  Skipping step, loss scaler 3 reducing loss scale to 8192.0
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([4, 48, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [45, 45, 45,  ..., 45, 45, 45],
[1,0]<stdout>:         [46, 46, 46,  ..., 46, 46, 46],
[1,0]<stdout>:         [47, 47, 47,  ..., 47, 47, 47]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [45, 45, 45,  ..., 45, 45, 45],
[1,0]<stdout>:         [46, 46, 46,  ..., 46, 46, 46],
[1,0]<stdout>:         [47, 47, 47,  ..., 47, 47, 47]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [45, 45, 45,  ..., 45, 45, 45],
[1,0]<stdout>:         [46, 46, 46,  ..., 46, 46, 46],
[1,0]<stdout>:         [47, 47, 47,  ..., 47, 47, 47]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [45, 45, 45,  ..., 45, 45, 45],
[1,0]<stdout>:         [46, 46, 46,  ..., 46, 46, 46],
[1,0]<stdout>:         [47, 47, 47,  ..., 47, 47, 47]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([4, 48, 768])
[1,0]<stdout>:[Debug] item 81505 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([13])
[1,0]<stdout>:[Debug] item img 81505 is None
[1,0]<stdout>:[Debug] item 81505 speech is not None
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:[Debug] item 81505 speech attn mask torch.Size([50]) and final attn mask torch.Size([63])
[1,0]<stdout>:before gather embedding input torch.Size([4, 84, 768])
[1,0]<stdout>:[Debug] item 30522 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([11])
[1,0]<stdout>:[Debug] item img 30522 is None
[1,0]<stdout>:[Debug] item 30522 speech is not None
[1,0]<stdout>:[Debug] item 30522 speech attn mask torch.Size([52]) and final attn mask torch.Size([63])
[1,0]<stdout>:[Debug] batch indexs [81505, 30522]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 63])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[  101,  2431,  1996,  2111,  2006,  2023,  2911,   103, 16118,  2000,
[1,0]<stdout>:          3280,  1012,   102],
[1,0]<stdout>:        [  101,  1998,  3531,  5959, 14094,  1997,  2014,  3256,  6949,  1012,
[1,0]<stdout>:           102,     0,     0]]) torch.Size([2, 13])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 52, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
[1,0]<stdout>:         54, 55, 56, 57, 58, 59, 60, 61, 62],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 13, 14, 15, 16, 17, 18, 19,
[1,0]<stdout>:         20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,
[1,0]<stdout>:         38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,
[1,0]<stdout>:         56, 57, 58, 59, 60, 61, 62, 63, 64]]) torch.Size([2, 63])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [81, 81, 81,  ..., 81, 81, 81],
[1,0]<stdout>:         [82, 82, 82,  ..., 82, 82, 82],
[1,0]<stdout>:         [83, 83, 83,  ..., 83, 83, 83]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [81, 81, 81,  ..., 81, 81, 81],
[1,0]<stdout>:         [82, 82, 82,  ..., 82, 82, 82],
[1,0]<stdout>:         [83, 83, 83,  ..., 83, 83, 83]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [81, 81, 81,  ..., 81, 81, 81],
[1,0]<stdout>:         [82, 82, 82,  ..., 82, 82, 82],
[1,0]<stdout>:         [83, 83, 83,  ..., 83, 83, 83]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [81, 81, 81,  ..., 81, 81, 81],
[1,0]<stdout>:         [82, 82, 82,  ..., 82, 82, 82],
[1,0]<stdout>:         [83, 83, 83,  ..., 83, 83, 83]]], device='cuda:0')[1,0]<stdout>:
[1,0]<stdout>:after gather embedding_output torch.Size([4, 84, 768])
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([2, 42, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [36, 36, 36,  ..., 36, 36, 36],
[1,0]<stdout>:         [37, 37, 37,  ..., 37, 37, 37],
[1,0]<stdout>:         [38, 38, 38,  ..., 38, 38, 38]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [39, 39, 39,  ..., 39, 39, 39],
[1,0]<stdout>:         [40, 40, 40,  ..., 40, 40, 40],
[1,0]<stdout>:         [41, 41, 41,  ..., 41, 41, 41]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([2, 39, 768])
[1,0]<stdout>:Warning: NaN or Inf found in input tensor.
[1,0]<stdout>:Gradient overflow.  Skipping step, loss scaler 5 reducing loss scale to 8192.0
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([4, 64, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [57, 57, 57,  ..., 57, 57, 57],
[1,0]<stdout>:         [58, 58, 58,  ..., 58, 58, 58],
[1,0]<stdout>:         [59, 59, 59,  ..., 59, 59, 59]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [57, 57, 57,  ..., 57, 57, 57],
[1,0]<stdout>:         [58, 58, 58,  ..., 58, 58, 58],
[1,0]<stdout>:         [59, 59, 59,  ..., 59, 59, 59]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [61, 61, 61,  ..., 61, 61, 61],
[1,0]<stdout>:         [62, 62, 62,  ..., 62, 62, 62],
[1,0]<stdout>:         [63, 63, 63,  ..., 63, 63, 63]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [61, 61, 61,  ..., 61, 61, 61],
[1,0]<stdout>:         [62, 62, 62,  ..., 62, 62, 62],
[1,0]<stdout>:         [63, 63, 63,  ..., 63, 63, 63]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([4, 60, 768])
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([4, 80, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [77, 77, 77,  ..., 77, 77, 77],
[1,0]<stdout>:         [78, 78, 78,  ..., 78, 78, 78],
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [77, 77, 77,  ..., 77, 77, 77],
[1,0]<stdout>:         [78, 78, 78,  ..., 78, 78, 78],
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [77, 77, 77,  ..., 77, 77, 77],
[1,0]<stdout>:         [78, 78, 78,  ..., 78, 78, 78],
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [77, 77, 77,  ..., 77, 77, 77],
[1,0]<stdout>:         [78, 78, 78,  ..., 78, 78, 78],
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([4, 80, 768])
[1,0]<stdout>:[Debug] item 38480 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([8])
[1,0]<stdout>:[Debug] item img 38480 is None
[1,0]<stdout>:[Debug] item 38480 speech is not None
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([4, 73, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [70, 70, 70,  ..., 70, 70, 70],
[1,0]<stdout>:         [71, 71, 71,  ..., 71, 71, 71],
[1,0]<stdout>:         [72, 72, 72,  ..., 72, 72, 72]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [70, 70, 70,  ..., 70, 70, 70],
[1,0]<stdout>:         [71, 71, 71,  ..., 71, 71, 71],
[1,0]<stdout>:         [72, 72, 72,  ..., 72, 72, 72]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [70, 70, 70,  ..., 70, 70, 70],
[1,0]<stdout>:         [71, 71, 71,  ..., 71, 71, 71],
[1,0]<stdout>:         [72, 72, 72,  ..., 72, 72, 72]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [70, 70, 70,  ..., 70, 70, 70],
[1,0]<stdout>:         [71, 71, 71,  ..., 71, 71, 71],
[1,0]<stdout>:         [72, 72, 72,  ..., 72, 72, 72]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([4, 73, 768])
[1,0]<stdout>:[Debug] item 38480 speech attn mask torch.Size([34]) and final attn mask torch.Size([42])
[1,0]<stdout>:[Debug] item 22290 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([9])
[1,0]<stdout>:[Debug] item img 22290 is None
[1,0]<stdout>:[Debug] item 22290 speech is not None
[1,0]<stdout>:[Debug] item 22290 speech attn mask torch.Size([33]) and final attn mask torch.Size([42])
[1,0]<stdout>:[Debug] batch indexs [38480, 22290]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 42])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[ 101,  103, 2002, 2145, 2573, 2045, 1029,  102,    0],
[1,0]<stdout>:        [ 101,  103, 1005, 1055, 1037, 2204, 2269, 1012,  102]]) torch.Size([2, 9])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 34, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,
[1,0]<stdout>:         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,
[1,0]<stdout>:         37, 38, 39, 40, 41, 42],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38, 39, 40, 41]]) torch.Size([2, 42])
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([2, 69, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [66, 66, 66,  ..., 66, 66, 66],
[1,0]<stdout>:         [67, 67, 67,  ..., 67, 67, 67],
[1,0]<stdout>:         [68, 68, 68,  ..., 68, 68, 68]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [59, 59, 59,  ..., 59, 59, 59],
[1,0]<stdout>:         [60, 60, 60,  ..., 60, 60, 60],
[1,0]<stdout>:         [61, 61, 61,  ..., 61, 61, 61]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([2, 62, 768])
[1,0]<stdout>:Warning: NaN or Inf found in input tensor.
[1,0]<stdout>:Gradient overflow.  Skipping step, loss scaler 1 reducing loss scale to 16384.0
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([4, 82, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79],
[1,0]<stdout>:         [80, 80, 80,  ..., 80, 80, 80],
[1,0]<stdout>:         [81, 81, 81,  ..., 81, 81, 81]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [77, 77, 77,  ..., 77, 77, 77],
[1,0]<stdout>:         [78, 78, 78,  ..., 78, 78, 78],
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79],
[1,0]<stdout>:         [80, 80, 80,  ..., 80, 80, 80],
[1,0]<stdout>:         [81, 81, 81,  ..., 81, 81, 81]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [79, 79, 79,  ..., 79, 79, 79],
[1,0]<stdout>:         [80, 80, 80,  ..., 80, 80, 80],
[1,0]<stdout>:         [81, 81, 81,  ..., 81, 81, 81]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([4, 80, 768])
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([4, 70, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [62, 62, 62,  ..., 62, 62, 62],
[1,0]<stdout>:         [63, 63, 63,  ..., 63, 63, 63],
[1,0]<stdout>:         [64, 64, 64,  ..., 64, 64, 64]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [63, 63, 63,  ..., 63, 63, 63],
[1,0]<stdout>:         [64, 64, 64,  ..., 64, 64, 64],
[1,0]<stdout>:         [65, 65, 65,  ..., 65, 65, 65]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [62, 62, 62,  ..., 62, 62, 62],
[1,0]<stdout>:         [63, 63, 63,  ..., 63, 63, 63],
[1,0]<stdout>:         [64, 64, 64,  ..., 64, 64, 64]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [67, 67, 67,  ..., 67, 67, 67],
[1,0]<stdout>:         [68, 68, 68,  ..., 68, 68, 68],
[1,0]<stdout>:         [69, 69, 69,  ..., 69, 69, 69]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([4, 65, 768])
[1,0]<stdout>:Warning: NaN or Inf found in input tensor.
[1,0]<stdout>:Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:[Debug] item 27040 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([8])
[1,0]<stdout>:[Debug] item img 27040 is None
[1,0]<stdout>:[Debug] item 27040 speech is not None
[1,0]<stdout>:before gather embedding input torch.Size([4, 84, 768])
[1,0]<stdout>:[Debug] item 27040 speech attn mask torch.Size([23]) and final attn mask torch.Size([31])
[1,0]<stdout>:[Debug] item 12706 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([8])
[1,0]<stdout>:[Debug] item img 12706 is None
[1,0]<stdout>:[Debug] item 12706 speech is not None
[1,0]<stdout>:[Debug] item 12706 speech attn mask torch.Size([23]) and final attn mask torch.Size([31])
[1,0]<stdout>:[Debug] batch indexs [27040, 12706]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 31])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[ 101,  103, 2017, 2156, 2008, 1029,  999,  102],
[1,0]<stdout>:        [ 101, 2009, 1005, 1055, 6057, 2157, 1029,  102]]) torch.Size([2, 8])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 23, 130])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [81, 81, 81,  ..., 81, 81, 81],
[1,0]<stdout>:         [82, 82, 82,  ..., 82, 82, 82],
[1,0]<stdout>:         [83, 83, 83,  ..., 83, 83, 83]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [67, 67, 67,  ..., 67, 67, 67],
[1,0]<stdout>:         [68, 68, 68,  ..., 68, 68, 68],
[1,0]<stdout>:         [69, 69, 69,  ..., 69, 69, 69]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [67, 67, 67,  ..., 67, 67, 67],
[1,0]<stdout>:         [68, 68, 68,  ..., 68, 68, 68],
[1,0]<stdout>:         [69, 69, 69,  ..., 69, 69, 69]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [67, 67, 67,  ..., 67, 67, 67],
[1,0]<stdout>:         [68, 68, 68,  ..., 68, 68, 68],
[1,0]<stdout>:         [69, 69, 69,  ..., 69, 69, 69]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([4, 70, 768])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]]) torch.Size([2, 31])
[1,0]<stdout>:[Debug] item 30061 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([16])
[1,0]<stdout>:[Debug] item img 30061 is None
[1,0]<stdout>:[Debug] item 30061 speech is not None
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([2, 38, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [35, 35, 35,  ..., 35, 35, 35],
[1,0]<stdout>:         [36, 36, 36,  ..., 36, 36, 36],
[1,0]<stdout>:         [37, 37, 37,  ..., 37, 37, 37]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [32, 32, 32,  ..., 32, 32, 32],
[1,0]<stdout>:         [33, 33, 33,  ..., 33, 33, 33],
[1,0]<stdout>:         [34, 34, 34,  ..., 34, 34, 34]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([2, 35, 768])
[1,0]<stdout>:[Debug] item 30061 speech attn mask torch.Size([64]) and final attn mask torch.Size([80])
[1,0]<stdout>:[Debug] item 44757 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([16])
[1,0]<stdout>:[Debug] item img 44757 is None
[1,0]<stdout>:[Debug] item 44757 speech is not None
[1,0]<stdout>:[Debug] item 44757 speech attn mask torch.Size([64]) and final attn mask torch.Size([80])
[1,0]<stdout>:[Debug] batch indexs [30061, 44757]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 80])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[  101,   103,  1010,  1038,  1012, 18178, 25062,  8090,   103,  1012,
[1,0]<stdout>:          2507,  2033,  2026,  2769,  1012,   102],
[1,0]<stdout>:        [  101,  1037,  2095,  3849,   103,  2200,  2146,  2051,  2000,  3524,
[1,0]<stdout>:          2077,  1045,   103,  2068,   103,   102]]) torch.Size([2, 16])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 64, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
[1,0]<stdout>:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
[1,0]<stdout>:         72, 73, 74, 75, 76, 77, 78, 79],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
[1,0]<stdout>:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
[1,0]<stdout>:         72, 73, 74, 75, 76, 77, 78, 79]]) torch.Size([2, 80])
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([2, 29, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [26, 26, 26,  ..., 26, 26, 26],
[1,0]<stdout>:         [27, 27, 27,  ..., 27, 27, 27],
[1,0]<stdout>:         [28, 28, 28,  ..., 28, 28, 28]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [26, 26, 26,  ..., 26, 26, 26],
[1,0]<stdout>:         [27, 27, 27,  ..., 27, 27, 27],
[1,0]<stdout>:         [28, 28, 28,  ..., 28, 28, 28]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([2, 29, 768])
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([4, 60, 768])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [57, 57, 57,  ..., 57, 57, 57],
[1,0]<stdout>:         [58, 58, 58,  ..., 58, 58, 58],
[1,0]<stdout>:         [59, 59, 59,  ..., 59, 59, 59]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [52, 52, 52,  ..., 52, 52, 52],
[1,0]<stdout>:         [53, 53, 53,  ..., 53, 53, 53],
[1,0]<stdout>:         [54, 54, 54,  ..., 54, 54, 54]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [50, 50, 50,  ..., 50, 50, 50],
[1,0]<stdout>:         [51, 51, 51,  ..., 51, 51, 51],
[1,0]<stdout>:         [52, 52, 52,  ..., 52, 52, 52]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [52, 52, 52,  ..., 52, 52, 52],
[1,0]<stdout>:         [53, 53, 53,  ..., 53, 53, 53],
[1,0]<stdout>:         [54, 54, 54,  ..., 54, 54, 54]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([4, 53, 768])
[1,0]<stdout>:[Debug] item 6427 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([10])
[1,0]<stdout>:[Debug] item img 6427 is None
[1,0]<stdout>:[Debug] item 6427 speech is not None
[1,0]<stdout>:[Debug] item 6427 speech attn mask torch.Size([32]) and final attn mask torch.Size([42])
[1,0]<stdout>:[Debug] item 13797 text attn mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) torch.Size([12])
[1,0]<stdout>:[Debug] item img 13797 is None
[1,0]<stdout>:[Debug] item 13797 speech is not None
[1,0]<stdout>:[Debug] item 13797 speech attn mask torch.Size([30]) and final attn mask torch.Size([42])
[1,0]<stdout>:[Debug] batch indexs [6427, 13797]
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) torch.Size([2, 42])
[1,0]<stdout>:before gather embedding input torch.Size([4, 76, 768])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[  101,   103,  1010,  2984,  1010,  1045,  2001, 24929,  1012,   102,
[1,0]<stdout>:             0,     0],
[1,0]<stdout>:        [  101,  2009,  1005,  1055,  2005,  8594,   103,   103,  2005,  2033,
[1,0]<stdout>:          1012,   102]]) torch.Size([2, 12])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 32, 130])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 12, 13, 14, 15, 16, 17, 18, 19,
[1,0]<stdout>:         20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,
[1,0]<stdout>:         38, 39, 40, 41, 42, 43],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
[1,0]<stdout>:         36, 37, 38, 39, 40, 41]]) torch.Size([2, 42])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [72, 72, 72,  ..., 72, 72, 72],
[1,0]<stdout>:         [73, 73, 73,  ..., 73, 73, 73],
[1,0]<stdout>:         [74, 74, 74,  ..., 74, 74, 74]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [72, 72, 72,  ..., 72, 72, 72],
[1,0]<stdout>:         [73, 73, 73,  ..., 73, 73, 73],
[1,0]<stdout>:         [74, 74, 74,  ..., 74, 74, 74]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [73, 73, 73,  ..., 73, 73, 73],
[1,0]<stdout>:         [74, 74, 74,  ..., 74, 74, 74],
[1,0]<stdout>:         [75, 75, 75,  ..., 75, 75, 75]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [72, 72, 72,  ..., 72, 72, 72],
[1,0]<stdout>:         [73, 73, 73,  ..., 73, 73, 73],
[1,0]<stdout>:         [74, 74, 74,  ..., 74, 74, 74]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([4, 75, 768])
[1,0]<stdout>:[Debug] item 28056 text attn mask tensor([1, 1, 1, 1, 1, 1, 1]) torch.Size([7])
[1,0]<stdout>:[Debug] item img 28056 is None
[1,0]<stdout>:[Debug] item 28056 speech is not None
[1,0]<stdout>:extended_attention_mask
[1,0]<stdout>:before gather embedding input torch.Size([2, 72, 768])
[1,0]<stdout>:[Debug] item 28056 speech attn mask torch.Size([21]) and final attn mask torch.Size([28])
[1,0]<stdout>:[Debug] item 46906 text attn mask tensor([1, 1, 1, 1, 1]) torch.Size([5])
[1,0]<stdout>:[Debug] item img 46906 is None
[1,0]<stdout>:[Debug] item 46906 speech is not None
[1,0]<stdout>:[Debug] item 46906 speech attn mask torch.Size([23]) and final attn mask torch.Size([28])
[1,0]<stdout>:[Debug] batch indexs [28056, 46906]
[1,0]<stdout>:[Debug] batch attn_masks tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1],
[1,0]<stdout>:        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
[1,0]<stdout>:         1, 1, 1, 1]]) torch.Size([2, 28])
[1,0]<stdout>:[Debug] batch padding input_ids tensor([[ 101,  103, 1005, 1055, 2157, 1012,  102],
[1,0]<stdout>:        [ 101,  103, 2239, 1029,  102,    0,    0]]) torch.Size([2, 7])
[1,0]<stdout>:[Debug] batch padding speech input torch.Size([2, 23, 130])
[1,0]<stdout>:gather index is tensor([[[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [69, 69, 69,  ..., 69, 69, 69],
[1,0]<stdout>:         [70, 70, 70,  ..., 70, 70, 70],
[1,0]<stdout>:         [71, 71, 71,  ..., 71, 71, 71]],
[1,0]<stdout>:
[1,0]<stdout>:        [[ 0,  0,  0,  ...,  0,  0,  0],
[1,0]<stdout>:         [ 1,  1,  1,  ...,  1,  1,  1],
[1,0]<stdout>:         [ 2,  2,  2,  ...,  2,  2,  2],
[1,0]<stdout>:         ...,
[1,0]<stdout>:         [64, 64, 64,  ..., 64, 64, 64],
[1,0]<stdout>:         [65, 65, 65,  ..., 65, 65, 65],
[1,0]<stdout>:         [66, 66, 66,  ..., 66, 66, 66]]], device='cuda:0')
[1,0]<stdout>:after gather embedding_output torch.Size([2, 67, 768])
[1,0]<stdout>:[Debug] batch gather_index tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
[1,0]<stdout>:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27],
[1,0]<stdout>:        [ 0,  1,  2,  3,  4,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
[1,0]<stdout>:         20, 21, 22, 23, 24, 25, 26, 27, 28, 29]]) torch.Size([2, 28])
