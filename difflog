diff --git a/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks.json b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks.json
index 908acdf..649087d 100644
--- a/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks.json
+++ b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks.json
@@ -40,7 +40,7 @@
                 "/data7/emobert/img_db_nomask/movies_v1/fc"
             ],
             "speech": [
-                "/data7/emobert/norm_comparE_db/movies_v1_5mean/"
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
             ],
             "tasks": [
                 "itm",
@@ -49,7 +49,7 @@
                 "mrckl"
             ],
             "mix_ratio": [
-                1,
+                2,
                 2,
                 1,
                 1
@@ -64,7 +64,7 @@
                 "/data7/emobert/img_db_nomask/movies_v2/fc"
             ],
             "speech": [
-                "/data7/emobert/norm_comparE_db/movies_v2_5mean/"
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
             ],
             "tasks": [
                 "itm",
@@ -73,7 +73,7 @@
                 "mrckl"
             ],
             "mix_ratio": [
-                1,
+                2,
                 2,
                 1,
                 1
@@ -88,7 +88,7 @@
                 "/data7/emobert/img_db_nomask/movies_v3/fc"
             ],
             "speech": [
-                "/data7/emobert/norm_comparE_db/movies_v3_5mean/"
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
             ],
             "tasks": [
                 "itm",
@@ -97,7 +97,7 @@
                 "mrckl"
             ],
             "mix_ratio": [
-                1,
+                2,
                 2,
                 1,
                 1
@@ -114,7 +114,7 @@
                 "/data7/emobert/img_db_nomask/movies_v1/fc"
             ],
             "speech": [
-                "/data7/emobert/norm_comparE_db/movies_v1_5mean/"
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
             ],
             "tasks": [
                 "itm",
@@ -132,7 +132,7 @@
                 "/data7/emobert/img_db_nomask/movies_v1/fc"
             ],
             "speech": [
-                "/data7/emobert/norm_comparE_db/movies_v1_5mean/"
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
             ],
             "tasks": [
                 "itm",
@@ -150,7 +150,7 @@
                 "/data7/emobert/img_db_nomask/movies_v2/fc"
             ],
             "speech": [
-                "/data7/emobert/norm_comparE_db/movies_v2_5mean/"
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
             ],
             "tasks": [
                 "itm",
@@ -168,7 +168,7 @@
                 "/data7/emobert/img_db_nomask/movies_v2/fc"
             ],
             "speech": [
-                "/data7/emobert/norm_comparE_db/movies_v2_5mean/"
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
             ],
             "tasks": [
                 "itm",
@@ -186,7 +186,7 @@
                 "/data7/emobert/img_db_nomask/movies_v3/fc"
             ],
             "speech": [
-                "/data7/emobert/norm_comparE_db/movies_v3_5mean/"
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
             ],
             "tasks": [
                 "itm",
@@ -204,7 +204,7 @@
                 "/data7/emobert/img_db_nomask/movies_v3/fc"
             ],
             "speech": [
-                "/data7/emobert/norm_comparE_db/movies_v3_5mean/"
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
             ],
             "tasks": [
                 "itm",
diff --git a/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks_debug.json b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks_debug.json
new file mode 100644
index 0000000..1de0e6b
--- /dev/null
+++ b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks_debug.json
@@ -0,0 +1,80 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/nomask_movies_v1v2_4tasks_lr5e5_bs1024_faceth0.5",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/resources/pretrained/uniter-base-uncased-init.pt",
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.5,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 256,
+    "val_batch_size": 256,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 500,
+    "warmup_steps": 400,
+    "num_train_steps": 5000,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 1234,
+    "fp16": true,
+    "n_workers": 4,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "movies_v1",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_emotype_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                2,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "movies_v1_val5k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_emotype_all_val5k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "mrfr" ,           
+                "mrckl"            
+            ]
+        }
+    
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks_emo.json b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks_emo.json
new file mode 100644
index 0000000..18deb06
--- /dev/null
+++ b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks_emo.json
@@ -0,0 +1,233 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/xxx",
+    "model_config": "config/xxx",
+    "checkpoint": "/data7/emobert/resources/pretrained/uniter-base-uncased-init.pt",
+    "melm_prob": 0.5,
+    "mlm_prob": 0.15,
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.5,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "Speech_DIM": 768,
+    "train_batch_size": 256,
+    "val_batch_size": 256,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 500,
+    "warmup_steps": 400,
+    "num_train_steps": 5000,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 1234,
+    "fp16": true,
+    "n_workers": 4,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "movies_v1",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_emotype3_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "movies_v2",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_emotype3_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "movies_v3",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_emotype3_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "movies_v1_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_emotype3_all_val3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr" ,           
+                "mrckl"            
+            ]
+        },
+        {
+            "name": "movies_v1_trn_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_emotype3_all_trn3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",            
+                "mrckl"            
+            ]
+        },
+        {
+            "name": "movies_v2_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_emotype3_all_val3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",            
+                "mrckl"            
+            ]
+        },
+        {
+            "name": "movies_v2_trn3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_emotype3_all_trn3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",            
+                "mrckl"            
+            ]
+        },
+        {
+            "name": "movies_v3_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_emotype3_all_val3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",            
+                "mrckl"            
+            ]
+        },
+        {
+            "name": "movies_v3_trn3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_emotype3_all_trn3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",            
+                "mrckl"            
+            ]
+        }
+    
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks_emo_sentiword.json b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks_emo_sentiword.json
new file mode 100644
index 0000000..80a67e2
--- /dev/null
+++ b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_4tasks_emo_sentiword.json
@@ -0,0 +1,233 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/xxx",
+    "model_config": "config/xxx",
+    "checkpoint": "/data7/emobert/resources/pretrained/uniter-base-uncased-init.pt",
+    "melm_prob": 0.5,
+    "mlm_prob": 0.15,
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.5,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "Speech_DIM": 768,
+    "train_batch_size": 256,
+    "val_batch_size": 256,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 500,
+    "warmup_steps": 400,
+    "num_train_steps": 5000,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 1234,
+    "fp16": true,
+    "n_workers": 4,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "movies_v1",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_sentiword_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "movies_v2",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_sentiword_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "movies_v3",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_sentiword_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "movies_v1_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_sentiword_all_val3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr" ,           
+                "mrckl"            
+            ]
+        },
+        {
+            "name": "movies_v1_trn_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_sentiword_all_trn3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",            
+                "mrckl"            
+            ]
+        },
+        {
+            "name": "movies_v2_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_sentiword_all_val3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",            
+                "mrckl"            
+            ]
+        },
+        {
+            "name": "movies_v2_trn3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_sentiword_all_trn3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",            
+                "mrckl"            
+            ]
+        },
+        {
+            "name": "movies_v3_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_sentiword_all_val3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",            
+                "mrckl"            
+            ]
+        },
+        {
+            "name": "movies_v3_trn3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_sentiword_all_trn3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",            
+                "mrckl"            
+            ]
+        }
+    
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_5tasks.json b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_5tasks.json
new file mode 100644
index 0000000..b3be68b
--- /dev/null
+++ b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_5tasks.json
@@ -0,0 +1,230 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/nomask_movies_v1v2_4tasks_lr5e5_bs1024_faceth0.5",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/resources/pretrained/uniter-base-uncased-init.pt",
+    "msrm_prob": 0.15,
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.5,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 256,
+    "val_batch_size": 256,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 500,
+    "warmup_steps": 400,
+    "num_train_steps": 5000,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 1234,
+    "fp16": true,
+    "n_workers": 1,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "movies_v1",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_sentiword_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "mlm",
+                "mrfr",
+                "mrckl",
+                "msrfr",
+                "itm"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                2
+            ]
+        },
+        {
+            "name": "movies_v2",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_sentiword_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
+            ],
+            "tasks": [
+                "mlm",
+                "mrfr",
+                "mrckl",
+                "msrfr",
+                "itm"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                2
+            ]
+        },
+        {
+            "name": "movies_v3",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_sentiword_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
+            ],
+            "tasks": [
+                "mlm",
+                "mrfr",
+                "mrckl",
+                "msrfr",
+                "itm"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                2
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "movies_v1_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_sentiword_all_val3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "mlm",
+                "mrfr",
+                "mrckl",
+                "msrfr",
+                "itm"      
+            ]
+        },
+        {
+            "name": "movies_v1_trn_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_sentiword_all_trn3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "mlm",
+                "mrfr",
+                "mrckl",
+                "msrfr",
+                "itm"       
+            ]
+        },
+        {
+            "name": "movies_v2_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_sentiword_all_val3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
+            ],
+            "tasks": [
+                "mlm",
+                "mrfr",
+                "mrckl",
+                "msrfr",
+                "itm"      
+            ]
+        },
+        {
+            "name": "movies_v2_trn3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_sentiword_all_trn3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
+            ],
+            "tasks": [
+                "mlm",
+                "mrfr",
+                "mrckl",
+                "msrfr",
+                "itm"         
+            ]
+        },
+        {
+            "name": "movies_v3_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_sentiword_all_val3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
+            ],
+            "tasks": [
+                "mlm",
+                "mrfr",
+                "mrckl",
+                "msrfr",
+                "itm"       
+            ]
+        },
+        {
+            "name": "movies_v3_trn3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_sentiword_all_trn3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
+            ],
+            "tasks": [
+                "mlm",
+                "mrfr",
+                "mrckl",
+                "msrfr",
+                "itm"          
+            ]
+        }
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_text_2tasks_mlmitm.json b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_text_2tasks_mlmitm.json
index ca3f54b..984d464 100644
--- a/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_text_2tasks_mlmitm.json
+++ b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_text_2tasks_mlmitm.json
@@ -3,6 +3,7 @@
     "model_config": "config/uniter-base.json",
     "checkpoint": "/data7/emobert/resources/pretrained/uniter-base-uncased-init.pt",
     "mrm_prob": 0.15,
+    "msrm_prob": 0.15,
     "itm_neg_prob": 0.5,
     "itm_ot_lambda": 0.1,
     "max_txt_len": 30,
@@ -44,7 +45,8 @@
             ],
             "tasks": [
                 "itm",
-                "mlm"
+                "mlm",
+                "msrm"
             ],
             "mix_ratio": [
                 1,
diff --git a/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_text_3tasks_mlmitmmsm.json b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_text_3tasks_mlmitmmsm.json
new file mode 100644
index 0000000..af694b8
--- /dev/null
+++ b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_text_3tasks_mlmitmmsm.json
@@ -0,0 +1,206 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/nomask_movies_v1v2_4tasks_lr5e5_bs1024_faceth0.5",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/resources/pretrained/uniter-base-uncased-init.pt",
+    "msrm_prob": 0.15,
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.5,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 256,
+    "val_batch_size": 256,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 500,
+    "warmup_steps": 400,
+    "num_train_steps": 5000,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 1234,
+    "fp16": true,
+    "n_workers": 1,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "movies_v1",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_sentiword_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"
+            ],
+            "mix_ratio": [
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "movies_v2",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_sentiword_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"
+            ],
+            "mix_ratio": [
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "movies_v3",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_sentiword_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"
+            ],
+            "mix_ratio": [
+                1,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "movies_v1_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_sentiword_all_val3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"         
+            ]
+        },
+        {
+            "name": "movies_v1_trn_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_sentiword_all_trn3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"          
+            ]
+        },
+        {
+            "name": "movies_v2_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_sentiword_all_val3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"         
+            ]
+        },
+        {
+            "name": "movies_v2_trn3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_sentiword_all_trn3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"          
+            ]
+        },
+        {
+            "name": "movies_v3_val3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_sentiword_all_val3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"          
+            ]
+        },
+        {
+            "name": "movies_v3_trn3k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_sentiword_all_trn3k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"           
+            ]
+        }
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vecasr_text_3tasks_mlmitmmsm.json b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vecasr_text_3tasks_mlmitmmsm.json
new file mode 100644
index 0000000..7cbdc41
--- /dev/null
+++ b/code/uniter3m/config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vecasr_text_3tasks_mlmitmmsm.json
@@ -0,0 +1,205 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/nomask_movies_v1v2_4tasks_lr5e5_bs1024_faceth0.5",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/resources/pretrained/uniter-base-uncased-init.pt",
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.5,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 256,
+    "val_batch_size": 256,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 500,
+    "warmup_steps": 400,
+    "num_train_steps": 5000,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 1234,
+    "fp16": true,
+    "n_workers": 1,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "movies_v1",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_emotype_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_asr_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"
+            ],
+            "mix_ratio": [
+                1,
+                2,
+                1
+            ]
+        },
+        {
+            "name": "movies_v2",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_emotype_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_asr_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"
+            ],
+            "mix_ratio": [
+                1,
+                2,
+                1
+            ]
+        },
+        {
+            "name": "movies_v3",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_emotype_all_trn.db/"
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_asr_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"
+            ],
+            "mix_ratio": [
+                1,
+                2,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "movies_v1_val5k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_emotype_all_val5k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_asr_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"         
+            ]
+        },
+        {
+            "name": "movies_v1_trn_val5k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v1_th0.5_emowords_emotype_all_trn5k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v1/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v1_asr_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"          
+            ]
+        },
+        {
+            "name": "movies_v2_val5k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_emotype_all_val5k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_asr_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"         
+            ]
+        },
+        {
+            "name": "movies_v2_trn5k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v2_th0.5_emowords_emotype_all_trn5k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v2/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v2_asr_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"          
+            ]
+        },
+        {
+            "name": "movies_v3_val5k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_emotype_all_val5k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_asr_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"          
+            ]
+        },
+        {
+            "name": "movies_v3_trn5k",
+            "db": [
+                "/data7/emobert/txt_db/movies_v3_th0.5_emowords_emotype_all_trn5k.db/"            
+            ],
+            "img": [
+                "/data7/emobert/img_db_nomask/movies_v3/fc"
+            ],
+            "speech": [
+                "/data7/emobert/wav2vec_db/movies_v3_asr_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"           
+            ]
+        }
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-task-iemocap-base-2gpu_4tasks-emo.json b/code/uniter3m/config/pretrain-task-iemocap-base-2gpu_4tasks-emo.json
new file mode 100644
index 0000000..2e93d09
--- /dev/null
+++ b/code/uniter3m/config/pretrain-task-iemocap-base-2gpu_4tasks-emo.json
@@ -0,0 +1,127 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/tasks/iemocap_basedon-nomask_movies_v1_uniter_4tasks_faceth0.1-4tasks",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks_faceth0.1/model_step_10000.pt",
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.0,
+    "max_bb": 64,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "val_batch_size": 64,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 50,
+    "warmup_steps": 0,
+    "num_train_steps": 500,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "iemocap_trn",
+            "db": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/trn_emowords_emotype3.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "iemocap_trnval",
+            "db": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/val_emowords_emotype3.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "iemocap_val",
+            "db": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/val_emowords_emotype3.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"          
+            ]
+        },
+        {
+            "name": "iemocap_tst",
+            "db": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/tst_emowords_emotype3.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"          
+            ]
+        }
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-task-iemocap-base-2gpu_4tasks-emo_sentiword.json b/code/uniter3m/config/pretrain-task-iemocap-base-2gpu_4tasks-emo_sentiword.json
new file mode 100644
index 0000000..8283650
--- /dev/null
+++ b/code/uniter3m/config/pretrain-task-iemocap-base-2gpu_4tasks-emo_sentiword.json
@@ -0,0 +1,127 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/tasks/iemocap_basedon-nomask_movies_v1_uniter_4tasks_faceth0.1-4tasks",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks_faceth0.1/model_step_10000.pt",
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.0,
+    "max_bb": 64,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "val_batch_size": 64,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 50,
+    "warmup_steps": 0,
+    "num_train_steps": 500,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "iemocap_trn",
+            "db": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/trn_emowords_sentiword.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"     
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "iemocap_trnval",
+            "db": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/val_emowords_sentiword.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"     
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "iemocap_val",
+            "db": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/val_emowords_sentiword.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"             
+            ]
+        },
+        {
+            "name": "iemocap_tst",
+            "db": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/tst_emowords_sentiword.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"           
+            ]
+        }
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-task-iemocap_wav2vec-base-2gpu_3tasks.json b/code/uniter3m/config/pretrain-task-iemocap_wav2vec-base-2gpu_3tasks.json
new file mode 100644
index 0000000..e41e442
--- /dev/null
+++ b/code/uniter3m/config/pretrain-task-iemocap_wav2vec-base-2gpu_3tasks.json
@@ -0,0 +1,115 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/tasks/iemocap_basedon-nomask_movies_v1_uniter_4tasks_faceth0.1-4tasks",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks_faceth0.1/model_step_10000.pt",
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.0,
+    "max_bb": 64,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "val_batch_size": 64,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 50,
+    "warmup_steps": 0,
+    "num_train_steps": 500,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "iemocap_trn",
+            "db": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/trn_emowords_emotype.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"
+            ],
+            "mix_ratio": [
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "iemocap_trnval",
+            "db": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/val_emowords_emotype.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"
+            ],
+            "mix_ratio": [
+                1,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "iemocap_val",
+            "db": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/val_emowords_emotype.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"           
+            ]
+        },
+        {
+            "name": "iemocap_tst",
+            "db": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/tst_emowords_emotype.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"           
+            ]
+        }
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-task-meld-base-2gpu_4tasks-emo.json b/code/uniter3m/config/pretrain-task-meld-base-2gpu_4tasks-emo.json
new file mode 100644
index 0000000..634dee6
--- /dev/null
+++ b/code/uniter3m/config/pretrain-task-meld-base-2gpu_4tasks-emo.json
@@ -0,0 +1,128 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/tasks/meld_basedon-nomask_movies_v1_uniter_4tasks_faceth0.1-4tasks",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks_faceth0.1/model_step_10000.pt",
+    "melm_prob": 0.5,
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.0,
+    "max_bb": 64,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "val_batch_size": 64,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 50,
+    "warmup_steps": 0,
+    "num_train_steps": 500,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "meld_trn",
+            "db": [
+                "/data7/emobert/exp/evaluation/MELD/txt_db/{}/train_emowords_emotype3.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "meld_trnval",
+            "db": [
+                "/data7/emobert/exp/evaluation/MELD/txt_db/{}/val_emowords_emotype3.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "meld_val",
+            "db": [
+                "/data7/emobert/exp/evaluation/MELD/txt_db/{}/val_emowords_emotype3.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"          
+            ]
+        },
+        {
+            "name": "meld_tst",
+            "db": [
+                "/data7/emobert/exp/evaluation/MELD/txt_db/{}/test_emowords_emotype3.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"          
+            ]
+        }
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-task-meld-base-2gpu_4tasks-emo_sentiword.json b/code/uniter3m/config/pretrain-task-meld-base-2gpu_4tasks-emo_sentiword.json
new file mode 100644
index 0000000..0542a97
--- /dev/null
+++ b/code/uniter3m/config/pretrain-task-meld-base-2gpu_4tasks-emo_sentiword.json
@@ -0,0 +1,127 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/tasks/meld_basedon-nomask_movies_v1_uniter_4tasks_faceth0.1-4tasks",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks_faceth0.1/model_step_10000.pt",
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.0,
+    "max_bb": 64,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "val_batch_size": 64,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 50,
+    "warmup_steps": 0,
+    "num_train_steps": 500,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "meld_trn",
+            "db": [
+                "/data7/emobert/exp/evaluation/MELD/txt_db/{}/train_emowords_sentiword.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"     
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "meld_trnval",
+            "db": [
+                "/data7/emobert/exp/evaluation/MELD/txt_db/{}/val_emowords_sentiword.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"     
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "meld_val",
+            "db": [
+                "/data7/emobert/exp/evaluation/MELD/txt_db/{}/val_emowords_sentiword.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"             
+            ]
+        },
+        {
+            "name": "meld_tst",
+            "db": [
+                "/data7/emobert/exp/evaluation/MELD/txt_db/{}/test_emowords_sentiword.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"           
+            ]
+        }
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-task-meld_wav2vec-base-2gpu_3tasks.json b/code/uniter3m/config/pretrain-task-meld_wav2vec-base-2gpu_3tasks.json
new file mode 100644
index 0000000..74ad7cc
--- /dev/null
+++ b/code/uniter3m/config/pretrain-task-meld_wav2vec-base-2gpu_3tasks.json
@@ -0,0 +1,115 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/tasks/meld_basedon-nomask_movies_v1_uniter_4tasks_faceth0.1-4tasks",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/exp/task_pretrain/nomask_movies_v1_uniter_4tasks_faceth0.1/model_step_10000.pt",
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.1,
+    "max_bb": 64,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "val_batch_size": 64,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 50,
+    "warmup_steps": 0,
+    "num_train_steps": 500,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "meld_trn",
+            "db": [
+                "/data7/emobert/exp/evaluation/MELD/txt_db/{}/train_emowords_emotype.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"
+            ],
+            "mix_ratio": [
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "meld_trnval",
+            "db": [
+                "/data7/emobert/exp/evaluation/MELD/txt_db/{}/val_emowords_emotype.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"           
+            ],
+            "mix_ratio": [
+                1,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "meld_val",
+            "db": [
+                "/data7/emobert/exp/evaluation/MELD/txt_db/{}/val_emowords_emotype.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"           
+            ]
+        },
+        {
+            "name": "meld_tst",
+            "db": [
+                "/data7/emobert/exp/evaluation/MELD/txt_db/{}/test_emowords_emotype.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"           
+            ]
+        }
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-task-msp-base-2gpu_4tasks-emo.json b/code/uniter3m/config/pretrain-task-msp-base-2gpu_4tasks-emo.json
new file mode 100644
index 0000000..a91be74
--- /dev/null
+++ b/code/uniter3m/config/pretrain-task-msp-base-2gpu_4tasks-emo.json
@@ -0,0 +1,127 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/tasks/msp_basedon-nomask_movies_v1_uniter_4tasks_faceth0.1-4tasks",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks_faceth0.1/model_step_10000.pt",
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.0,
+    "max_bb": 64,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "val_batch_size": 64,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 50,
+    "warmup_steps": 0,
+    "num_train_steps": 500,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "msp_trn",
+            "db": [
+                "/data7/emobert/exp/evaluation/MSP/txt_db/{}/trn_emowords_emotype3.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "msp_trnval",
+            "db": [
+                "/data7/emobert/exp/evaluation/MSP/txt_db/{}/val_emowords_emotype3.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "msp_val",
+            "db": [
+                "/data7/emobert/exp/evaluation/MSP/txt_db/{}/val_emowords_emotype3.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"          
+            ]
+        },
+        {
+            "name": "msp_tst",
+            "db": [
+                "/data7/emobert/exp/evaluation/MSP/txt_db/{}/tst_emowords_emotype3.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"          
+            ]
+        }
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-task-msp-base-2gpu_4tasks-emo_sentiword.json b/code/uniter3m/config/pretrain-task-msp-base-2gpu_4tasks-emo_sentiword.json
new file mode 100644
index 0000000..8cfc102
--- /dev/null
+++ b/code/uniter3m/config/pretrain-task-msp-base-2gpu_4tasks-emo_sentiword.json
@@ -0,0 +1,127 @@
+{
+    "output_dir": "/data7/emobert/exp/pretrain/tasks/msp_basedon-nomask_movies_v1_uniter_4tasks_faceth0.1-4tasks",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks_faceth0.1/model_step_10000.pt",
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.0,
+    "max_bb": 64,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "val_batch_size": 64,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 5e-05,
+    "valid_steps": 50,
+    "warmup_steps": 0,
+    "num_train_steps": 500,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "msp_trn",
+            "db": [
+                "/data7/emobert/exp/evaluation/MSP/txt_db/{}/trn_emowords_sentiword.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"     
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "msp_trnval",
+            "db": [
+                "/data7/emobert/exp/evaluation/MSP/txt_db/{}/val_emowords_sentiword.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"     
+            ],
+            "mix_ratio": [
+                2,
+                1,
+                1,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "msp_val",
+            "db": [
+                "/data7/emobert/exp/evaluation/MSP/txt_db/{}/val_emowords_sentiword.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"             
+            ]
+        },
+        {
+            "name": "msp_tst",
+            "db": [
+                "/data7/emobert/exp/evaluation/MSP/txt_db/{}/tst_emowords_sentiword.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "melm",
+                "mrfr",
+                "mrckl"           
+            ]
+        }
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/pretrain-task-msp_wav2vec-base-2gpu_3tasks.json b/code/uniter3m/config/pretrain-task-msp_wav2vec-base-2gpu_3tasks.json
new file mode 100644
index 0000000..cdace4a
--- /dev/null
+++ b/code/uniter3m/config/pretrain-task-msp_wav2vec-base-2gpu_3tasks.json
@@ -0,0 +1,115 @@
+{
+    "output_dir": "/data7/emobert/exp/task_pretrain/xxx",
+    "model_config": "config/uniter-base.json",
+    "checkpoint": "/data7/emobert/exp/pretrain/xxx/xxx.pt",
+    "mrm_prob": 0.15,
+    "itm_neg_prob": 0.5,
+    "itm_ot_lambda": 0.1,
+    "max_txt_len": 30,
+    "conf_th": 0.0,
+    "max_bb": 64,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "val_batch_size": 64,
+    "gradient_accumulation_steps": 2,
+    "learning_rate": 2e-05,
+    "valid_steps": 50,
+    "warmup_steps": 0,
+    "num_train_steps": 500,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_datasets": [
+        {
+            "name": "msp_trn",
+            "db": [
+                "/data7/emobert/exp/evaluation/MSP/txt_db/{}/trn_emowords_emotype.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"
+            ],
+            "mix_ratio": [
+                1,
+                1,
+                1
+            ]
+        },
+        {
+            "name": "msp_trnval",
+            "db": [
+                "/data7/emobert/exp/evaluation/MSP/txt_db/{}/val_emowords_emotype.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"
+            ],
+            "mix_ratio": [
+                1,
+                1,
+                1
+            ]
+        }
+    ],
+    "val_datasets": [
+        {
+            "name": "msp_val",
+            "db": [
+                "/data7/emobert/exp/evaluation/MSP/txt_db/{}/val_emowords_emotype.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"           
+            ]
+        },
+        {
+            "name": "msp_tst",
+            "db": [
+                "/data7/emobert/exp/evaluation/MSP/txt_db/{}/tst_emowords_emotype.db"
+            ],
+            "img": [
+                "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+            ],
+            "speech": [
+                "/data7/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+            ],
+            "tasks": [
+                "itm",
+                "mlm",
+                "msrfr"           
+            ]
+        }
+    ]
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/train-emo-iemocap-openface_wav2vec-base-2gpu-emo.json b/code/uniter3m/config/train-emo-iemocap-openface_wav2vec-base-2gpu-emo.json
new file mode 100644
index 0000000..6bfa9b7
--- /dev/null
+++ b/code/uniter3m/config/train-emo-iemocap-openface_wav2vec-base-2gpu-emo.json
@@ -0,0 +1,53 @@
+{
+    "compressed_db": false,
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks/ckpt/model_step_100000.pt",
+    "model_config": "config/uniter-base.json",
+    "output_dir": "/data7/emobert/exp/finetune/msp_baseon_nomask_movies_v1_uniter_4tasks/",
+    "melm_type_multask": false,
+    "max_txt_len": 30,
+    "conf_th": 0.1,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "inf_batch_size": 64,
+    "cls_num": 7,
+    "cls_dropout": 0.3,
+    "frozen_en_layers": 11,
+    "patience": 5,
+    "gradient_accumulation_steps": 1,
+    "learning_rate": 2e-05,
+    "valid_steps": 100,
+    "num_train_steps": 1000,
+    "warmup_steps": 100,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "full_val": true,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_txt_dbs": [
+        "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/trn_emowords_emotype3.db",
+        "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/val_emowords_emotype3.db"
+    ],
+    "train_img_dbs": [
+        "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+    ],
+    "train_speech_dbs": [
+        "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+    ],
+    "val_txt_db": "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/tst_emowords_emotype3.db",
+    "val_img_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/",
+    "val_speech_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/",
+    "test_txt_db": "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/tst_emowords_emotype3.db",
+    "test_img_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/",
+    "test_speech_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/train-emo-iemocap-openface_wav2vec-base-2gpu-emo_sentiword.json b/code/uniter3m/config/train-emo-iemocap-openface_wav2vec-base-2gpu-emo_sentiword.json
new file mode 100644
index 0000000..78ff2a1
--- /dev/null
+++ b/code/uniter3m/config/train-emo-iemocap-openface_wav2vec-base-2gpu-emo_sentiword.json
@@ -0,0 +1,53 @@
+{
+    "compressed_db": false,
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks/ckpt/model_step_100000.pt",
+    "model_config": "config/uniter-base.json",
+    "output_dir": "/data7/emobert/exp/finetune/msp_baseon_nomask_movies_v1_uniter_4tasks/",
+    "melm_type_multask": false,
+    "max_txt_len": 30,
+    "conf_th": 0.1,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "inf_batch_size": 64,
+    "cls_num": 7,
+    "cls_dropout": 0.3,
+    "frozen_en_layers": 11,
+    "patience": 5,
+    "gradient_accumulation_steps": 1,
+    "learning_rate": 2e-05,
+    "valid_steps": 100,
+    "num_train_steps": 1000,
+    "warmup_steps": 100,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "full_val": true,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_txt_dbs": [
+        "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/trn_emowords_sentiword.db",
+        "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/val_emowords_sentiword.db"
+    ],
+    "train_img_dbs": [
+        "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+    ],
+    "train_speech_dbs": [
+        "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+    ],
+    "val_txt_db": "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/tst_emowords_sentiword.db",
+    "val_img_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/",
+    "val_speech_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/",
+    "test_txt_db": "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/tst_emowords_sentiword.db",
+    "test_img_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/",
+    "test_speech_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/train-emo-iemocap-openface_wav2vec-base-2gpu.json b/code/uniter3m/config/train-emo-iemocap-openface_wav2vec-base-2gpu.json
new file mode 100644
index 0000000..6bfa9b7
--- /dev/null
+++ b/code/uniter3m/config/train-emo-iemocap-openface_wav2vec-base-2gpu.json
@@ -0,0 +1,53 @@
+{
+    "compressed_db": false,
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks/ckpt/model_step_100000.pt",
+    "model_config": "config/uniter-base.json",
+    "output_dir": "/data7/emobert/exp/finetune/msp_baseon_nomask_movies_v1_uniter_4tasks/",
+    "melm_type_multask": false,
+    "max_txt_len": 30,
+    "conf_th": 0.1,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "inf_batch_size": 64,
+    "cls_num": 7,
+    "cls_dropout": 0.3,
+    "frozen_en_layers": 11,
+    "patience": 5,
+    "gradient_accumulation_steps": 1,
+    "learning_rate": 2e-05,
+    "valid_steps": 100,
+    "num_train_steps": 1000,
+    "warmup_steps": 100,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "full_val": true,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_txt_dbs": [
+        "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/trn_emowords_emotype3.db",
+        "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/val_emowords_emotype3.db"
+    ],
+    "train_img_dbs": [
+        "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+    ],
+    "train_speech_dbs": [
+        "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+    ],
+    "val_txt_db": "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/tst_emowords_emotype3.db",
+    "val_img_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/",
+    "val_speech_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/",
+    "test_txt_db": "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/tst_emowords_emotype3.db",
+    "test_img_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/",
+    "test_speech_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_db_3mean/"
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/train-emo-iemocap-openface_wav2vecasr-base-2gpu.json b/code/uniter3m/config/train-emo-iemocap-openface_wav2vecasr-base-2gpu.json
new file mode 100644
index 0000000..e3ed079
--- /dev/null
+++ b/code/uniter3m/config/train-emo-iemocap-openface_wav2vecasr-base-2gpu.json
@@ -0,0 +1,53 @@
+{
+    "compressed_db": false,
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks/ckpt/model_step_100000.pt",
+    "model_config": "config/uniter-base.json",
+    "output_dir": "/data7/emobert/exp/finetune/msp_baseon_nomask_movies_v1_uniter_4tasks/",
+    "melm_type_multask": false,
+    "max_txt_len": 30,
+    "conf_th": 0.1,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "inf_batch_size": 64,
+    "cls_num": 7,
+    "cls_dropout": 0.3,
+    "frozen_en_layers": 11,
+    "patience": 5,
+    "gradient_accumulation_steps": 1,
+    "learning_rate": 2e-05,
+    "valid_steps": 100,
+    "num_train_steps": 1000,
+    "warmup_steps": 100,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "full_val": true,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_txt_dbs": [
+        "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/trn_emowords_emotype.db",
+        "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/val_emowords_emotype.db"
+    ],
+    "train_img_dbs": [
+        "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/"
+    ],
+    "train_speech_dbs": [
+        "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_asr_db_3mean/"
+    ],
+    "val_txt_db": "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/tst_emowords_emotype.db",
+    "val_img_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/",
+    "val_speech_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_asr_db_3mean/",
+    "test_txt_db": "/data7/emobert/exp/evaluation/IEMOCAP/txt_db/{}/tst_emowords_emotype.db",
+    "test_img_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/denseface_openface_iemocap_mean_std_torch/img_db/fc/",
+    "test_speech_db": "/data7/emobert/exp/evaluation/IEMOCAP/feature/wav2vec_asr_db_3mean/"
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/train-emo-meld-openface_wav2vec-base-2gpu-emo.json b/code/uniter3m/config/train-emo-meld-openface_wav2vec-base-2gpu-emo.json
new file mode 100644
index 0000000..ef57c45
--- /dev/null
+++ b/code/uniter3m/config/train-emo-meld-openface_wav2vec-base-2gpu-emo.json
@@ -0,0 +1,53 @@
+{
+    "compressed_db": false,
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1v2_uniter_4tasks_lr5e5_bs1024_faceth0.5/ckpt/model_step_5000.pt",
+    "model_config": "config/uniter-base-emoword_nomultitask.json",
+    "output_dir": "/data7/MEmoBert/emobert/exp/evaluation/MELD/finetune/xxx",
+    "max_txt_len": 30,
+    "conf_th": 0.0,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "inf_batch_size": 64,
+    "cls_num": 7,
+    "cls_dropout": 0.1,
+    "frozen_en_layers": 0,
+    "patience": 5,
+    "gradient_accumulation_steps": 1,
+    "learning_rate": 2e-05,
+    "valid_steps": 100,
+    "num_train_steps": 1000,
+    "warmup_steps": 100,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "full_val": true,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_txt_dbs": [
+        "/data7/emobert/exp/evaluation/MELD/txt_db/{}/train_emowords_emotype3.db",
+        "/data7/emobert/exp/evaluation/MELD/txt_db/{}/val_emowords_emotype3.db"
+    ],
+    "train_img_dbs": [
+        "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+    ],
+    "train_speech_dbs": [
+        "/data7/MEmoBert/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+    ],
+    "val_txt_db": "/data7/emobert/exp/evaluation/MELD/txt_db/{}/test_emowords_emotype3.db",
+    "val_img_db": "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/",
+    "val_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/",
+    "test_txt_db": "/data7/emobert/exp/evaluation/MELD/txt_db/{}/test_emowords_emotype3.db",
+    "test_img_db": "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/",
+    "test_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/train-emo-meld-openface_wav2vec-base-2gpu-emo_sentiword.json b/code/uniter3m/config/train-emo-meld-openface_wav2vec-base-2gpu-emo_sentiword.json
new file mode 100644
index 0000000..c4e413d
--- /dev/null
+++ b/code/uniter3m/config/train-emo-meld-openface_wav2vec-base-2gpu-emo_sentiword.json
@@ -0,0 +1,53 @@
+{
+    "compressed_db": false,
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1v2_uniter_4tasks_lr5e5_bs1024_faceth0.5/ckpt/model_step_5000.pt",
+    "model_config": "config/uniter-base-emoword_nomultitask.json",
+    "output_dir": "/data7/MEmoBert/emobert/exp/evaluation/MELD/finetune/xxx",
+    "max_txt_len": 30,
+    "conf_th": 0.0,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "inf_batch_size": 64,
+    "cls_num": 7,
+    "cls_dropout": 0.1,
+    "frozen_en_layers": 0,
+    "patience": 5,
+    "gradient_accumulation_steps": 1,
+    "learning_rate": 2e-05,
+    "valid_steps": 100,
+    "num_train_steps": 1000,
+    "warmup_steps": 100,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "full_val": true,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_txt_dbs": [
+        "/data7/emobert/exp/evaluation/MELD/txt_db/{}/train_emowords_sentiword.db",
+        "/data7/emobert/exp/evaluation/MELD/txt_db/{}/val_emowords_sentiword.db"
+    ],
+    "train_img_dbs": [
+        "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+    ],
+    "train_speech_dbs": [
+        "/data7/MEmoBert/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+    ],
+    "val_txt_db": "/data7/emobert/exp/evaluation/MELD/txt_db/{}/test_emowords_sentiword.db",
+    "val_img_db": "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/",
+    "val_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/",
+    "test_txt_db": "/data7/emobert/exp/evaluation/MELD/txt_db/{}/test_emowords_sentiword.db",
+    "test_img_db": "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/",
+    "test_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/train-emo-meld-openface_wav2vec-base-2gpu.json b/code/uniter3m/config/train-emo-meld-openface_wav2vec-base-2gpu.json
new file mode 100644
index 0000000..77f4239
--- /dev/null
+++ b/code/uniter3m/config/train-emo-meld-openface_wav2vec-base-2gpu.json
@@ -0,0 +1,53 @@
+{
+    "compressed_db": false,
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1v2_uniter_4tasks_lr5e5_bs1024_faceth0.5/ckpt/model_step_5000.pt",
+    "model_config": "config/uniter-base-emoword_nomultitask.json",
+    "output_dir": "/data7/MEmoBert/emobert/exp/evaluation/MELD/finetune/xxx",
+    "max_txt_len": 30,
+    "conf_th": 0.0,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "inf_batch_size": 64,
+    "cls_num": 7,
+    "cls_dropout": 0.1,
+    "frozen_en_layers": 0,
+    "patience": 5,
+    "gradient_accumulation_steps": 1,
+    "learning_rate": 2e-05,
+    "valid_steps": 100,
+    "num_train_steps": 1000,
+    "warmup_steps": 100,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "full_val": true,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_txt_dbs": [
+        "/data7/emobert/exp/evaluation/MELD/txt_db/{}/train_emowords_emotype.db",
+        "/data7/emobert/exp/evaluation/MELD/txt_db/{}/val_emowords_emotype.db"
+    ],
+    "train_img_dbs": [
+        "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+    ],
+    "train_speech_dbs": [
+        "/data7/MEmoBert/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+    ],
+    "val_txt_db": "/data7/emobert/exp/evaluation/MELD/txt_db/{}/test_emowords_emotype.db",
+    "val_img_db": "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/",
+    "val_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/",
+    "test_txt_db": "/data7/emobert/exp/evaluation/MELD/txt_db/{}/test_emowords_emotype.db",
+    "test_img_db": "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/",
+    "test_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MELD/feature/wav2vec_db_3mean/"
+
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/train-emo-meld-openface_wav2vecasr-base-2gpu.json b/code/uniter3m/config/train-emo-meld-openface_wav2vecasr-base-2gpu.json
new file mode 100644
index 0000000..699a250
--- /dev/null
+++ b/code/uniter3m/config/train-emo-meld-openface_wav2vecasr-base-2gpu.json
@@ -0,0 +1,53 @@
+{
+    "compressed_db": false,
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1v2_uniter_4tasks_lr5e5_bs1024_faceth0.5/ckpt/model_step_5000.pt",
+    "model_config": "config/uniter-base-emoword_nomultitask.json",
+    "output_dir": "/data7/MEmoBert/emobert/exp/evaluation/MELD/finetune/xxx",
+    "max_txt_len": 30,
+    "conf_th": 0.0,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "inf_batch_size": 64,
+    "cls_num": 7,
+    "cls_dropout": 0.1,
+    "frozen_en_layers": 0,
+    "patience": 5,
+    "gradient_accumulation_steps": 1,
+    "learning_rate": 2e-05,
+    "valid_steps": 100,
+    "num_train_steps": 1000,
+    "warmup_steps": 100,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "full_val": true,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_txt_dbs": [
+        "/data7/emobert/exp/evaluation/MELD/txt_db/{}/train_emowords_emotype.db",
+        "/data7/emobert/exp/evaluation/MELD/txt_db/{}/val_emowords_emotype.db"
+    ],
+    "train_img_dbs": [
+        "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/"
+    ],
+    "train_speech_dbs": [
+        "/data7/MEmoBert/emobert/exp/evaluation/MELD/feature/wav2vec_asr_db_3mean/"
+    ],
+    "val_txt_db": "/data7/emobert/exp/evaluation/MELD/txt_db/{}/test_emowords_emotype.db",
+    "val_img_db": "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/",
+    "val_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MELD/feature/wav2vec_asr_db_3mean/",
+    "test_txt_db": "/data7/emobert/exp/evaluation/MELD/txt_db/{}/test_emowords_emotype.db",
+    "test_img_db": "/data7/emobert/exp/evaluation/MELD/feature/denseface_openface_meld_mean_std_torch/img_db/fc/",
+    "test_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MELD/feature/wav2vec_asr_db_3mean/"
+
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/train-emo-msp-openface_wav2vec-base-2gpu-emo.json b/code/uniter3m/config/train-emo-msp-openface_wav2vec-base-2gpu-emo.json
new file mode 100644
index 0000000..87d4aac
--- /dev/null
+++ b/code/uniter3m/config/train-emo-msp-openface_wav2vec-base-2gpu-emo.json
@@ -0,0 +1,52 @@
+{
+    "compressed_db": false,
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks/ckpt/model_step_100000.pt",
+    "model_config": "config/uniter-base.json",
+    "output_dir": "/data7/emobert/exp/finetune/msp_baseon_nomask_movies_v1_uniter_4tasks/",
+    "max_txt_len": 30,
+    "conf_th": 0.1,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "inf_batch_size": 64,
+    "cls_num": 7,
+    "cls_dropout": 0.3,
+    "frozen_en_layers": 11,
+    "patience": 2,
+    "gradient_accumulation_steps": 1,
+    "learning_rate": 2e-05,
+    "valid_steps": 100,
+    "num_train_steps": 1000,
+    "warmup_steps": 100,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "full_val": true,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_txt_dbs": [
+        "/data7/emobert/exp/evaluation/MSP/txt_db/{}/trn_emowords_emotype3.db",
+        "/data7/emobert/exp/evaluation/MSP/txt_db/{}/val_emowords_emotype3.db"
+    ],
+    "train_img_dbs": [
+        "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+    ],
+    "train_speech_dbs": [
+        "/data7/MEmoBert/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+    ],
+    "val_txt_db": "/data7/emobert/exp/evaluation/MSP/txt_db/{}/tst_emowords_emotype3.db",
+    "val_img_db": "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/",
+    "val_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/",
+    "test_txt_db": "/data7/emobert/exp/evaluation/MSP/txt_db/{}/tst_emowords_emotype3.db",
+    "test_img_db": "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/",
+    "test_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/train-emo-msp-openface_wav2vec-base-2gpu-emo_sentiword.json b/code/uniter3m/config/train-emo-msp-openface_wav2vec-base-2gpu-emo_sentiword.json
new file mode 100644
index 0000000..178bbc5
--- /dev/null
+++ b/code/uniter3m/config/train-emo-msp-openface_wav2vec-base-2gpu-emo_sentiword.json
@@ -0,0 +1,52 @@
+{
+    "compressed_db": false,
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks/ckpt/model_step_100000.pt",
+    "model_config": "config/uniter-base.json",
+    "output_dir": "/data7/emobert/exp/finetune/msp_baseon_nomask_movies_v1_uniter_4tasks/",
+    "max_txt_len": 30,
+    "conf_th": 0.1,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "inf_batch_size": 64,
+    "cls_num": 7,
+    "cls_dropout": 0.3,
+    "frozen_en_layers": 11,
+    "patience": 2,
+    "gradient_accumulation_steps": 1,
+    "learning_rate": 2e-05,
+    "valid_steps": 100,
+    "num_train_steps": 1000,
+    "warmup_steps": 100,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "full_val": true,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_txt_dbs": [
+        "/data7/emobert/exp/evaluation/MSP/txt_db/{}/trn_emowords_sentiword.db",
+        "/data7/emobert/exp/evaluation/MSP/txt_db/{}/val_emowords_sentiword.db"
+    ],
+    "train_img_dbs": [
+        "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+    ],
+    "train_speech_dbs": [
+        "/data7/MEmoBert/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+    ],
+    "val_txt_db": "/data7/emobert/exp/evaluation/MSP/txt_db/{}/tst_emowords_sentiword.db",
+    "val_img_db": "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/",
+    "val_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/",
+    "test_txt_db": "/data7/emobert/exp/evaluation/MSP/txt_db/{}/tst_emowords_sentiword.db",
+    "test_img_db": "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/",
+    "test_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/train-emo-msp-openface_wav2vec-base-2gpu.json b/code/uniter3m/config/train-emo-msp-openface_wav2vec-base-2gpu.json
new file mode 100644
index 0000000..87d4aac
--- /dev/null
+++ b/code/uniter3m/config/train-emo-msp-openface_wav2vec-base-2gpu.json
@@ -0,0 +1,52 @@
+{
+    "compressed_db": false,
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks/ckpt/model_step_100000.pt",
+    "model_config": "config/uniter-base.json",
+    "output_dir": "/data7/emobert/exp/finetune/msp_baseon_nomask_movies_v1_uniter_4tasks/",
+    "max_txt_len": 30,
+    "conf_th": 0.1,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "inf_batch_size": 64,
+    "cls_num": 7,
+    "cls_dropout": 0.3,
+    "frozen_en_layers": 11,
+    "patience": 2,
+    "gradient_accumulation_steps": 1,
+    "learning_rate": 2e-05,
+    "valid_steps": 100,
+    "num_train_steps": 1000,
+    "warmup_steps": 100,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "full_val": true,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_txt_dbs": [
+        "/data7/emobert/exp/evaluation/MSP/txt_db/{}/trn_emowords_emotype3.db",
+        "/data7/emobert/exp/evaluation/MSP/txt_db/{}/val_emowords_emotype3.db"
+    ],
+    "train_img_dbs": [
+        "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+    ],
+    "train_speech_dbs": [
+        "/data7/MEmoBert/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+    ],
+    "val_txt_db": "/data7/emobert/exp/evaluation/MSP/txt_db/{}/tst_emowords_emotype3.db",
+    "val_img_db": "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/",
+    "val_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/",
+    "test_txt_db": "/data7/emobert/exp/evaluation/MSP/txt_db/{}/tst_emowords_emotype3.db",
+    "test_img_db": "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/",
+    "test_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MSP/feature/wav2vec_db_3mean/"
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/train-emo-msp-openface_wav2vecasr-base-2gpu.json b/code/uniter3m/config/train-emo-msp-openface_wav2vecasr-base-2gpu.json
new file mode 100644
index 0000000..69ab373
--- /dev/null
+++ b/code/uniter3m/config/train-emo-msp-openface_wav2vecasr-base-2gpu.json
@@ -0,0 +1,52 @@
+{
+    "compressed_db": false,
+    "checkpoint": "/data7/emobert/exp/pretrain/nomask_movies_v1_uniter_4tasks/ckpt/model_step_100000.pt",
+    "model_config": "config/uniter-base.json",
+    "output_dir": "/data7/emobert/exp/finetune/msp_baseon_nomask_movies_v1_uniter_4tasks/",
+    "max_txt_len": 30,
+    "conf_th": 0.1,
+    "max_bb": 36,
+    "min_bb": 10,
+    "num_bb": 36,
+    "IMG_DIM": 342,
+    "train_batch_size": 64,
+    "inf_batch_size": 64,
+    "cls_num": 7,
+    "cls_dropout": 0.3,
+    "frozen_en_layers": 11,
+    "patience": 2,
+    "gradient_accumulation_steps": 1,
+    "learning_rate": 2e-05,
+    "valid_steps": 100,
+    "num_train_steps": 1000,
+    "warmup_steps": 100,
+    "optim": "adamw",
+    "betas": [
+        0.9,
+        0.98
+    ],
+    "dropout": 0.1,
+    "weight_decay": 0.01,
+    "grad_norm": 5.0,
+    "seed": 42,
+    "full_val": true,
+    "fp16": true,
+    "n_workers": 2,
+    "pin_mem": true,
+    "train_txt_dbs": [
+        "/data7/emobert/exp/evaluation/MSP/txt_db/{}/trn_emowords_emotype.db",
+        "/data7/emobert/exp/evaluation/MSP/txt_db/{}/val_emowords_emotype.db"
+    ],
+    "train_img_dbs": [
+        "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/"
+    ],
+    "train_speech_dbs": [
+        "/data7/MEmoBert/emobert/exp/evaluation/MSP/feature/wav2vec_asr_db_3mean/"
+    ],
+    "val_txt_db": "/data7/emobert/exp/evaluation/MSP/txt_db/{}/tst_emowords_emotype.db",
+    "val_img_db": "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/",
+    "val_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MSP/feature/wav2vec_asr_db_3mean/",
+    "test_txt_db": "/data7/emobert/exp/evaluation/MSP/txt_db/{}/tst_emowords_emotype.db",
+    "test_img_db": "/data7/emobert/exp/evaluation/MSP/feature/denseface_openface_msp_mean_std_torch/img_db/fc/",
+    "test_speech_db": "/data7/MEmoBert/emobert/exp/evaluation/MSP/feature/wav2vec_asr_db_3mean/"
+}
\ No newline at end of file
diff --git a/code/uniter3m/config/uniter-base-emoword_multitask.json b/code/uniter3m/config/uniter-base-emoword_multitask.json
index e012205..19bebb3 100644
--- a/code/uniter3m/config/uniter-base-emoword_multitask.json
+++ b/code/uniter3m/config/uniter-base-emoword_multitask.json
@@ -6,13 +6,16 @@
   "initializer_range": 0.02,
   "intermediate_size": 3072,
   "max_position_embeddings": 512,
+  "img_max_position_embeddings": 64,
+  "speech_max_position_embeddings": 128,
   "num_attention_heads": 12,
   "num_hidden_layers": 12,
   "type_vocab_size": 2,
-  "melm_type_emo_size": 4,
+  "speech_visual_use_same_type": true,
+  "melm_type_emo_size": 3,
   "use_emo_type_emb": false,
   "melm_multitask": true,
   "melm_multitask_rate": 1.0,
-  "melm_emo_category_size": 4,
+  "melm_emo_category_size": 3,
   "vocab_size": 30522
 }
diff --git a/code/uniter3m/config/uniter-base-emoword_nomultitask.json b/code/uniter3m/config/uniter-base-emoword_nomultitask.json
index 4a8c3a4..8dcba57 100644
--- a/code/uniter3m/config/uniter-base-emoword_nomultitask.json
+++ b/code/uniter3m/config/uniter-base-emoword_nomultitask.json
@@ -11,6 +11,7 @@
   "num_attention_heads": 12,
   "num_hidden_layers": 12,
   "type_vocab_size": 2,
+  "speech_visual_use_same_type": true,
   "melm_type_emo_size": 4,
   "use_emo_type_emb": false,
   "melm_multitask": false,
diff --git a/code/uniter3m/data/data.py b/code/uniter3m/data/data.py
index aa2e72c..2092954 100644
--- a/code/uniter3m/data/data.py
+++ b/code/uniter3m/data/data.py
@@ -21,24 +21,20 @@ import msgpack
 import msgpack_numpy
 msgpack_numpy.patch()
 
+# from uniter 
+from code.uniter.data.data import TxtLmdb, TxtTokLmdb, \
+        get_ids_and_lens, _check_distributed, _fp16_to_fp32, pad_tensors, ConcatDatasetWithLens
 
-def _fp16_to_fp32(feat_dict):
-    out = {k: arr.astype(np.float32)
-           if arr.dtype == np.float16 else arr
-           for k, arr in feat_dict.items()}
-    return out
-
-def _check_distributed():
+@contextmanager
+def open_lmdb(db_dir, readonly=False):
+    db = TxtLmdb(db_dir, readonly)
     try:
-        dist = hvd.size() != hvd.local_size()
-    except ValueError:
-        # not using horovod
-        dist = False
-    return dist
+        yield db
+    finally:
+        del db
 
 class DetectFeatLmdb(object):
-    def __init__(self, img_dir, conf_th=0.2, max_bb=100, min_bb=10,
-                 compress=False):
+    def __init__(self, img_dir, conf_th=0.2, max_bb=100, min_bb=10, compress=False):
         self.img_dir = img_dir
         # read the generated json file
         db_name = f'feat_th{conf_th}_max{max_bb}_min{min_bb}'
@@ -86,113 +82,9 @@ class DetectFeatLmdb(object):
                 img_dump = {'features': img_dump['features']}
         else:
             img_dump = msgpack.loads(dump, raw=False)
-        img_feat = torch.tensor(img_dump['features'][:nbb, :]).float()
+        img_feat = torch.tensor(np.array(img_dump['features'])[:nbb, :]).float()
         return img_feat
 
-
-@contextmanager
-def open_lmdb(db_dir, readonly=False):
-    db = TxtLmdb(db_dir, readonly)
-    try:
-        yield db
-    finally:
-        del db
-
-class TxtLmdb(object):
-    def __init__(self, db_dir, readonly=True):
-        self.readonly = readonly
-        if readonly:
-            # training
-            self.env = lmdb.open(db_dir,
-                                 readonly=True, create=False,
-                                 readahead=not _check_distributed())
-            self.txn = self.env.begin(buffers=True)
-            self.write_cnt = None
-        else:
-            # prepro
-            self.env = lmdb.open(db_dir, readonly=False, create=True,
-                                 map_size=4 * 1024**4)
-            self.txn = self.env.begin(write=True)
-            self.write_cnt = 0
-
-    def __del__(self):
-        if self.write_cnt:
-            self.txn.commit()
-        self.env.close()
-
-    def __getitem__(self, key):
-        return msgpack.loads(decompress(self.txn.get(key.encode('utf-8'))),
-                             raw=False)
-
-    def __setitem__(self, key, value):
-        # NOTE: not thread safe
-        if self.readonly:
-            raise ValueError('readonly text DB')
-        ret = self.txn.put(key.encode('utf-8'),
-                           compress(msgpack.dumps(value, use_bin_type=True)))
-        self.write_cnt += 1
-        if self.write_cnt % 1000 == 0:
-            self.txn.commit()
-            self.txn = self.env.begin(write=True)
-            self.write_cnt = 0
-        return ret
-
-
-class TxtTokLmdb(object):
-    def __init__(self, db_dir, max_txt_len=60):
-        if max_txt_len == -1:
-            # 
-            self.id2len = json.load(open(f'{db_dir}/id2len.json'))
-        else:
-            self.id2len = {
-                id_: len_
-                for id_, len_ in json.load(open(f'{db_dir}/id2len.json')).items()
-                if len_ <= max_txt_len
-            }
-        self.db_dir = db_dir
-        self.db = TxtLmdb(db_dir, readonly=True)
-        meta = json.load(open(f'{db_dir}/meta.json', 'r'))
-        self.cls_ = meta['CLS']
-        self.sep = meta['SEP']
-        self.mask = meta['MASK']
-        self.v_range = meta['v_range']
-
-    def __getitem__(self, id_):
-        txt_dump = self.db[id_]
-        return txt_dump
-
-    def combine_inputs(self, *inputs):
-        input_ids = [self.cls_]
-        for ids in inputs:
-            input_ids.extend(ids + [self.sep])
-        return torch.tensor(input_ids)
-
-    @property
-    def txt2img(self):
-        txt2img = json.load(open(f'{self.db_dir}/txt2img.json'))
-        return txt2img
-
-    @property
-    def img2txts(self):
-        img2txts = json.load(open(f'{self.db_dir}/img2txts.json'))
-        return img2txts
-
-
-def get_ids_and_lens(db):
-    assert isinstance(db, TxtTokLmdb)
-    lens = []
-    ids = []
-    # Modify by zjm: hvd.rank() is current process Id and hvd.size() is total gpus
-    # for example, gpu0: 0~1/4L, gpu2: 1/4L~2/4L, gpu3: 2/4L~3/4L
-    # Then the hvd.allgather() can restore original sequential order.
-    splice_size = len(list(db.id2len.keys())) / hvd.size()
-    start = int(hvd.rank() * splice_size)
-    end = int(hvd.rank() * splice_size + splice_size)
-    for id_ in list(db.id2len.keys())[start:end]:
-        lens.append(db.id2len[id_])
-        ids.append(id_)
-    return lens, ids
-
 class DetectFeatTxtTokDataset(Dataset):
     def __init__(self, txt_db, img_db=None, speech_db=None):
         assert isinstance(txt_db, TxtTokLmdb)
@@ -207,9 +99,11 @@ class DetectFeatTxtTokDataset(Dataset):
         self.lens = copy.deepcopy(self.txt_lens)
         txt2img = txt_db.txt2img
         if img_db:
+            print('[Debug in data] add the img lens')
             self.lens = [tl + self.img_db.name2nbb[txt2img[id_]]
                      for tl, id_ in zip(self.lens, self.ids)]
         if speech_db:
+            print('[Debug in data] add the speech lens')
             self.lens = [tl + self.speech_db.name2nbb[txt2img[id_]]
                      for tl, id_ in zip(self.lens, self.ids)]
 
@@ -242,22 +136,6 @@ class DetectFeatTxtTokDataset(Dataset):
         num_bb = speech_feat.size(0)
         return speech_feat, num_bb
 
-
-def pad_tensors(tensors, lens=None, pad=0):
-    """B x [T, ...]"""
-    if lens is None:
-        lens = [t.size(0) for t in tensors]
-    max_len = max(lens)
-    bs = len(tensors)
-    hid = tensors[0].size(-1)
-    dtype = tensors[0].dtype
-    output = torch.zeros(bs, max_len, hid, dtype=dtype)
-    if pad:
-        output.data.fill_(pad)
-    for i, (t, l) in enumerate(zip(tensors, lens)):
-        output.data[i, :l, ...] = t.data
-    return output
-
 def get_gather_index(txt_lens, num_bbs, num_frames, batch_size, max_len, out_size):
     '''
     Jinming modify this for multimodalies.
@@ -272,28 +150,18 @@ def get_gather_index(txt_lens, num_bbs, num_frames, batch_size, max_len, out_siz
         for i, (tl, nbb) in enumerate(zip(txt_lens, num_frames)):
             gather_index.data[i, tl:tl+nbb] = torch.arange(max_len, max_len+nbb,
                                                         dtype=torch.long).data
-    elif num_bbs is not None and num_frames is not  None:
+    elif num_bbs is not None and num_frames is not None:
+        max_bb = max(num_bbs)
         for i, (tl, nbb, nframe) in enumerate(zip(txt_lens, num_bbs, num_frames)):
-            gather_index.data[i, tl:tl+nbb+nframe] = torch.arange(max_len, max_len+nbb+nframe,
+            # a bug 
+            gather_index.data[i, tl:tl+nbb] = torch.arange(max_len, max_len+nbb,
                                                         dtype=torch.long).data
+            gather_index.data[i, tl+nbb:tl+nbb+nframe] = torch.arange(max_len+max_bb, max_len+max_bb+nframe,
+                                                        dtype=torch.long).data
+    else:
+        print('[Error] Error in gather index')
     return gather_index
 
-class ConcatDatasetWithLens(ConcatDataset):
-    """ A thin wrapper on pytorch concat dataset for lens batching """
-    def __init__(self, datasets):
-        super().__init__(datasets)
-        self.lens = [l for dset in datasets for l in dset.lens]
-
-    def __getattr__(self, name):
-        return self._run_method_on_all_dsets(name)
-
-    def _run_method_on_all_dsets(self, name):
-        def run_all(*args, **kwargs):
-            return [dset.__getattribute__(name)(*args, **kwargs)
-                    for dset in self.datasets]
-        return run_all
-
-
 class ImageLmdbGroup(object):
     def __init__(self, conf_th, max_bb, min_bb, compress):
         self.path2imgdb = {}
diff --git a/code/uniter3m/data/emocls.py b/code/uniter3m/data/emocls.py
index e22cdcf..891509a 100644
--- a/code/uniter3m/data/emocls.py
+++ b/code/uniter3m/data/emocls.py
@@ -9,11 +9,11 @@ import torch
 import numpy as np
 from torch.nn.utils.rnn import pad_sequence
 from toolz.sandbox import unzip
-from code.uniter3m.data.data import (DetectFeatTxtTokDataset, DetectFeatLmdb, TxtTokLmdb, \
+from code.uniter3m.data.data import (DetectFeatTxtTokDataset, TxtTokLmdb, \
                    pad_tensors, get_gather_index)
                    
 class EmoCLsDataset(DetectFeatTxtTokDataset):
-    def __init__(self, txt_db, img_db, speech_db=None):
+    def __init__(self, txt_db, img_db=None, speech_db=None):
         assert isinstance(txt_db, TxtTokLmdb)
         super().__init__(txt_db, img_db, speech_db)
         self.img_shape = None
@@ -34,20 +34,31 @@ class EmoCLsDataset(DetectFeatTxtTokDataset):
         # text input
         input_ids = example['input_ids']
         input_ids = torch.tensor([self.txt_db.cls_] + input_ids + [self.txt_db.sep])
-        img_feat, num_bb = self._get_img_feat(example['img_fname'], self.img_shape)
-        self.img_shape = img_feat.shape[1:]
-        attn_masks = torch.ones(len(input_ids) + num_bb, dtype=torch.long)
+        attn_masks = torch.ones(len(input_ids), dtype=torch.long)
 
-        if self.speech_db:
+        if self.img_db is not None:
+            # print(f'[Debug] item {i} img is not None')
+            img_feat, num_bb = self._get_img_feat(example['img_fname'], self.img_shape)
+            img_attn_masks = torch.ones(num_bb, dtype=torch.long)
+            self.img_shape = img_feat.shape[1:]
+            attn_masks = torch.cat((attn_masks, img_attn_masks))
+        else:
+            # print(f'[Debug] item img {i} is None')
+            img_feat = None
+        
+        if self.speech_db is not None:
+            # print(f'[Debug] item {i} speech is not None')
             speech_feat, num_frame = self._get_speech_feat(example['img_fname'])
             speech_attn_masks = torch.ones(num_frame, dtype=torch.long)
             attn_masks = torch.cat((attn_masks, speech_attn_masks))
+            # print('[Debug] item {} speech attn mask {} and final attn mask {}'.format(i, speech_attn_masks.shape, attn_masks.shape))
         else:
-            speech_feat  = None
+            speech_feat = None
+
         # for visualization
-        img_frame_name = example['img_fname']
+        frame_name = example['img_fname']
         # print("[Debug empty] txt {} img {}".format(len(input_ids), num_bb))
-        return input_ids, img_feat, speech_feat, attn_masks, target, img_frame_name
+        return input_ids, img_feat, speech_feat, attn_masks, target, frame_name
 
 def emocls_collate(inputs):
     """
@@ -60,24 +71,21 @@ def emocls_collate(inputs):
     :num_bbs      list of [num_bb], real num_bbs
     :attn_masks   (n, max_{L + num_bb}) padded with 0
     """
-    (input_ids, img_feats, speech_feats, attn_masks, targets, batch_img_frame_names) = map(list, unzip(inputs))
+    (input_ids, img_feats, speech_feats, attn_masks, targets, batch_frame_names) = map(list, unzip(inputs))
 
     # text batches
     txt_lens = [i.size(0) for i in input_ids]
     input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)
     position_ids = torch.arange(0, input_ids.size(1), dtype=torch.long
                                 ).unsqueeze(0)
-    # image batches
-    num_bbs = [f.size(0) for f in img_feats]
-    img_feat = pad_tensors(img_feats, num_bbs) # (n, max_num_nbb, dim)
-    img_position_ids = torch.arange(0, img_feat.size(1), dtype=torch.long).unsqueeze(0)
     attn_masks = pad_sequence(attn_masks, batch_first=True, padding_value=0)
 
     if img_feats[0] is not None:
         ## image batches
         num_bbs = [f.size(0) for f in img_feats]
         img_feat = pad_tensors(img_feats, num_bbs)
-        img_position_ids = torch.arange(0, max(num_bbs), dtype=torch.long).unsqueeze(0)      
+        # print('[Debug] batch padding img input {}'.format(img_feat.shape)) # (n, max_num_nbb, dim)
+        img_position_ids = torch.arange(0, img_feat.size(1), dtype=torch.long).unsqueeze(0)      
     else:
         img_feat, num_bbs, img_position_ids = None, None, None
     
@@ -85,8 +93,8 @@ def emocls_collate(inputs):
         ## speech batches
         num_frames = [f.size(0) for f in speech_feats]
         speech_feat = pad_tensors(speech_feats, num_frames)
-        # print('[Debug] the batch input {}'.format(img_feat.shape)) # (n, max_num_nbb, dim)
-        speech_position_ids = torch.arange(0, max(num_frames), dtype=torch.long).unsqueeze(0)    
+        # print('[Debug] batch padding speech input {}'.format(speech_feat.shape)) # (n, max_num_frame, dim)
+        speech_position_ids = torch.arange(0, speech_feat.size(1), dtype=torch.long).unsqueeze(0)    
     else:
         speech_feat, num_frames, speech_position_ids = None, None, None
 
@@ -98,15 +106,15 @@ def emocls_collate(inputs):
 
     batch = {'input_ids': input_ids,
              'position_ids': position_ids,
-             'txt_lens': txt_lens,
              'img_feat': img_feat,
              'img_position_ids': img_position_ids,
              'speech_feat': speech_feat,
              'speech_position_ids': speech_position_ids,
-             'img_lens': num_bbs,
              'attn_masks': attn_masks,
              'gather_index': gather_index,
              'targets': targets,
-             'img_frame_names': batch_img_frame_names
+             'txt_lens': txt_lens,
+             'img_lens': num_bbs,
+             'img_frame_names': batch_frame_names
              }
     return batch
\ No newline at end of file
diff --git a/code/uniter3m/data/itm.py b/code/uniter3m/data/itm.py
index 55132cb..a76c8f6 100644
--- a/code/uniter3m/data/itm.py
+++ b/code/uniter3m/data/itm.py
@@ -4,10 +4,7 @@ Licensed under the MIT license.
 
 Itm dataset
 """
-from collections import defaultdict
-import copy
 import random
-
 import torch
 from torch.nn.utils.rnn import pad_sequence
 from toolz.sandbox import unzip
@@ -16,40 +13,8 @@ import numpy as np
 
 from code.uniter3m.data.data import (DetectFeatTxtTokDataset, DetectFeatLmdb, TxtTokLmdb,
                    pad_tensors, get_gather_index, get_ids_and_lens)
-from code.uniter3m.data.sampler import TokenBucketSampler
-
-
-class TokenBucketSamplerForItm(TokenBucketSampler):
-    def __init__(self, dset, *args, **kwargs):
-        super().__init__(dset.lens, *args, **kwargs)
-        self.dset = dset
-
-    def __iter__(self):
-        it = super().__iter__()
-        self.dset.new_epoch()
-        self._lens = self.dset.lens
-        return it
-
-
-def _has_overlap(la, lb):
-    if len(la) < len(lb):
-        la, lb = lb, la
-    s = set(la)
-    return any(b in s for b in lb)
-
-
-def sample_negative(sample_pool, ground_truths, num_sample):
-    """ random and retry 
-    imagenum_sample  image 
-    sample_pool: all_imgs
-    ground_truths:  image-frame
-    num_sample: imagenum_sample  image 
-    """
-    outputs = ground_truths[:1]
-    while _has_overlap(outputs, ground_truths):
-        outputs = random.sample(sample_pool, num_sample)
-    return outputs
-
+# from uniter 
+from code.uniter.data.itm import TokenBucketSamplerForItm, sample_negative
 
 class ItmDataset(DetectFeatTxtTokDataset):
     """ NOTE this Dataset handles distributed training itself
@@ -65,9 +30,9 @@ class ItmDataset(DetectFeatTxtTokDataset):
         self.txt_db = txt_db
         self.img_db = img_db
         self.speech_db = speech_db
-        self.txt_lens, self.ids = get_ids_and_lens(txt_db)
         self.img_shape = None
 
+        self.txt_lens, self.ids = get_ids_and_lens(txt_db)
         self.all_imgs = list(set(txt_db[id_]['img_fname'] for id_ in self.ids))
 
         self.neg_sample_p = neg_sample_p
@@ -88,41 +53,40 @@ class ItmDataset(DetectFeatTxtTokDataset):
             img_fname = super().__getitem__(i)['img_fname']
             if self.labels[i] == 0:
                 img_fname = sample_negative(self.all_imgs, [img_fname], 1)[0]
+
             self.train_imgs.append(img_fname)
-            if self.img_db and self.speech_db is None:
+            if self.img_db is not None and self.speech_db is None:
                 self.lens.append(tl + self.img_db.name2nbb[img_fname])
-            if self.speech_db and self.img_db is None :
+            if self.speech_db is not None and self.img_db is None :
                 self.lens.append(tl + self.speech_db.name2nbb[img_fname])
-            if self.speech_db and self.img_db:
-                self.lens.append(tl + self.speech_db.name2nbb[img_fname] + self.img_db.name2nbb[img_fname])
+            if self.speech_db is not None and self.img_db is not None:
+                self.lens.append(tl + self.img_db.name2nbb[img_fname] + self.speech_db.name2nbb[img_fname])
 
     def __getitem__(self, i):
         # i only the index not the real text-id in txtdb
         example = super().__getitem__(i)
         # labels and negative images should be sampled every epoch
         ground_truth_label = self.labels[i]
-        img_fname = self.train_imgs[i]
         # text input
         input_ids = example['input_ids']
+        # add cls and sep special tokens
         input_ids = self.txt_db.combine_inputs(input_ids)
         attn_masks = torch.ones(len(input_ids), dtype=torch.long)
 
-        img_fname = self.train_imgs[i]
-        if self.img_db:
-            img_feat, num_bb = self._get_img_feat(img_fname, self.img_shape)
+        if self.img_db is not None:
+            img_feat, num_bb = self._get_img_feat(example['img_fname'], self.img_shape)
             self.img_shape = img_feat.shape[1:]
             img_attn_masks = torch.ones(num_bb, dtype=torch.long)
             attn_masks = torch.cat((attn_masks, img_attn_masks))
         else:
             img_feat = None
 
-        if self.speech_db:
+        if self.speech_db is not None:
             speech_feat, num_frame = self._get_speech_feat(example['img_fname'])
             speech_attn_masks = torch.ones(num_frame, dtype=torch.long)
             attn_masks = torch.cat((attn_masks, speech_attn_masks))
         else:
             speech_feat = None
-            
         target = torch.Tensor(1).long()
         target.data.fill_(ground_truth_label)
         return input_ids, img_feat, speech_feat, attn_masks, target
@@ -140,7 +104,7 @@ def itm_collate(inputs):
     if img_feats[0] is not None:
         num_bbs = [f.size(0) for f in img_feats]
         img_feat = pad_tensors(img_feats, num_bbs)
-        img_position_ids = torch.arange(0, max(num_bbs), dtype=torch.long
+        img_position_ids = torch.arange(0, img_feat.size(1), dtype=torch.long
                                     ).unsqueeze(0)
     else:
         img_feat, num_bbs, img_position_ids = None, None, None
@@ -150,7 +114,7 @@ def itm_collate(inputs):
         num_frames = [f.size(0) for f in speech_feats]
         speech_feat = pad_tensors(speech_feats, num_frames)
         # print('[Debug] the batch input {}'.format(img_feat.shape)) # (n, max_num_nbb, dim)
-        speech_position_ids = torch.arange(0, max(num_frames), dtype=torch.long).unsqueeze(0)    
+        speech_position_ids = torch.arange(0, speech_feat.size(1), dtype=torch.long).unsqueeze(0)    
     else:
         speech_feat, num_frames, speech_position_ids = None, None, None
 
@@ -159,6 +123,9 @@ def itm_collate(inputs):
     out_size = attn_masks.size(1)
     gather_index = get_gather_index(txt_lens, num_bbs, num_frames, bs, max_tl, out_size)
 
+    # print(f'[Debug in itm data] text {input_ids.shape} img {img_feat.shape} speech {speech_feat}')
+    # print(f'[Debug in itm data] targets {targets.shape} gather_index {gather_index.shape} attn_masks {attn_masks.shape}')
+
     batch = {'input_ids': input_ids,
              'position_ids': position_ids,
              'img_feat': img_feat,
diff --git a/code/uniter3m/data/melm.py b/code/uniter3m/data/melm.py
index 5db35ba..3cca929 100644
--- a/code/uniter3m/data/melm.py
+++ b/code/uniter3m/data/melm.py
@@ -12,14 +12,17 @@ import torch
 from torch.nn.utils.rnn import pad_sequence
 from toolz.sandbox import unzip
 from code.uniter3m.data.data import (DetectFeatTxtTokDataset, TxtTokLmdb, pad_tensors, get_gather_index)
+from code.uniter.data.mlm import random_word
 
 def random_emo_word(melm_prob, tokens, vocab_range, emo_tokens, mask):
     """
+    # mask60%mask. 
+    mlm
     Masking some emotion tokens for 
     :param melm_prob: prob of masked emotional words
     :param tokens: list of int, tokenized sentence.
     :param vocab_range: for choosing a random word
-    :param emo_tokens: emotional tokens in tokens and the token's emotion
+    :param emo_tokens: emotional tokens ids
     :return: (list of int, list of int), masked tokens and related labels for
         LM prediction
     """
@@ -46,10 +49,11 @@ def random_emo_word(melm_prob, tokens, vocab_range, emo_tokens, mask):
             # no masking token (will be ignored by loss function later)
             output_label.append(-1)
     if all(o == -1 for o in output_label):
-        # at least mask 1
-        output_label[0] = tokens[0]
-        tokens[0] = mask
-
+        #  emotion mask, mlm
+        tokens, output_label = random_word(tokens, vocab_range, mask)
+        if all(o == -1 for o in output_label):
+            output_label[0] = tokens[0]
+            tokens[0] = mask
     return tokens, output_label
 
 class MelmDataset(DetectFeatTxtTokDataset):
@@ -57,9 +61,9 @@ class MelmDataset(DetectFeatTxtTokDataset):
     emotional words masked modeling, the 
     melm_prob: masked probility
     '''
-    def __init__(self, mask_prob, txt_db, img_db):
+    def __init__(self, mask_prob, txt_db, img_db, speech_db):
         assert isinstance(txt_db, TxtTokLmdb)
-        super().__init__(txt_db, img_db)
+        super().__init__(txt_db, img_db, speech_db)
         self.melm_prob = mask_prob
         self.img_shape = None
 
@@ -76,8 +80,10 @@ class MelmDataset(DetectFeatTxtTokDataset):
 
         # text input
         input_ids, txt_labels = self.create_melm_io(example['input_ids'], example['emo_input_ids'])
+        attn_masks = torch.ones(len(input_ids), dtype=torch.long)
 
-        # Jinming: add for emo_type_ids (0~4), 0 is the no-emotion-words
+        # Jinming: add for emo_type_ids (0~2), 0 is the no-emotion-words
+        # emo_type_ids , token: 0 1 2 
         if example.get('emo_type_ids') is not None:
             emo_type_ids = torch.tensor([0] + example['emo_type_ids'] + [0])
             # generate the labels for multitask emotion,  txt_labels txt_labels  -1 -1.
@@ -86,15 +92,30 @@ class MelmDataset(DetectFeatTxtTokDataset):
             # print("[Debug] txt_emo_labels {}".format(txt_emo_labels))
         else:
             txt_emo_labels = None
+        
+        if self.img_db is not None:
+            # print(f'[Debug] item {i} img is not None')
+            img_feat, num_bb = self._get_img_feat(example['img_fname'], self.img_shape)
+            img_attn_masks = torch.ones(num_bb, dtype=torch.long)
+            self.img_shape = img_feat.shape[1:]
+            attn_masks = torch.cat((attn_masks, img_attn_masks))
+        else:
+            # print(f'[Debug] item img {i} is None')
+            img_feat = None
+        
+        if self.speech_db is not None:
+            # print(f'[Debug] item {i} speech is not None')
+            speech_feat, num_frame = self._get_speech_feat(example['img_fname'])
+            speech_attn_masks = torch.ones(num_frame, dtype=torch.long)
+            attn_masks = torch.cat((attn_masks, speech_attn_masks))
+            # print('[Debug] item {} speech attn mask {} and final attn mask {}'.format(i, speech_attn_masks.shape, attn_masks.shape))
+        else:
+            speech_feat = None
 
-        img_feat, num_bb = self._get_img_feat(example['img_fname'], self.img_shape)
-        self.img_shape = img_feat.shape[1:]
-
-        attn_masks = torch.ones(len(input_ids) + num_bb, dtype=torch.long)
-
-        return input_ids, img_feat, attn_masks, txt_labels, txt_emo_labels
+        return input_ids, img_feat, speech_feat, attn_masks, txt_labels, txt_emo_labels
 
     def create_melm_io(self, input_ids, emo_input_ids):
+        # emo_input_ids: input_ids  id
         input_ids, txt_labels = random_emo_word(self.melm_prob, input_ids, 
                                                     self.txt_db.v_range, emo_input_ids, self.txt_db.mask)
         input_ids = torch.tensor([self.txt_db.cls_] + input_ids + [self.txt_db.sep])
@@ -117,7 +138,7 @@ def melm_collate(inputs):
     :emo_type_ids (n, max_L) padded with 0
     :txt_emo_labels (n, max_L) padded with -1, similar with the emo_type_ids
     """
-    (input_ids, img_feats, attn_masks, txt_labels, \
+    (input_ids, img_feats, speech_feats, attn_masks, txt_labels, \
              batch_txt_emo_labels) = map(list, unzip(inputs))
 
     # text batches
@@ -133,24 +154,38 @@ def melm_collate(inputs):
     txt_labels = pad_sequence(txt_labels, batch_first=True, padding_value=-1)
     position_ids = torch.arange(0, input_ids.size(1), dtype=torch.long
                                 ).unsqueeze(0)
-
-    # image batches
-    num_bbs = [f.size(0) for f in img_feats]
-    img_feat = pad_tensors(img_feats, num_bbs)
-    # img_feat = (n, max_num_nbb, dim)
-    img_position_ids = torch.arange(0, max(num_bbs), dtype=torch.long
-                                ).unsqueeze(0)
-
     attn_masks = pad_sequence(attn_masks, batch_first=True, padding_value=0)
 
+    if img_feats[0] is not None:
+        ## image batches
+        num_bbs = [f.size(0) for f in img_feats]
+        img_feat = pad_tensors(img_feats, num_bbs)
+        # print('[Debug] batch padding img input {}'.format(img_feat.shape)) # (n, max_num_nbb, dim)
+        img_position_ids = torch.arange(0, img_feat.size(1), dtype=torch.long).unsqueeze(0)      
+    else:
+        img_feat, num_bbs, img_position_ids = None, None, None
+    
+    if speech_feats[0] is not None:
+        ## speech batches
+        num_frames = [f.size(0) for f in speech_feats]
+        speech_feat = pad_tensors(speech_feats, num_frames)
+        # print('[Debug] batch padding speech input {}'.format(speech_feat.shape)) # (n, max_num_frame, dim)
+        speech_position_ids = torch.arange(0, speech_feat.size(1), dtype=torch.long).unsqueeze(0)    
+    else:
+        speech_feat, num_frames, speech_position_ids = None, None, None
+
     bs, max_tl = input_ids.size()
     out_size = attn_masks.size(1)
-    gather_index = get_gather_index(txt_lens, num_bbs, bs, max_tl, out_size)
+    # multi-modality atten mask
+    gather_index = get_gather_index(txt_lens, num_bbs, num_frames, bs, max_tl, out_size)
+    # print('[Debug] batch gather_index {} {}'.format(gather_index, gather_index.shape))
 
     batch = {'input_ids': input_ids,
              'position_ids': position_ids,
              'img_feat': img_feat,
              'img_position_ids': img_position_ids,
+             'speech_feat': speech_feat,
+             'speech_position_ids': speech_position_ids,
              'attn_masks': attn_masks,
              'gather_index': gather_index,
              'txt_labels': txt_labels, 
diff --git a/code/uniter3m/data/mlm.py b/code/uniter3m/data/mlm.py
index ddc5c32..7fd2182 100644
--- a/code/uniter3m/data/mlm.py
+++ b/code/uniter3m/data/mlm.py
@@ -11,45 +11,8 @@ from torch.nn.utils.rnn import pad_sequence
 from toolz.sandbox import unzip
 from code.uniter3m.data.data import (DetectFeatTxtTokDataset, TxtTokLmdb,
                    pad_tensors, get_gather_index)
-
-def random_word(tokens, vocab_range, mask):
-    """
-    Masking some random tokens for Language Model task with probabilities as in
-        the original BERT paper.
-    :param tokens: list of int, tokenized sentence.
-    :param vocab_range: for choosing a random word
-    :return: (list of int, list of int), masked tokens and related labels for
-        LM prediction
-    """
-    output_label = []
-
-    for i, token in enumerate(tokens):
-        prob = random.random()
-        # mask token with 15% probability
-        if prob < 0.15:
-            prob /= 0.15
-            # 80% randomly change token to mask token
-            if prob < 0.8:
-                # print('[Debug] 0.8 predict mask token {}'.format(token))
-                tokens[i] = mask
-            # 10% randomly change token to random token
-            elif prob < 0.9:
-                # print('[Debug] 0.1 predict random token {}'.format(token))
-                tokens[i] = random.choice(list(range(*vocab_range)))
-            # -> rest 10% randomly keep current token
-            # append current token to output (we will predict these later)
-            output_label.append(token)
-        else:
-            # no masking token (will be ignored by loss function later)
-            output_label.append(-1)
-    if all(o == -1 for o in output_label):
-        # at least mask 1
-        output_label[0] = tokens[0]
-        tokens[0] = mask
-
-    return tokens, output_label
-
-
+# from uniter 
+from code.uniter.data.mlm import random_word
 class MlmDataset(DetectFeatTxtTokDataset):
     def __init__(self, txt_db, img_db, speech_db):
         assert isinstance(txt_db, TxtTokLmdb)
@@ -71,7 +34,7 @@ class MlmDataset(DetectFeatTxtTokDataset):
         attn_masks = torch.ones(len(input_ids), dtype=torch.long)
         # print('[Debug] item {} text attn mask {} {}'.format(i, attn_masks, attn_masks.shape))
 
-        if self.img_db:
+        if self.img_db is not None:
             # print(f'[Debug] item {i} img is not None')
             img_feat, num_bb = self._get_img_feat(example['img_fname'], self.img_shape)
             img_attn_masks = torch.ones(num_bb, dtype=torch.long)
@@ -81,7 +44,7 @@ class MlmDataset(DetectFeatTxtTokDataset):
             # print(f'[Debug] item img {i} is None')
             img_feat = None
         
-        if self.speech_db:
+        if self.speech_db is not None:
             # print(f'[Debug] item {i} speech is not None')
             speech_feat, num_frame = self._get_speech_feat(example['img_fname'])
             speech_attn_masks = torch.ones(num_frame, dtype=torch.long)
@@ -147,7 +110,7 @@ def mlm_collate(inputs):
         num_frames = [f.size(0) for f in speech_feats]
         speech_feat = pad_tensors(speech_feats, num_frames)
         # print('[Debug] batch padding speech input {}'.format(speech_feat.shape)) # (n, max_num_frame, dim)
-        speech_position_ids = torch.arange(0, max(num_frames), dtype=torch.long).unsqueeze(0)    
+        speech_position_ids = torch.arange(0, speech_feat.size(1), dtype=torch.long).unsqueeze(0)    
     else:
         speech_feat, num_frames, speech_position_ids = None, None, None
 
@@ -156,7 +119,7 @@ def mlm_collate(inputs):
     # multi-modality atten mask
     gather_index = get_gather_index(txt_lens, num_bbs, num_frames, bs, max_tl, out_size)
     # print('[Debug] batch gather_index {} {}'.format(gather_index, gather_index.shape))
-
+    
     batch = {'input_ids': input_ids,
              'position_ids': position_ids,
              'img_feat': img_feat,
diff --git a/code/uniter3m/data/mrm.py b/code/uniter3m/data/mrm.py
index 2082c75..899a75f 100644
--- a/code/uniter3m/data/mrm.py
+++ b/code/uniter3m/data/mrm.py
@@ -11,39 +11,22 @@ from torch.nn.utils.rnn import pad_sequence
 from toolz.sandbox import unzip
 from code.uniter3m.data.data import DetectFeatTxtTokDataset, pad_tensors, get_gather_index
 
-
-def _get_img_mask(mask_prob, num_bb):
-    img_mask = [random.random() < mask_prob for _ in range(num_bb)]
-    if not any(img_mask):
-        # at least mask 1
-        img_mask[random.choice(range(num_bb))] = True
-    img_mask = torch.tensor(img_mask)
-    return img_mask
-
+# from uniter 
+from code.uniter.data.mrm import _get_img_mask, _get_feat_target, _mask_img_feat, _get_targets
 
 def _get_img_tgt_mask(img_mask, txt_len, speech_len):
-    z = torch.zeros(txt_len, dtype=torch.uint8)
+    if float(torch.__version__[:3]) <= 1.2:
+        bool_type = torch.uint8
+    else:
+        bool_type = torch.bool
+    z = torch.zeros(txt_len, dtype=bool_type)
     if speech_len > 0:
-        zs = torch.zeros(speech_len, dtype=torch.uint8)
+        zs = torch.zeros(speech_len, dtype=bool_type)
         img_mask_tgt = torch.cat([z, img_mask, zs], dim=0)
     else:
         img_mask_tgt = torch.cat([z, img_mask], dim=0)
     return img_mask_tgt
 
-def _get_feat_target(img_feat, img_masks):
-    img_masks_ext = img_masks.unsqueeze(-1).expand_as(img_feat)  # (n, m, d)
-    feat_dim = img_feat.size(-1)
-    feat_targets = img_feat[img_masks_ext].contiguous().view(
-        -1, feat_dim)  # (s, d)
-    return feat_targets
-
-
-def _mask_img_feat(img_feat, img_masks):
-    img_masks_ext = img_masks.unsqueeze(-1).expand_as(img_feat)
-    img_feat_masked = img_feat.data.masked_fill(img_masks_ext, 0)
-    return img_feat_masked
-
-
 class MrfrDataset(DetectFeatTxtTokDataset):
     def __init__(self, mask_prob, *args, **kwargs):
         super().__init__(*args, **kwargs)
@@ -53,7 +36,6 @@ class MrfrDataset(DetectFeatTxtTokDataset):
         self.mask_prob = mask_prob
         self.img_shape = None
 
-
     def __getitem__(self, i):
         """
         Return:
@@ -107,7 +89,7 @@ def mrfr_collate(inputs):
 
     num_bbs = [f.size(0) for f in img_feats]
     img_feat = pad_tensors(img_feats, num_bbs)
-    img_position_ids = torch.arange(0, max(num_bbs), dtype=torch.long
+    img_position_ids = torch.arange(0, img_feat.size(1), dtype=torch.long
                                 ).unsqueeze(0)
 
     # mask features
@@ -124,7 +106,7 @@ def mrfr_collate(inputs):
         num_frames = [f.size(0) for f in speech_feats]
         speech_feat = pad_tensors(speech_feats, num_frames)
         # print('[Debug] the batch input {}'.format(img_feat.shape)) # (n, max_num_nbb, dim)
-        speech_position_ids = torch.arange(0, max(num_frames), dtype=torch.long).unsqueeze(0)    
+        speech_position_ids = torch.arange(0, speech_feat.size(1), dtype=torch.long).unsqueeze(0)    
     else:
         speech_feat, num_frames, speech_position_ids = None, None, None
 
@@ -146,15 +128,6 @@ def mrfr_collate(inputs):
              'img_mask_tgt': img_mask_tgt}
     return batch
 
-
-def _get_targets(img_masks, img_soft_label):
-    soft_label_dim = img_soft_label.size(-1)
-    img_masks_ext_for_label = img_masks.unsqueeze(-1).expand_as(img_soft_label)
-    label_targets = img_soft_label[img_masks_ext_for_label].contiguous().view(
-        -1, soft_label_dim)
-    return label_targets
-
-
 class MrcDataset(DetectFeatTxtTokDataset):
     def __init__(self, mask_prob, *args, **kwargs):
         super().__init__(*args, **kwargs)
@@ -171,9 +144,12 @@ class MrcDataset(DetectFeatTxtTokDataset):
         img_soft_label = torch.tensor(img_dump['soft_labels'])
         if num_bb == 0:
             if img_shape is None:
-                img_shape = torch.zeros(342)
+                print("[Warning] Set the img_shape to 342!!!")
+                img_shape = 342   
             img_feat = torch.zeros(img_shape).unsqueeze(0)
             img_soft_label = torch.zeros(8).unsqueeze(0)
+            # set to neutral
+            img_soft_label[0][0] = 1
             num_bb = 1
         return img_feat, img_soft_label, num_bb
 
@@ -231,7 +207,7 @@ def mrc_collate(inputs):
         num_frames = [f.size(0) for f in speech_feats]
         speech_feat = pad_tensors(speech_feats, num_frames)
         # print('[Debug] the batch input {}'.format(img_feat.shape)) # (n, max_num_nbb, dim)
-        speech_position_ids = torch.arange(0, max(num_frames), dtype=torch.long).unsqueeze(0)    
+        speech_position_ids = torch.arange(0, speech_feat.size(1), dtype=torch.long).unsqueeze(0)    
     else:
         speech_feat, num_frames, speech_position_ids = None, None, None
 
diff --git a/code/uniter3m/data/msrm.py b/code/uniter3m/data/msrm.py
index 1616bfc..6ea02b4 100644
--- a/code/uniter3m/data/msrm.py
+++ b/code/uniter3m/data/msrm.py
@@ -13,10 +13,10 @@ from code.uniter3m.data.mrm import _get_img_mask, _get_feat_target, _mask_img_fe
 
 
 def _get_speech_tgt_mask(speech_mask, txt_len, img_len):
-    # text, img, speech 
-    z = torch.zeros(txt_len, dtype=torch.uint8)
+    # text, img, speech 
+    z = torch.zeros(txt_len, dtype=torch.bool)
     if img_len > 0:
-        zi = torch.zeros(img_len, dtype=torch.uint8)
+        zi = torch.zeros(img_len, dtype=torch.bool)
         speech_mask_tgt = torch.cat([z, zi, speech_mask], dim=0)
     else:
         speech_mask_tgt = torch.cat([z, speech_mask], dim=0)
@@ -46,14 +46,14 @@ class MsrfrDataset(DetectFeatTxtTokDataset):
         # text input
         input_ids = example['input_ids']
         input_ids = self.txt_db.combine_inputs(input_ids)
-        attn_masks = torch.ones(len(input_ids))
+        attn_masks = torch.ones(len(input_ids), dtype=torch.long)
         
         # speech input
         speech_feat, num_frame = self._get_speech_feat(example['img_fname'])
         speech_attn_masks = torch.ones(num_frame, dtype=torch.long)
 
         if not self.img_db:
-            img_feat, num_bb = None, None
+            img_feat, num_bb = None, 0
             attn_masks = torch.cat((attn_masks, speech_attn_masks))
         else:
             img_feat, num_bb = self._get_img_feat(example['img_fname'], self.img_shape)
@@ -103,7 +103,7 @@ def msrfr_collate(inputs):
 
     if img_feats[0] is not None:
         ## images batches
-        num_bbs = [f.size(0) for f in speech_feats]
+        num_bbs = [f.size(0) for f in img_feats]
         img_feat = pad_tensors(img_feats, num_bbs)
         img_position_ids = torch.arange(0, max(num_bbs), dtype=torch.long).unsqueeze(0)    
     else:
diff --git a/code/uniter3m/model/attention.py b/code/uniter3m/model/attention.py
deleted file mode 100644
index 7d7e2b0..0000000
--- a/code/uniter3m/model/attention.py
+++ /dev/null
@@ -1,402 +0,0 @@
-"""
-copy multi-head attention code from pytorch
-(https://github.com/pytorch/pytorch),
-"""
-import warnings
-
-import torch
-from torch.nn import Module, Parameter, Linear
-from torch.nn.init import xavier_normal_, xavier_uniform_, constant_
-from torch.nn.functional import linear, softmax, dropout
-
-
-def multi_head_attention_forward(query,                           # type: Tensor
-                                 key,                             # type: Tensor
-                                 value,                           # type: Tensor
-                                 embed_dim_to_check,              # type: int
-                                 num_heads,                       # type: int
-                                 in_proj_weight,                  # type: Tensor
-                                 in_proj_bias,                    # type: Tensor
-                                 bias_k,                          # type: Optional[Tensor]
-                                 bias_v,                          # type: Optional[Tensor]
-                                 add_zero_attn,                   # type: bool
-                                 dropout_p,                       # type: float
-                                 out_proj_weight,                 # type: Tensor
-                                 out_proj_bias,                   # type: Tensor
-                                 training=True,                   # type: bool
-                                 key_padding_mask=None,           # type: Optional[Tensor]
-                                 need_weights=True,               # type: bool
-                                 attn_mask=None,                  # type: Optional[Tensor]
-                                 use_separate_proj_weight=False,  # type: bool
-                                 q_proj_weight=None,              # type: Optional[Tensor]
-                                 k_proj_weight=None,              # type: Optional[Tensor]
-                                 v_proj_weight=None,              # type: Optional[Tensor]
-                                 static_k=None,                   # type: Optional[Tensor]
-                                 static_v=None                    # type: Optional[Tensor]
-                                 ):
-    # type: (...) -> Tuple[Tensor, Optional[Tensor]]
-    r"""
-    Args:
-        query, key, value: map a query and a set of key-value pairs to an output.
-            See "Attention Is All You Need" for more details.
-        embed_dim_to_check: total dimension of the model.
-        num_heads: parallel attention heads.
-        in_proj_weight, in_proj_bias: input projection weight and bias.
-        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
-        add_zero_attn: add a new batch of zeros to the key and
-                       value sequences at dim=1.
-        dropout_p: probability of an element to be zeroed.
-        out_proj_weight, out_proj_bias: the output projection weight and bias.
-        training: apply dropout if is ``True``.
-        key_padding_mask: if provided, specified padding elements in the key will
-            be ignored by the attention. This is an binary mask. When the value is True,
-            the corresponding value on the attention layer will be filled with -inf.
-        need_weights: output attn_output_weights.
-        attn_mask: mask that prevents attention to certain positions. This is an additive mask
-            (i.e. the values will be added to the attention layer).
-        use_separate_proj_weight: the function accept the proj. weights for query, key,
-            and value in differnt forms. If false, in_proj_weight will be used, which is
-            a combination of q_proj_weight, k_proj_weight, v_proj_weight.
-        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
-        static_k, static_v: static key and value used for attention operators.
-
-
-    Shape:
-        Inputs:
-        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
-          the embedding dimension.
-        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
-          the embedding dimension.
-        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
-          the embedding dimension.
-        - key_padding_mask: :math:`(N, S)`, ByteTensor, where N is the batch size, S is the source sequence length.
-        - attn_mask: :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
-        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
-          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
-        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,
-          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.
-
-        Outputs:
-        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
-          E is the embedding dimension.
-        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,
-          L is the target sequence length, S is the source sequence length.
-    """
-
-    qkv_same = torch.equal(query, key) and torch.equal(key, value)
-    kv_same = torch.equal(key, value)
-
-    tgt_len, bsz, embed_dim = query.size()
-    assert embed_dim == embed_dim_to_check
-    assert list(query.size()) == [tgt_len, bsz, embed_dim]
-    assert key.size() == value.size()
-
-    head_dim = embed_dim // num_heads
-    assert head_dim * num_heads == embed_dim, "embed_dim must be divisible by num_heads"
-    scaling = float(head_dim) ** -0.5
-
-    if use_separate_proj_weight is not True:
-        if qkv_same:
-            # self-attention
-            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)
-
-        elif kv_same:
-            # encoder-decoder attention
-            # This is inline in_proj function with in_proj_weight and in_proj_bias
-            _b = in_proj_bias
-            _start = 0
-            _end = embed_dim
-            _w = in_proj_weight[_start:_end, :]
-            if _b is not None:
-                _b = _b[_start:_end]
-            q = linear(query, _w, _b)
-
-            if key is None:
-                assert value is None
-                k = None
-                v = None
-            else:
-
-                # This is inline in_proj function with in_proj_weight and in_proj_bias
-                _b = in_proj_bias
-                _start = embed_dim
-                _end = None
-                _w = in_proj_weight[_start:, :]
-                if _b is not None:
-                    _b = _b[_start:]
-                k, v = linear(key, _w, _b).chunk(2, dim=-1)
-
-        else:
-            # This is inline in_proj function with in_proj_weight and in_proj_bias
-            _b = in_proj_bias
-            _start = 0
-            _end = embed_dim
-            _w = in_proj_weight[_start:_end, :]
-            if _b is not None:
-                _b = _b[_start:_end]
-            q = linear(query, _w, _b)
-
-            # This is inline in_proj function with in_proj_weight and in_proj_bias
-            _b = in_proj_bias
-            _start = embed_dim
-            _end = embed_dim * 2
-            _w = in_proj_weight[_start:_end, :]
-            if _b is not None:
-                _b = _b[_start:_end]
-            k = linear(key, _w, _b)
-
-            # This is inline in_proj function with in_proj_weight and in_proj_bias
-            _b = in_proj_bias
-            _start = embed_dim * 2
-            _end = None
-            _w = in_proj_weight[_start:, :]
-            if _b is not None:
-                _b = _b[_start:]
-            v = linear(value, _w, _b)
-    else:
-        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)
-        len1, len2 = q_proj_weight_non_opt.size()
-        assert len1 == embed_dim and len2 == query.size(-1)
-
-        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)
-        len1, len2 = k_proj_weight_non_opt.size()
-        assert len1 == embed_dim and len2 == key.size(-1)
-
-        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)
-        len1, len2 = v_proj_weight_non_opt.size()
-        assert len1 == embed_dim and len2 == value.size(-1)
-
-        if in_proj_bias is not None:
-            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])
-            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])
-            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2):])
-        else:
-            q = linear(query, q_proj_weight_non_opt, in_proj_bias)
-            k = linear(key, k_proj_weight_non_opt, in_proj_bias)
-            v = linear(value, v_proj_weight_non_opt, in_proj_bias)
-    q = q * scaling
-
-    if bias_k is not None and bias_v is not None:
-        if static_k is None and static_v is None:
-            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
-            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
-            if attn_mask is not None:
-                attn_mask = torch.cat([attn_mask,
-                                      torch.zeros((attn_mask.size(0), 1),
-                                                  dtype=attn_mask.dtype,
-                                                  device=attn_mask.device)], dim=1)
-            if key_padding_mask is not None:
-                key_padding_mask = torch.cat(
-                    [key_padding_mask, torch.zeros((key_padding_mask.size(0), 1),
-                                                   dtype=key_padding_mask.dtype,
-                                                   device=key_padding_mask.device)], dim=1)
-        else:
-            assert static_k is None, "bias cannot be added to static key."
-            assert static_v is None, "bias cannot be added to static value."
-    else:
-        assert bias_k is None
-        assert bias_v is None
-
-    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
-    if k is not None:
-        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
-    if v is not None:
-        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
-
-    if static_k is not None:
-        assert static_k.size(0) == bsz * num_heads
-        assert static_k.size(2) == head_dim
-        k = static_k
-
-    if static_v is not None:
-        assert static_v.size(0) == bsz * num_heads
-        assert static_v.size(2) == head_dim
-        v = static_v
-
-    src_len = k.size(1)
-
-    if key_padding_mask is not None:
-        assert key_padding_mask.size(0) == bsz
-        assert key_padding_mask.size(1) == src_len
-
-    if add_zero_attn:
-        src_len += 1
-        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)
-        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)
-        if attn_mask is not None:
-            attn_mask = torch.cat([attn_mask, torch.zeros((attn_mask.size(0), 1),
-                                                          dtype=attn_mask.dtype,
-                                                          device=attn_mask.device)], dim=1)
-        if key_padding_mask is not None:
-            key_padding_mask = torch.cat(
-                [key_padding_mask, torch.zeros((key_padding_mask.size(0), 1),
-                                               dtype=key_padding_mask.dtype,
-                                               device=key_padding_mask.device)], dim=1)
-
-    attn_output_weights = torch.bmm(q, k.transpose(1, 2))
-    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]
-
-    if attn_mask is not None:
-        attn_mask = attn_mask.unsqueeze(0)
-        attn_output_weights += attn_mask
-
-    if key_padding_mask is not None:
-        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
-        attn_output_weights = attn_output_weights.masked_fill(
-            key_padding_mask.unsqueeze(1).unsqueeze(2),
-            float('-inf'),
-        )
-        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)
-
-    attn_output_weights = softmax(
-        attn_output_weights, dim=-1)
-    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)
-
-    attn_output = torch.bmm(attn_output_weights, v)
-    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]
-    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
-    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
-
-    if need_weights:
-        # average attention weights over heads
-        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
-        return attn_output, attn_output_weights.sum(dim=1) / num_heads
-    else:
-        return attn_output, None
-
-
-class MultiheadAttention(Module):
-    r"""Allows the model to jointly attend to information
-    from different representation subspaces.
-    See reference: Attention Is All You Need
-
-    .. math::
-        \text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O
-        \text{where} head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
-
-    Args:
-        embed_dim: total dimension of the model.
-        num_heads: parallel attention heads.
-        dropout: a Dropout layer on attn_output_weights. Default: 0.0.
-        bias: add bias as module parameter. Default: True.
-        add_bias_kv: add bias to the key and value sequences at dim=0.
-        add_zero_attn: add a new batch of zeros to the key and
-                       value sequences at dim=1.
-        kdim: total number of features in key. Default: None.
-        vdim: total number of features in key. Default: None.
-
-        Note: if kdim and vdim are None, they will be set to embed_dim such that
-        query, key, and value have the same number of features.
-
-    Examples::
-
-        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)
-        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)
-    """
-
-    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None):
-        super(MultiheadAttention, self).__init__()
-        self.embed_dim = embed_dim
-        self.kdim = kdim if kdim is not None else embed_dim
-        self.vdim = vdim if vdim is not None else embed_dim
-        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim
-
-        self.num_heads = num_heads
-        self.dropout = dropout
-        self.head_dim = embed_dim // num_heads
-        assert self.head_dim * num_heads == self.embed_dim, "embed_dim must be divisible by num_heads"
-
-        self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))
-
-        if self._qkv_same_embed_dim is False:
-            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))
-            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))
-            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))
-
-        if bias:
-            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))
-        else:
-            self.register_parameter('in_proj_bias', None)
-        self.out_proj = Linear(embed_dim, embed_dim, bias=bias)
-
-        if add_bias_kv:
-            self.bias_k = Parameter(torch.empty(1, 1, embed_dim))
-            self.bias_v = Parameter(torch.empty(1, 1, embed_dim))
-        else:
-            self.bias_k = self.bias_v = None
-
-        self.add_zero_attn = add_zero_attn
-
-        self._reset_parameters()
-
-    def _reset_parameters(self):
-        if self._qkv_same_embed_dim:
-            xavier_uniform_(self.in_proj_weight)
-        else:
-            xavier_uniform_(self.q_proj_weight)
-            xavier_uniform_(self.k_proj_weight)
-            xavier_uniform_(self.v_proj_weight)
-
-        if self.in_proj_bias is not None:
-            constant_(self.in_proj_bias, 0.)
-            constant_(self.out_proj.bias, 0.)
-        if self.bias_k is not None:
-            xavier_normal_(self.bias_k)
-        if self.bias_v is not None:
-            xavier_normal_(self.bias_v)
-
-    def forward(self, query, key, value, key_padding_mask=None,
-                need_weights=True, attn_mask=None):
-        r"""
-    Args:
-        query, key, value: map a query and a set of key-value pairs to an output.
-            See "Attention Is All You Need" for more details.
-        key_padding_mask: if provided, specified padding elements in the key will
-            be ignored by the attention. This is an binary mask. When the value is True,
-            the corresponding value on the attention layer will be filled with -inf.
-        need_weights: output attn_output_weights.
-        attn_mask: mask that prevents attention to certain positions. This is an additive mask
-            (i.e. the values will be added to the attention layer).
-
-    Shape:
-        - Inputs:
-        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
-          the embedding dimension.
-        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
-          the embedding dimension.
-        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
-          the embedding dimension.
-        - key_padding_mask: :math:`(N, S)`, ByteTensor, where N is the batch size, S is the source sequence length.
-        - attn_mask: :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
-
-        - Outputs:
-        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
-          E is the embedding dimension.
-        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,
-          L is the target sequence length, S is the source sequence length.
-        """
-        if hasattr(self, '_qkv_same_embed_dim') and self._qkv_same_embed_dim is False:
-            return multi_head_attention_forward(
-                query, key, value, self.embed_dim, self.num_heads,
-                self.in_proj_weight, self.in_proj_bias,
-                self.bias_k, self.bias_v, self.add_zero_attn,
-                self.dropout, self.out_proj.weight, self.out_proj.bias,
-                training=self.training,
-                key_padding_mask=key_padding_mask, need_weights=need_weights,
-                attn_mask=attn_mask, use_separate_proj_weight=True,
-                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,
-                v_proj_weight=self.v_proj_weight)
-        else:
-            if not hasattr(self, '_qkv_same_embed_dim'):
-                warnings.warn('A new version of MultiheadAttention module has been implemented. \
-                    Please re-train your model with the new module',
-                              UserWarning)
-
-            return multi_head_attention_forward(
-                query, key, value, self.embed_dim, self.num_heads,
-                self.in_proj_weight, self.in_proj_bias,
-                self.bias_k, self.bias_v, self.add_zero_attn,
-                self.dropout, self.out_proj.weight, self.out_proj.bias,
-                training=self.training,
-                key_padding_mask=key_padding_mask, need_weights=need_weights,
-                attn_mask=attn_mask)
diff --git a/code/uniter3m/model/emocls.py b/code/uniter3m/model/emocls.py
index 4b6fe65..1abeb47 100644
--- a/code/uniter3m/model/emocls.py
+++ b/code/uniter3m/model/emocls.py
@@ -15,8 +15,9 @@ from torch.nn import CrossEntropyLoss
 from apex.normalization.fused_layer_norm import FusedLayerNorm as LayerNorm
 from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix
 from code.uniter3m.model.model import UniterPreTrainedModel, UniterModel
-from code.uniter3m.model.layer import GELU
-from code.uniter3m.utils.misc import NoOp
+## from uniter
+from code.uniter.model.layer import GELU
+from code.uniter.utils.misc import NoOp
 
 class UniterForEmoRecognition(UniterPreTrainedModel):
     """ Finetune UNITER for Emotion Recognition
diff --git a/code/uniter3m/model/layer.py b/code/uniter3m/model/layer.py
deleted file mode 100644
index ed0203f..0000000
--- a/code/uniter3m/model/layer.py
+++ /dev/null
@@ -1,233 +0,0 @@
-"""
-BERT layers from the huggingface implementation
-(https://github.com/huggingface/transformers)
-"""
-# coding=utf-8
-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.
-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-import logging
-import math
-
-import torch
-from torch import nn
-from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm
-
-
-logger = logging.getLogger(__name__)
-
-
-def gelu(x):
-    """Implementation of the gelu activation function.
-        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):
-        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))
-        Also see https://arxiv.org/abs/1606.08415
-    """
-    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
-
-
-def swish(x):
-    return x * torch.sigmoid(x)
-
-
-ACT2FN = {"gelu": gelu, "relu": torch.nn.functional.relu, "swish": swish}
-
-
-class GELU(nn.Module):
-    def forward(self, input_):
-        output = gelu(input_)
-        return output
-
-
-class BertSelfAttention(nn.Module):
-    def __init__(self, config):
-        super(BertSelfAttention, self).__init__()
-        if config.hidden_size % config.num_attention_heads != 0:
-            raise ValueError(
-                "The hidden size (%d) is not a multiple of the number of attention "
-                "heads (%d)" % (config.hidden_size, config.num_attention_heads))
-        self.num_attention_heads = config.num_attention_heads
-        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
-        self.all_head_size = self.num_attention_heads * self.attention_head_size
-
-        self.query = nn.Linear(config.hidden_size, self.all_head_size)
-        self.key = nn.Linear(config.hidden_size, self.all_head_size)
-        self.value = nn.Linear(config.hidden_size, self.all_head_size)
-
-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
-
-    def transpose_for_scores(self, x):
-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
-        x = x.view(*new_x_shape)
-        return x.permute(0, 2, 1, 3)
-
-    def forward(self, hidden_states, attention_mask):
-        mixed_query_layer = self.query(hidden_states)
-        mixed_key_layer = self.key(hidden_states)
-        mixed_value_layer = self.value(hidden_states)
-
-        query_layer = self.transpose_for_scores(mixed_query_layer)
-        key_layer = self.transpose_for_scores(mixed_key_layer)
-        value_layer = self.transpose_for_scores(mixed_value_layer)
-
-        # Take the dot product between "query" and "key" to get the raw attention scores.
-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
-        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
-        attention_scores = attention_scores + attention_mask
-
-        # Normalize the attention scores to probabilities.
-        attention_probs = nn.Softmax(dim=-1)(attention_scores)
-
-        # This is actually dropping out entire tokens to attend to, which might
-        # seem a bit unusual, but is taken from the original Transformer paper.
-        attention_probs = self.dropout(attention_probs)
-
-        context_layer = torch.matmul(attention_probs, value_layer)
-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
-        context_layer = context_layer.view(*new_context_layer_shape)
-        return context_layer
-
-
-class BertSelfOutput(nn.Module):
-    def __init__(self, config):
-        super(BertSelfOutput, self).__init__()
-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
-
-    def forward(self, hidden_states, input_tensor):
-        hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
-        return hidden_states
-
-
-class BertAttention(nn.Module):
-    def __init__(self, config):
-        super(BertAttention, self).__init__()
-        self.self = BertSelfAttention(config)
-        self.output = BertSelfOutput(config)
-
-    def forward(self, input_tensor, attention_mask):
-        self_output = self.self(input_tensor, attention_mask)
-        attention_output = self.output(self_output, input_tensor)
-        return attention_output
-
-
-class BertIntermediate(nn.Module):
-    def __init__(self, config):
-        super(BertIntermediate, self).__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
-
-    def forward(self, hidden_states):
-        hidden_states = self.dense(hidden_states)
-        hidden_states = self.intermediate_act_fn(hidden_states)
-        return hidden_states
-
-
-class BertOutput(nn.Module):
-    def __init__(self, config):
-        super(BertOutput, self).__init__()
-        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
-
-    def forward(self, hidden_states, input_tensor):
-        hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
-        return hidden_states
-
-
-class BertLayer(nn.Module):
-    def __init__(self, config):
-        super(BertLayer, self).__init__()
-        self.attention = BertAttention(config)
-        self.intermediate = BertIntermediate(config)
-        self.output = BertOutput(config)
-
-    def forward(self, hidden_states, attention_mask):
-        attention_output = self.attention(hidden_states, attention_mask)
-        intermediate_output = self.intermediate(attention_output)
-        layer_output = self.output(intermediate_output, attention_output)
-        return layer_output
-
-
-class BertPooler(nn.Module):
-    def __init__(self, config):
-        super(BertPooler, self).__init__()
-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.activation = nn.Tanh()
-
-    def forward(self, hidden_states):
-        # We "pool" the model by simply taking the hidden state corresponding
-        # to the first token.
-        first_token_tensor = hidden_states[:, 0]
-        pooled_output = self.dense(first_token_tensor)
-        pooled_output = self.activation(pooled_output)
-        return pooled_output
-
-
-class BertPredictionHeadTransform(nn.Module):
-    def __init__(self, config):
-        super(BertPredictionHeadTransform, self).__init__()
-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        if isinstance(config.hidden_act, str):
-            self.transform_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.transform_act_fn = config.hidden_act
-        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)
-
-    def forward(self, hidden_states):
-        hidden_states = self.dense(hidden_states)
-        hidden_states = self.transform_act_fn(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states)
-        return hidden_states
-
-
-class BertLMPredictionHead(nn.Module):
-    def __init__(self, config, bert_model_embedding_weights):
-        super(BertLMPredictionHead, self).__init__()
-        self.transform = BertPredictionHeadTransform(config)
-
-        # The output weights are the same as the input embeddings, but there is
-        # an output-only bias for each token.
-        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),
-                                 bert_model_embedding_weights.size(0),
-                                 bias=False)
-        self.decoder.weight = bert_model_embedding_weights
-        self.bias = nn.Parameter(
-            torch.zeros(bert_model_embedding_weights.size(0)))
-
-    def forward(self, hidden_states):
-        hidden_states = self.transform(hidden_states)
-        hidden_states = self.decoder(hidden_states) + self.bias
-        return hidden_states
-
-
-class BertOnlyMLMHead(nn.Module):
-    def __init__(self, config, bert_model_embedding_weights):
-        super(BertOnlyMLMHead, self).__init__()
-        self.predictions = BertLMPredictionHead(config,
-                                                bert_model_embedding_weights)
-
-    def forward(self, sequence_output):
-        prediction_scores = self.predictions(sequence_output)
-        return prediction_scores
diff --git a/code/uniter3m/model/model.py b/code/uniter3m/model/model.py
index 48763cc..4f882da 100644
--- a/code/uniter3m/model/model.py
+++ b/code/uniter3m/model/model.py
@@ -14,8 +14,8 @@ from io import open
 import torch
 from torch import nn
 from apex.normalization.fused_layer_norm import FusedLayerNorm
-
-from code.uniter3m.model.layer import BertLayer, BertPooler
+from code.uniter.model.layer import BertLayer, BertPooler
+from code.uniter.model.model import UniterTextEmbeddings, UniterImageEmbeddings
 
 logger = logging.getLogger(__name__)
 
@@ -211,72 +211,6 @@ class UniterPreTrainedModel(nn.Module):
                                    "\n\t".join(error_msgs)))
         return model
 
-
-class UniterTextEmbeddings(nn.Module):
-    def __init__(self, config):
-        super().__init__()
-
-        self.config = config
-
-        self.word_embeddings = nn.Embedding(config.vocab_size,
-                                            config.hidden_size, padding_idx=0)
-        # build position vocab embeddings = 512
-        self.position_embeddings = nn.Embedding(config.max_position_embeddings,
-                                                config.hidden_size)
-        self.token_type_embeddings = nn.Embedding(config.type_vocab_size,
-                                                  config.hidden_size)
-        print("[Warning] Donot use emo type embeddings add to input")
-        # self.LayerNorm is not snake-cased to stick with TensorFlow model
-        # variable name and be able to load any TensorFlow checkpoint file
-        self.LayerNorm = FusedLayerNorm(config.hidden_size, eps=1e-12)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
-
-    def forward(self, input_ids, position_ids, token_type_ids=None):
-        '''
-        batch-data
-        '''
-        if token_type_ids is None:
-            token_type_ids = torch.zeros_like(input_ids)
-        words_embeddings = self.word_embeddings(input_ids)
-        position_embeddings = self.position_embeddings(position_ids)
-        token_type_embeddings = self.token_type_embeddings(token_type_ids)
-        embeddings = (words_embeddings
-                      + position_embeddings
-                      + token_type_embeddings)
-                          
-        embeddings = self.LayerNorm(embeddings)
-        embeddings = self.dropout(embeddings)
-        return embeddings
-
-class UniterImageEmbeddings(nn.Module):
-    def __init__(self, config, img_dim):
-        super().__init__()
-        self.img_linear = nn.Linear(img_dim, config.hidden_size)
-        self.img_layer_norm = FusedLayerNorm(config.hidden_size, eps=1e-12)
-        self.position_embeddings = nn.Embedding(config.img_max_position_embeddings,
-                                                config.hidden_size)
-        self.mask_embedding = nn.Embedding(2, img_dim, padding_idx=0)
-
-        # tf naming convention for layer norm
-        self.LayerNorm = FusedLayerNorm(config.hidden_size, eps=1e-12)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
-
-    def forward(self, img_feat, img_position_ids, img_type_embeddings, img_masks=None):
-        if img_masks is not None:
-            self.mask_embedding.weight.data[0, :].fill_(0)
-            mask = self.mask_embedding(img_masks.long())
-            img_feat = img_feat + mask
-
-        transformed_im = self.img_layer_norm(self.img_linear(img_feat))
-        position_embeddings = self.position_embeddings(img_position_ids)
-        # print('transformed_im {} position_embeddings {} img_type_embeddings {}'.format(
-        #     transformed_im.size(), position_embeddings.size(), img_type_embeddings.size()
-        # ))
-        embeddings = transformed_im + position_embeddings + img_type_embeddings
-        embeddings = self.LayerNorm(embeddings)
-        embeddings = self.dropout(embeddings)
-        return embeddings
-
 class UniterSpeechEmbeddings(nn.Module):
     def __init__(self, config, speech_dim):
         super().__init__()
@@ -286,7 +220,7 @@ class UniterSpeechEmbeddings(nn.Module):
         '''
         self.speech_linear = nn.Linear(speech_dim, config.hidden_size)
         self.speech_layer_norm = FusedLayerNorm(config.hidden_size, eps=1e-12)
-        self.position_embeddings = nn.Embedding(config.speech_max_position_embeddings,
+        self.position_embeddings = nn.Embedding(config.max_position_embeddings,
                                                 config.hidden_size)
         self.mask_embedding = nn.Embedding(2, speech_dim, padding_idx=0)
 
@@ -379,8 +313,8 @@ class UniterModel(UniterPreTrainedModel):
     
     def _compute_speech_embeddings(self, speech_feat, speech_position_ids, speech_masks=None,
                                             speech_type_ids=None):
-        if not self.use_visual:
-            # logger.info('[Debug] no visual so use bert-type-token embedding for speech')
+        if not self.use_visual or self.config.speech_visual_use_same_type:
+            # logger.info('[Debug] use visual bert-type-token embedding for speech')
             speech_type_ids = torch.ones_like(speech_feat[:, :, 0].long())
             # share the embedding defined in txtEmbedding
             speech_type_embeddings = self.embeddings.token_type_embeddings(
@@ -410,11 +344,11 @@ class UniterModel(UniterPreTrainedModel):
     def _compute_speech_txt_embeddings(self, input_ids, position_ids,
                                     speech_feat, speech_position_ids,
                                     gather_index, speech_masks=None,
-                                    txt_type_ids=None):
+                                    txt_type_ids=None, speech_type_ids=None):
         txt_emb = self._compute_txt_embeddings(
             input_ids, position_ids, txt_type_ids)
         speech_emb = self._compute_speech_embeddings(
-            speech_feat, speech_position_ids, speech_masks)
+            speech_feat, speech_position_ids, speech_masks, speech_type_ids)
         # align back to most compact input
         gather_index = gather_index.unsqueeze(-1).expand(
             -1, -1, self.config.hidden_size)
@@ -428,14 +362,15 @@ class UniterModel(UniterPreTrainedModel):
                                     img_feat, img_position_ids,
                                     speech_feat, speech_position_ids,
                                     gather_index,
-                                    speech_masks=None, img_masks=None, 
-                                    img_type_ids=None, txt_type_ids=None):
+                                    img_masks=None, speech_masks=None,  
+                                    txt_type_ids=None, img_type_ids=None, 
+                                    speech_type_ids=None):
         txt_emb = self._compute_txt_embeddings(
             input_ids, position_ids, txt_type_ids)
-        speech_emb = self._compute_speech_embeddings(
-            speech_feat, speech_position_ids, speech_masks)
         img_emb = self._compute_img_embeddings(
             img_feat, img_position_ids, img_masks, img_type_ids)
+        speech_emb = self._compute_speech_embeddings(
+            speech_feat, speech_position_ids, speech_masks, speech_type_ids)
         # align back to most compact input
         gather_index = gather_index.unsqueeze(-1).expand(
             -1, -1, self.config.hidden_size)
@@ -444,8 +379,8 @@ class UniterModel(UniterPreTrainedModel):
         return embedding_output
 
     def forward(self, batch, img_masks=None, speech_masks=None,
-                frozen_en_layers=0, output_all_encoded_layers=True,
-                txt_type_ids=None, img_type_ids=None):
+                frozen_en_layers=0, output_all_encoded_layers=False,
+                txt_type_ids=None, img_type_ids=None, speech_type_ids=None):
         input_ids = batch['input_ids']
         position_ids = batch['position_ids']
         img_feat = batch['img_feat']
@@ -474,15 +409,17 @@ class UniterModel(UniterPreTrainedModel):
                     embedding_output = self._compute_speech_txt_embeddings(
                         input_ids, position_ids,
                         speech_feat, speech_position_ids,
-                        gather_index, speech_masks, txt_type_ids)
+                        gather_index, speech_masks, txt_type_ids,
+                        speech_type_ids)
                 elif self.use_speech and self.use_visual:
                     embedding_output = self._compute_speech_img_txt_embeddings(
                             input_ids, position_ids,
                             img_feat, img_position_ids,
                             speech_feat, speech_position_ids,
                             gather_index,
-                            speech_masks, img_masks, 
-                            img_type_ids, txt_type_ids)
+                            img_masks, speech_masks, 
+                            txt_type_ids, img_type_ids,
+                            speech_type_ids)
                 else:
                     logger.info('[Error] some error in UniterModel')
                     exit(0)
@@ -497,15 +434,18 @@ class UniterModel(UniterPreTrainedModel):
                 embedding_output = self._compute_speech_txt_embeddings(
                     input_ids, position_ids,
                     speech_feat, speech_position_ids,
-                    gather_index, speech_masks, txt_type_ids)
+                    gather_index, speech_masks, txt_type_ids, 
+                    speech_type_ids)
             elif self.use_speech and self.use_visual:
+                # logger.info('[Debug] use both visual and speech!!!')
                 embedding_output = self._compute_speech_img_txt_embeddings(
                         input_ids, position_ids,
                         img_feat, img_position_ids,
                         speech_feat, speech_position_ids,
                         gather_index,
-                        speech_masks, img_masks, 
-                        img_type_ids, txt_type_ids)
+                        img_masks, speech_masks, 
+                        txt_type_ids, img_type_ids,
+                        speech_type_ids)
             else:
                 logger.info('[Error] some error in UniterModel')
                 exit(0)
diff --git a/code/uniter3m/model/pretrain.py b/code/uniter3m/model/pretrain.py
index 47e0fc8..f832d52 100644
--- a/code/uniter3m/model/pretrain.py
+++ b/code/uniter3m/model/pretrain.py
@@ -11,53 +11,10 @@ from torch import nn
 from torch.nn import functional as F
 from apex.normalization.fused_layer_norm import FusedLayerNorm as LayerNorm
 
-from code.uniter3m.model.layer import GELU, BertOnlyMLMHead
 from code.uniter3m.model.model import UniterModel, UniterPreTrainedModel
-
-class RegionFeatureRegression(nn.Module):
-    " for MRM"
-    def __init__(self, hidden_size, feat_dim, img_linear_weight):
-        super().__init__()
-        self.net = nn.Sequential(nn.Linear(hidden_size, hidden_size),
-                                 GELU(),
-                                 LayerNorm(hidden_size, eps=1e-12))
-
-        self.weight = img_linear_weight
-        self.bias = nn.Parameter(torch.zeros(feat_dim))
-
-    def forward(self, input_):
-        hidden = self.net(input_)
-        output = F.linear(hidden, self.weight.t(), self.bias)
-        return output
-
-
-class RegionClassification(nn.Module):
-    " for MRC(-kl)"
-    def __init__(self, hidden_size, label_dim):
-        super().__init__()
-        self.net = nn.Sequential(nn.Linear(hidden_size, hidden_size),
-                                 GELU(),
-                                 LayerNorm(hidden_size, eps=1e-12),
-                                 nn.Linear(hidden_size, label_dim))
-
-    def forward(self, input_):
-        output = self.net(input_)
-        return output
-
-class EmoMelmClassification(nn.Module):
-    " for the multitask of MELM"
-    def __init__(self, hidden_size, label_dim):
-        super().__init__()
-        self.net = nn.Sequential(nn.Linear(hidden_size, hidden_size),
-                                 GELU(),
-                                 LayerNorm(hidden_size, eps=1e-12),
-                                 nn.Linear(hidden_size, label_dim))
-
-    def forward(self, input_):
-        output = self.net(input_)
-        # print('[Debug] in EmoMelmClassification input {}'.format(input_.shape))
-        # print('[Debug] in EmoMelmClassification output {}'.format(output.shape))
-        return output
+## from uniter 
+from code.uniter.model.pretrain import RegionFeatureRegression, RegionClassification, EmoMelmClassification
+from code.uniter.model.layer import GELU, BertOnlyMLMHead
 
 class UniterForPretraining(UniterPreTrainedModel):
     """ UNITER pretraining """
@@ -72,12 +29,18 @@ class UniterForPretraining(UniterPreTrainedModel):
             config, self.uniter.embeddings.word_embeddings.weight)
             
         if self.use_visual:
-            print('[Debug] use visual feature regression and region classification')
+            print('[Debug] use visual feature regression and region classification!!!')
             self.feat_regress = RegionFeatureRegression(
                 config.hidden_size, img_dim,
                 self.uniter.img_embeddings.img_linear.weight)
             self.region_classifier = RegionClassification(
                 config.hidden_size, img_label_dim)
+        
+        if self.use_speech:
+            print('[Debug] use speech feature regression!!!')
+            self.speech_feat_regress = RegionFeatureRegression(
+                config.hidden_size, speech_dim,
+                self.uniter.speech_embeddings.speech_linear.weight)
             
         # Jinming: add for melm multi-task
         if config.melm_multitask is True:
@@ -110,6 +73,7 @@ class UniterForPretraining(UniterPreTrainedModel):
                 txt_emo_labels = batch['txt_emo_labels']
             else:
                 txt_emo_labels = None
+            # print('[Debug in MELM forward] the txt_emo_labels {}'.format(txt_emo_labels))
             return self.forward_melm(batch, txt_labels, txt_emo_labels, compute_loss)
         elif task == 'mrfr':
             img_mask_tgt = batch['img_mask_tgt']
@@ -178,19 +142,18 @@ class UniterForPretraining(UniterPreTrainedModel):
                                              txt_labels[txt_labels != -1],
                                              reduction='none')
             # jinming: add multitask emo classification
-            if txt_emo_labels is not None:
+            if self.config.melm_multitask and txt_emo_labels is not None:
                 prediction_emo_scores = self.emomelm_classifier(masked_output)
                 masked_emo_loss = F.cross_entropy(prediction_emo_scores, 
                                                     txt_emo_labels[txt_emo_labels != -1],
                                                     reduction='none')
                 # print('[Debug] in MELM emoloss {}'.format(masked_emo_loss))
                 # print('[Debug] in MELM lmloss {}'.format(masked_lm_loss))
-                # loss melm_multitask_rate=1.0
                 masked_lm_loss += self.config.melm_multitask_rate * masked_emo_loss
             return masked_lm_loss
         else:
             # jinming: add multitask emo classification
-            if txt_emo_labels is not None:
+            if self.config.melm_multitask and txt_emo_labels is not None:
                 prediction_emo_scores = self.emomelm_classifier(masked_output)
                 return (prediction_scores, prediction_emo_scores)
             else:
@@ -227,7 +190,7 @@ class UniterForPretraining(UniterPreTrainedModel):
         # only compute masked tokens for better efficiency
         masked_output = self._compute_masked_hidden(sequence_output,
                                                     speech_mask_tgt)
-        prediction_feat = self.feat_regress(masked_output)
+        prediction_feat = self.speech_feat_regress(masked_output)
 
         if compute_loss:
             msrfr_loss = F.mse_loss(prediction_feat, feat_targets,
diff --git a/code/uniter3m/pretrain.py b/code/uniter3m/pretrain.py
index 8ee6a12..021236d 100644
--- a/code/uniter3m/pretrain.py
+++ b/code/uniter3m/pretrain.py
@@ -28,54 +28,27 @@ from code.uniter3m.data import (TokenBucketSampler, TokenBucketSamplerForItm,
                   TxtTokLmdb, ImageLmdbGroup, SpeechLmdbGroup, ConcatDatasetWithLens,
                   MlmDataset, MelmDataset, MrfrDataset, MrcDataset,
                   mlm_collate, melm_collate, mrfr_collate, mrc_collate,
-                  ItmDataset, itm_collate, MsrfrDataset, msrm)
-
+                  ItmDataset, itm_collate, MsrfrDataset, msrfr_collate)
 from code.uniter3m.model.pretrain import UniterForPretraining
-from code.uniter3m.optim import get_lr_sched
-from code.uniter3m.optim.misc import build_optimizer
 
-from code.uniter3m.utils.logger import LOGGER, TB_LOGGER, RunningMeter, add_log_to_file
-from code.uniter3m.utils.distributed import (all_reduce_and_rescale_tensors, all_gather_list,
+# from uniter
+from code.uniter.optim import get_lr_sched
+from code.uniter.optim.misc import build_optimizer
+from code.uniter.utils.const import IMG_LABEL_DIM, BUCKET_SIZE
+from code.uniter.utils.misc import NoOp, parse_with_config, set_dropout, set_random_seed
+from code.uniter.utils.distributed import (all_reduce_and_rescale_tensors, all_gather_list,
                                broadcast_tensors)
-from code.uniter3m.utils.save import ModelSaver, save_training_meta
-from code.uniter3m.utils.misc import NoOp, parse_with_config, set_dropout, set_random_seed
-from code.uniter3m.utils.const import IMG_LABEL_DIM, BUCKET_SIZE
-
-def build_dataloader(dataset, collate_fn, is_train, opts):
-    if is_train:
-        batch_size = opts.train_batch_size
-    else:
-        batch_size = opts.val_batch_size
-    sampler = TokenBucketSampler(dataset.lens, bucket_size=BUCKET_SIZE,
-                                 batch_size=batch_size, droplast=is_train,
-                                 size_multiple=4)
-    loader = DataLoader(dataset, batch_sampler=sampler,
-                        num_workers=opts.n_workers, pin_memory=opts.pin_mem,
-                        collate_fn=collate_fn)
-    return loader
-
-
-def build_dataloader_itm(dataset, collate_fn, is_train, opts):
-    if is_train:
-        batch_size = opts.train_batch_size
-    else:
-        batch_size = opts.val_batch_size
-    sampler = TokenBucketSamplerForItm(
-        dataset, bucket_size=BUCKET_SIZE,
-        batch_size=batch_size, droplast=is_train)
-    loader = DataLoader(dataset, batch_sampler=sampler,
-                        num_workers=opts.n_workers, pin_memory=opts.pin_mem,
-                        collate_fn=collate_fn)
-    return loader
-
+from code.uniter.utils.save import ModelSaver, save_training_meta
+from code.uniter.utils.logger import LOGGER, TB_LOGGER, RunningMeter, add_log_to_file
+from code.uniter.pretrain import build_dataloader, build_dataloader_itm
 
 def build_mlm_dataset(txt_db, img_db, speech_db, is_train, opts):
     if is_train:
-        if opts.use_speech and opts.use_visual:
+        if img_db is not None and speech_db is not None:
             datasets = [MlmDataset(t, i, s) for t, i, s in zip(txt_db, img_db, speech_db)]
-        elif opts.use_speech and not opts.use_visual:
+        elif img_db is None and speech_db is not None:
             datasets = [MlmDataset(t, None, s) for t, s in zip(txt_db, speech_db)]
-        elif not opts.use_speech and opts.use_visual:
+        elif img_db is not None and speech_db is None:
             datasets = [MlmDataset(t, i, None) for t, i in zip(txt_db, img_db)]
         else:
             LOGGER.info('[Error] Error mlm datasets')
@@ -87,27 +60,25 @@ def build_mlm_dataset(txt_db, img_db, speech_db, is_train, opts):
 
 def build_melm_dataset(txt_db, img_db, speech_db, is_train, opts):
     if is_train:
-        if opts.use_speech and opts.use_visual:
+        if img_db is not None and speech_db is not None:
             datasets = [MelmDataset(opts.melm_prob, t, i, s) for t, i, s in zip(txt_db, img_db, speech_db)]
-        elif opts.use_speech and not opts.use_visual:
+        elif img_db is None and speech_db is not None:
             datasets = [MelmDataset(opts.melm_prob, t, None, s) for t, s in zip(txt_db, speech_db)]
-        elif not opts.use_speech and opts.use_visual:
+        elif img_db is not None and speech_db is None:
             datasets = [MelmDataset(opts.melm_prob, t, i, None) for t, i in zip(txt_db, img_db)]
         else:
             LOGGER.info('[Error] Error melm datasets')
         dataset = ConcatDatasetWithLens(datasets)
     else:
         dataset = MelmDataset(opts.melm_prob, txt_db, img_db, speech_db)
-
     return dataset, melm_collate
 
 def build_mrfr_dataset(txt_db, img_db, speech_db, is_train, opts):
+    assert img_db != None
     if is_train:
-        if opts.use_speech and opts.use_visual:
+        if speech_db is not None:
             datasets = [MrfrDataset(opts.mrm_prob, t, i, s) for t, i, s in zip(txt_db, img_db, speech_db)]
-        elif opts.use_speech and not opts.use_visual:
-            datasets = [MrfrDataset(opts.mrm_prob, t, None, s) for t, s in zip(txt_db, speech_db)]
-        elif not opts.use_speech and opts.use_visual:
+        elif speech_db is None:
             datasets = [MrfrDataset(opts.mrm_prob, t, i, None) for t, i in zip(txt_db, img_db)]
         else:
             LOGGER.info('[Error] Error mrfr datasets')
@@ -117,14 +88,27 @@ def build_mrfr_dataset(txt_db, img_db, speech_db, is_train, opts):
 
     return dataset, mrfr_collate
 
+def build_msrfr_dataset(txt_db, img_db, speech_db, is_train, opts):
+    assert speech_db != None
+    if is_train:
+        if img_db is not None:
+            datasets = [MsrfrDataset(opts.msrm_prob, t, i, s) for t, i, s in zip(txt_db, img_db, speech_db)]
+        elif img_db is None:
+            datasets = [MsrfrDataset(opts.msrm_prob, t, None, s) for t, s in zip(txt_db, speech_db)]
+        else:
+            LOGGER.info('[Error] Error mrfr datasets')
+        dataset = ConcatDatasetWithLens(datasets)
+    else:
+        dataset = MsrfrDataset(opts.msrm_prob, txt_db, img_db, speech_db)
+
+    return dataset, msrfr_collate
 
 def build_mrc_dataset(txt_db, img_db, speech_db, is_train, opts):
+    assert img_db != None
     if is_train:
-        if opts.use_speech and opts.use_visual:
+        if speech_db is not None:
             datasets = [MrcDataset(opts.mrm_prob, t, i, s) for t, i, s in zip(txt_db, img_db, speech_db)]
-        elif opts.use_speech and not opts.use_visual:
-            datasets = [MrcDataset(opts.mrm_prob, t, None, s) for t, s in zip(txt_db, speech_db)]
-        elif not opts.use_speech and opts.use_visual:
+        elif speech_db is None:
             datasets = [MrcDataset(opts.mrm_prob, t, i, None) for t, i in zip(txt_db, img_db)]
         else:
             LOGGER.info('[Error] Error mrc datasets')
@@ -137,11 +121,11 @@ def build_mrc_dataset(txt_db, img_db, speech_db, is_train, opts):
 
 def build_itm_dataset(txt_db, img_db, speech_db, is_train, opts):
     if is_train:
-        if opts.use_speech and opts.use_visual:
+        if img_db is not None and speech_db is not None:
             datasets = [ItmDataset(t, i, s, opts.itm_neg_prob) for t, i, s in zip(txt_db, img_db, speech_db)]
-        elif opts.use_speech and not opts.use_visual:
+        elif img_db is None and speech_db is not None:
             datasets = [ItmDataset(t, None, s, opts.itm_neg_prob) for t, s in zip(txt_db, speech_db)]
-        elif not opts.use_speech and opts.use_visual:
+        elif img_db is not None and speech_db is None:
             datasets = [ItmDataset(t, i, None, opts.itm_neg_prob) for t, i in zip(txt_db, img_db)]
         else:
             LOGGER.info('[Error] Error itm datasets')
@@ -202,6 +186,8 @@ def create_dataloaders(datasets, is_train, opts, all_img_dbs=None, all_speech_db
                 dataset = build_melm_dataset(txt_db, img_db, speech_db, is_train, opts)
             elif task.startswith('mrfr'):
                 dataset = build_mrfr_dataset(txt_db, img_db, speech_db, is_train, opts)
+            elif task.startswith('msrfr'):
+                dataset = build_msrfr_dataset(txt_db, img_db, speech_db, is_train, opts)
             elif task.startswith('mrc'):
                 dataset = build_mrc_dataset(txt_db, img_db, speech_db, is_train, opts)
             elif task.startswith('itm'):
@@ -309,7 +295,7 @@ def main(opts):
     # to compute training statistics
     task2loss = {task: RunningMeter(f'loss/{task}')
                  for task in train_dataloaders.keys()}
-
+    LOGGER.info(f'[Debug] {task2loss}')
     n_examples = defaultdict(int)
     n_in_units = defaultdict(int)
     n_loss_units = defaultdict(int)
@@ -320,6 +306,7 @@ def main(opts):
     optimizer.zero_grad()
     optimizer.step()
     for step, (name, batch) in enumerate(meta_loader):
+        # LOGGER.info(f'[Debug] train on step: {step}')
         n_examples[name] += batch['input_ids'].size(0)
         n_in_units[name] += (batch['attn_masks'] == 1).sum().item()
         task = name.split('_')[0]
@@ -377,7 +364,7 @@ def main(opts):
             optimizer.zero_grad()
             pbar.update(1)
 
-            if global_step % 100 == 0:
+            if global_step % 200 == 0:
                 # monitor training throughput
                 LOGGER.info(f'==============Step {global_step}===============')
                 LOGGER.info('Current learning rate {}'.format(lr_this_step))
@@ -420,6 +407,8 @@ def validate(model, val_dataloaders):
             val_log = validate_melm(model, loader)
         elif task.startswith('mrfr') and args.use_visual:
             val_log = validate_mrfr(model, loader)
+        elif task.startswith('msrfr') and args.use_speech:
+            val_log = validate_msrfr(model, loader)
         elif task.startswith('mrc') and args.use_visual:
             val_log = validate_mrc(model, loader, task)
         elif task.startswith('itm'):
@@ -543,6 +532,25 @@ def validate_mrfr(model, val_loader):
                 f"loss: {val_loss:.2f}")
     return val_log
 
+@torch.no_grad()
+def validate_msrfr(model, val_loader):
+    LOGGER.info("start running MSRFR validation...")
+    val_loss = 0
+    n_feat = 0
+    st = time()
+    for i, batch in enumerate(val_loader):
+        loss = model(batch, task='msrfr', compute_loss=True)
+        val_loss += loss.sum().item() / IMG_DIM
+        n_feat += batch['speech_mask_tgt'].sum().item()
+    val_loss = sum(all_gather_list(val_loss))
+    n_feat = sum(all_gather_list(n_feat))
+    tot_time = time()-st
+    val_loss /= n_feat
+    val_log = {'loss': val_loss,
+               'feat_per_s': n_feat/tot_time}
+    LOGGER.info(f"validation finished in {int(tot_time)} seconds, "
+                f"loss: {val_loss:.2f}")
+    return val_log
 
 @torch.no_grad()
 def validate_mrc(model, val_loader, task):
@@ -649,6 +657,8 @@ if __name__ == "__main__":
                              'in ITM training')
     parser.add_argument('--itm_ot_lambda', default=0.0, type=float,
                         help='weight of OT (optimal transport) loss (WRA)')
+    parser.add_argument('--msrm_prob', default=0.15, type=float,
+                        help='probability to mask in MSRM training(for speech)')
 
     # Prepro parameters
     parser.add_argument('--max_txt_len', type=int, default=60,
diff --git a/code/uniter3m/pretrain.sh b/code/uniter3m/pretrain.sh
index 9f858a8..1af5fb4 100644
--- a/code/uniter3m/pretrain.sh
+++ b/code/uniter3m/pretrain.sh
@@ -24,14 +24,68 @@ export PYTHONPATH=/data7/MEmoBert
 #         --num_train_steps 20000 --warmup_steps 2000 --valid_steps 2000 \
 #         --output_dir /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_speech-berttype_text_2tasks_mlmitm_lr5e5_bs1024_faceth0.5
 
-CUDA_VISIBLE_DEVICES=2,3,4,5 horovodrun -np 4 python pretrain.py \
-        --cvNo 0 --n_workers 4 --use_speech  \
-        --config config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_text_2tasks_mlmitm.json \
+# CUDA_VISIBLE_DEVICES=2,3,4,5 horovodrun -np 4 python pretrain.py \
+#         --cvNo 0 --n_workers 4 --use_speech  \
+#         --config config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_text_2tasks_mlmitm.json \
+#         --model_config config/uniter-base-emoword_nomultitask.json \
+#         --learning_rate 5e-5 --lr_sched_type 'linear' --gradient_accumulation_steps 4 \
+#         --IMG_DIM 342 --Speech_DIM 768 \
+#         --conf_th 0.5 --max_txt_len 30 --max_bb 36 \
+#         --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#         --train_batch_size 64 --val_batch_size 64 \
+#         --num_train_steps 10000 --warmup_steps 1000 --valid_steps 1000 \
+#         --output_dir /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_speech_wav2vec-berttype_text_2tasks_mlmitm_lr5e5_bs1024_faceth0.5
+
+# CUDA_VISIBLE_DEVICES=4,5 horovodrun -np 2 python pretrain.py \
+#         --cvNo 0 --n_workers 4 --use_speech --use_visual \
+#         --config config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_5tasks.json \
+#         --model_config config/uniter-base-emoword_multitask.json \
+#         --learning_rate 5e-5 --lr_sched_type 'linear' --gradient_accumulation_steps 8 \
+#         --IMG_DIM 342 --Speech_DIM 768 \
+#         --conf_th 0.5 --max_txt_len 30 --max_bb 36 \
+#         --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#         --train_batch_size 64 --val_batch_size 64 \
+#         --num_train_steps 40000 --warmup_steps 4000 --valid_steps 4000 \
+#         --output_dir /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_5tasks_vstype1_lr5e5_bs512_faceth0.5_debug
+
+# CUDA_VISIBLE_DEVICES=0,1,2,3 horovodrun -np 4 python pretrain.py \
+#         --cvNo 0 --n_workers 4 --use_speech  \
+#         --config config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vecasr_text_3tasks_mlmitmmsm.json \
+#         --model_config config/uniter-base-emoword_nomultitask.json \
+#         --learning_rate 5e-5 --lr_sched_type 'linear' --gradient_accumulation_steps 4 \
+#         --IMG_DIM 342 --Speech_DIM 768 \
+#         --conf_th 0.5 --max_txt_len 30 --max_bb 36 \
+#         --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#         --train_batch_size 64 --val_batch_size 64 \
+#         --num_train_steps 10000 --warmup_steps 1000 --valid_steps 1000 \
+#         --output_dir /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_speech_wav2vecasr-berttype_text_3tasks_mlmitmmsrfr_lr5e5_bs1024_faceth0.5
+
+### text+visual.
+CUDA_VISIBLE_DEVICES=2,6 horovodrun -np 2 python pretrain.py \
+        --cvNo 0 --n_workers 4 --use_visual  \
+        --config config/pretrain-movies-v1v2v3-base-2gpu_4tasks.json \
         --model_config config/uniter-base-emoword_nomultitask.json \
         --learning_rate 5e-5 --lr_sched_type 'linear' --gradient_accumulation_steps 4 \
         --IMG_DIM 342 --Speech_DIM 768 \
         --conf_th 0.5 --max_txt_len 30 --max_bb 36 \
-        --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
         --train_batch_size 64 --val_batch_size 64 \
         --num_train_steps 10000 --warmup_steps 1000 --valid_steps 1000 \
-        --output_dir /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_speech_wav2vec-berttype_text_2tasks_mlmitm_lr5e5_bs1024_faceth0.5
\ No newline at end of file
+        --output_dir /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_bs1024_faceth0.5_test2
+
+#  emobert uniterbaidu40 --bug, 
+
+### based on three modality to train two modality
+# CUDA_VISIBLE_DEVICES=2,6 horovodrun -np 2 python pretrain.py \
+#         --cvNo 0 --n_workers 4 --use_speech  \
+#         --config config/pretrain-movies-v1v2v3-base-2gpu_speechwav2vec_text_3tasks_mlmitmmsm.json \
+#         --model_config config/uniter-base-emoword_nomultitask.json \
+#         --checkpoint /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_5tasks_vstype1_lr5e5_bs1024_faceth0.5/ckpt/model_step_20000.pt \
+#         --learning_rate 2e-5 --lr_sched_type 'linear' --gradient_accumulation_steps 4 \
+#         --IMG_DIM 342 --Speech_DIM 768 \
+#         --conf_th 0.5 --max_txt_len 30 --max_bb 36 \
+#         --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#         --train_batch_size 64 --val_batch_size 64 \
+#         --num_train_steps 10000 --warmup_steps 0 --valid_steps 1000 \
+#         --output_dir /data7/emobert/exp/pretrain/nomask-basedon_nomask_movies_v1v2v3_uniter3m_5tasks-text_wav2vec_3tasks_lr2e5_bs1024_faceth0.5
+
+
diff --git a/code/uniter3m/pretrain_emo.sh b/code/uniter3m/pretrain_emo.sh
new file mode 100644
index 0000000..502c5a5
--- /dev/null
+++ b/code/uniter3m/pretrain_emo.sh
@@ -0,0 +1,28 @@
+export PYTHONPATH=/data7/MEmoBert
+
+### text+visual.
+# CUDA_VISIBLE_DEVICES=0,1 horovodrun -np 2 python pretrain.py \
+#         --cvNo 0 --n_workers 4 --use_visual  \
+#         --config config/pretrain-movies-v1v2v3-base-2gpu_4tasks_emo.json \
+#         --model_config config/uniter-base-emoword_multitask.json \
+#         --learning_rate 5e-5 --lr_sched_type 'linear' --gradient_accumulation_steps 4 \
+#         --melm_prob 0.5 \
+#         --IMG_DIM 342 --Speech_DIM 768 \
+#         --conf_th 0.5 --max_txt_len 30 --max_bb 36 \
+#         --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#         --train_batch_size 64 --val_batch_size 64 \
+#         --num_train_steps 20000 --warmup_steps 2000 --valid_steps 2000 \
+#         --output_dir /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_faceth0.5_melm.5
+
+CUDA_VISIBLE_DEVICES=2,5 horovodrun -np 2 python pretrain.py \
+        --cvNo 0 --n_workers 4 --use_visual  \
+        --config config/pretrain-movies-v1v2v3-base-2gpu_4tasks_emo_sentiword.json \
+        --model_config config/uniter-base-emoword_multitask.json \
+        --learning_rate 5e-5 --lr_sched_type 'linear' --gradient_accumulation_steps 4 \
+        --melm_prob 0.5 \
+        --IMG_DIM 342 --Speech_DIM 768 \
+        --conf_th 0.5 --max_txt_len 30 --max_bb 36 \
+        --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+        --train_batch_size 64 --val_batch_size 64 \
+        --num_train_steps 20000 --warmup_steps 2000 --valid_steps 2000 \
+        --output_dir /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_faceth0.5_melm.5_sentiword
\ No newline at end of file
diff --git a/code/uniter3m/pretrain_ontask_iemocap.sh b/code/uniter3m/pretrain_ontask_iemocap.sh
index 65ff0d1..ab64b6a 100644
--- a/code/uniter3m/pretrain_ontask_iemocap.sh
+++ b/code/uniter3m/pretrain_ontask_iemocap.sh
@@ -3,21 +3,18 @@ export PYTHONPATH=/data7/MEmoBert
 gpu_id=$1
 corpus_name='iemocap'
 
-## First use trn-data to pretrain on the task dataset
-## Then use same trn-data train on downsteam task 
-
-# iemocap maxbb=64, conf_th=0.0, 4500 / 128  * 10 = 400
-for cvNo in `seq 1 10`;
+for cvNo in $(seq 2 5)
 do
         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python pretrain.py \
-                --cvNo ${cvNo} --use_speech --use_visual \
-                --config config/pretrain-task-${corpus_name}_norm-base-2gpu_4tasks.json \
-                --model_config config/uniter-base-emoword_nomultitask.json \
-                --checkpoint /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024_faceth0.5/ckpt/model_step_20000.pt \
+                --cvNo ${cvNo} --use_visual \
+                --config config/pretrain-task-${corpus_name}-base-2gpu_4tasks-emo_sentiword.json \
+                --model_config config/uniter-base-emoword_multitask.json \
+                --checkpoint  /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_faceth0.5_melm.5_sentiword/ckpt/model_step_20000.pt \
                 --learning_rate 2e-05 --lr_sched_type 'linear'  --gradient_accumulation_steps 4 \
+                --IMG_DIM 342 --Speech_DIM 768 \
                 --conf_th 0.0 --max_bb 36 \
                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
                 --train_batch_size 64 --val_batch_size 64 \
-                --num_train_steps 1500 --warmup_steps 200 --valid_steps 500 \
-                --output_dir /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-nomask_movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024-4tasks_maxbb36_faceth0.0_trnval/${cvNo}
-done
+                --num_train_steps 2000 --warmup_steps 200 --valid_steps 2000 \
+                --output_dir /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_melm.5_sentiword-faceth0.0_trnval/${cvNo}
+done
\ No newline at end of file
diff --git a/code/uniter3m/pretrain_ontask_meld.sh b/code/uniter3m/pretrain_ontask_meld.sh
index a141847..55e093d 100644
--- a/code/uniter3m/pretrain_ontask_meld.sh
+++ b/code/uniter3m/pretrain_ontask_meld.sh
@@ -1,26 +1,19 @@
 export PYTHONPATH=/data7/MEmoBert
-
 gpu_id=$1
-
-## First use trn-data to pretrain on the task dataset
-## Then use same trn-data train on downsteam task, train_emo_meld.sh
-
-### 10000 / 128 * 10 = 1000
 corpus_name='meld'
-for cvNo in `seq 1 1`;
+
+for cvNo in $(seq 1 1)
 do
-        for conth in 0.5;
-        do
         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python pretrain.py \
-                --cvNo ${cvNo} --use_speech --use_visual \
-                --config config/pretrain-task-${corpus_name}_norm-base-2gpu_4tasks.json \
-                --model_config config/uniter-base-emoword_nomultitask.json \
-                --checkpoint /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024_faceth0.5/ckpt/model_step_20000.pt \
+                --cvNo ${cvNo} --use_visual \
+                --config config/pretrain-task-${corpus_name}-base-2gpu_4tasks-emo.json \
+                --model_config config/uniter-base-emoword_multitask.json \
+                --checkpoint  /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_faceth0.5_melm.5/ckpt/model_step_20000.pt \
                 --learning_rate 2e-05 --lr_sched_type 'linear'  --gradient_accumulation_steps 4 \
-                --conf_th ${conth} --max_bb 36 \
+                --IMG_DIM 342 --Speech_DIM 768 \
+                --conf_th 0.5 --max_bb 36 \
                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
                 --train_batch_size 64 --val_batch_size 64 \
-                --num_train_steps 2500 --warmup_steps 200 --valid_steps 500 \
-                --output_dir /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-nomask_movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024-4tasks_maxbb36_faceth${conth}_trnval/${cvNo}
-        done
+                --num_train_steps 2500 --warmup_steps 250 --valid_steps 2500 \
+                --output_dir /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_melm.5-faceth0.0_trnval/${cvNo}
 done
\ No newline at end of file
diff --git a/code/uniter3m/pretrain_ontask_msp.sh b/code/uniter3m/pretrain_ontask_msp.sh
index e8eb7a6..f501e55 100644
--- a/code/uniter3m/pretrain_ontask_msp.sh
+++ b/code/uniter3m/pretrain_ontask_msp.sh
@@ -3,19 +3,18 @@ export PYTHONPATH=/data7/MEmoBert
 gpu_id=$1
 corpus_name='msp'
 
-## First use trn-data to pretrain on the task dataset
-## Then use same trn-data train on downsteam task 
-for cvNo in `seq 1 12`;
+for cvNo in $(seq 7 12)
 do
         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python pretrain.py \
-                --cvNo ${cvNo} --use_speech --use_visual \
-                --config config/pretrain-task-${corpus_name}_norm-base-2gpu_4tasks.json \
-                --model_config config/uniter-base-emoword_nomultitask.json \
-                --checkpoint /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024_faceth0.5/ckpt/model_step_20000.pt \
+                --cvNo ${cvNo} --use_visual \
+                --config config/pretrain-task-${corpus_name}-base-2gpu_4tasks-emo_sentiword.json \
+                --model_config config/uniter-base-emoword_multitask.json \
+                --checkpoint  /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_faceth0.5_melm.5_sentiword/ckpt/model_step_20000.pt \
                 --learning_rate 2e-05 --lr_sched_type 'linear'  --gradient_accumulation_steps 4 \
+                --IMG_DIM 342 --Speech_DIM 768 \
                 --conf_th 0.0 --max_bb 36 \
                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
                 --train_batch_size 64 --val_batch_size 64 \
-                --num_train_steps 1000 --warmup_steps 200 --valid_steps 500 \
-                --output_dir /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-nomask_movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024-4tasks_maxbb36_faceth0.0_trnval/${cvNo}
+                --num_train_steps 2000 --warmup_steps 200 --valid_steps 2000 \
+                --output_dir /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_melm.5_sentiword-faceth0.0_trnval/${cvNo}
 done
\ No newline at end of file
diff --git a/code/uniter3m/train_emo.py b/code/uniter3m/train_emo.py
index 7d1cd37..36ca0f9 100644
--- a/code/uniter3m/train_emo.py
+++ b/code/uniter3m/train_emo.py
@@ -75,39 +75,61 @@ def main(opts):
         model_saver = NoOp()
 
     LOGGER.info("Loading Train Dataset {} {}".format(opts.train_txt_dbs, opts.train_img_dbs,
-                         opts.train_speech_dbs))
-    train_all_img_dbs = ImageLmdbGroup(opts.conf_th, opts.max_bb, opts.min_bb, \
-                    compress=opts.compressed_db)
-    train_speech_dbs = SpeechLmdbGroup(opts.speech_conf_th, opts.max_frames, opts.min_frames, \
-                    compress=opts.compressed_db)
+                                                        opts.train_speech_dbs))
+    if opts.use_visual:
+        train_all_img_dbs = ImageLmdbGroup(opts.conf_th, opts.max_bb, opts.min_bb, \
+                        compress=opts.compressed_db)
+    if opts.use_speech:
+        train_speech_dbs = SpeechLmdbGroup(opts.speech_conf_th, opts.max_frames, opts.min_frames, \
+                        compress=opts.compressed_db)
     train_datasets = []
     for txt_path, img_path, speech_path in zip(opts.train_txt_dbs, opts.train_img_dbs, opts.train_speech_dbs):
         txt_path = txt_path.format(opts.cvNo)
-        img_db = train_all_img_dbs[img_path]
-        speech_db = train_speech_dbs[speech_path]
+        if opts.use_visual:
+            img_db = train_all_img_dbs[img_path]
+        else:
+            img_db = None
+        if opts.use_speech:
+            speech_db = train_speech_dbs[speech_path]
+        else:
+            speech_db = None
         txt_db = TxtTokLmdb(txt_path, opts.max_txt_len)
         print(type(txt_db), type(img_db), type(speech_path))
         train_datasets.append(EmoCLsDataset(txt_db, img_db, speech_db))
     train_dataset = ConcatDataset(train_datasets)
 
     LOGGER.info("Loading no image_data_augmentation for validation and testing")
-    eval_all_img_dbs = ImageLmdbGroup(opts.conf_th, opts.max_bb, opts.min_bb, \
-                                    compress=opts.compressed_db)
-    eval_all_speech_dbs = SpeechLmdbGroup(opts.speech_conf_th, opts.max_frames, opts.min_frames, \
+    if opts.use_visual:
+        eval_all_img_dbs = ImageLmdbGroup(opts.conf_th, opts.max_bb, opts.min_bb, \
+                                        compress=opts.compressed_db)
+    if opts.use_speech:
+        eval_all_speech_dbs = SpeechLmdbGroup(opts.speech_conf_th, opts.max_frames, opts.min_frames, \
                                     compress=opts.compressed_db)
     # val
     opts.val_txt_db = opts.val_txt_db.format(opts.cvNo)
     LOGGER.info(f"Loading Val Dataset {opts.val_img_db}, {opts.val_txt_db}, {opts.val_speech_db}")
-    val_img_db = eval_all_img_dbs[opts.val_img_db]
-    val_speech_db = eval_all_speech_dbs[opts.val_speech_db]
+    if opts.use_visual:
+        val_img_db = eval_all_img_dbs[opts.val_img_db]
+    else:
+        val_img_db = None
+    if opts.use_speech:
+        val_speech_db = eval_all_speech_dbs[opts.val_speech_db]
+    else:
+        val_speech_db = None
     val_txt_db = TxtTokLmdb(opts.val_txt_db, -1)
     val_dataset = EmoCLsDataset(val_txt_db, val_img_db, val_speech_db)
     val_dataloader = build_dataloader(val_dataset, emocls_collate, False, opts)
     # test
     opts.test_txt_db = opts.test_txt_db.format(opts.cvNo)
     LOGGER.info(f"Loading Test Dataset {opts.test_img_db}, {opts.test_txt_db} {opts.test_speech_db}")
-    test_img_db = eval_all_img_dbs[opts.test_img_db]
-    test_speech_db = eval_all_speech_dbs[opts.test_speech_db]
+    if opts.use_visual:
+        test_img_db = eval_all_img_dbs[opts.test_img_db]
+    else:
+        test_img_db = None
+    if opts.use_speech:
+        test_speech_db = eval_all_speech_dbs[opts.test_speech_db]
+    else:
+        test_speech_db = None
     test_txt_db = TxtTokLmdb(opts.test_txt_db, -1)
     test_dataset = EmoCLsDataset(test_txt_db, test_img_db, test_speech_db)
     test_dataloader = build_dataloader(test_dataset, emocls_collate, False, opts)
@@ -148,7 +170,7 @@ def main(opts):
 
     global_step = 0
     n_examples = 0
-    best_eval_WA = 0
+    best_eval_metrix = 0
     best_eval_step = 0
     steps2test_results = {}
     steps2val_results = {}
@@ -224,9 +246,9 @@ def main(opts):
                     steps2val_results[global_step] = val_log
                     steps2test_results[global_step] = test_log
                     # update the current best model based on validation results
-                    if val_log['WA'] > best_eval_WA:
+                    if val_log[select_metrix] > best_eval_metrix:
                         best_eval_step = global_step
-                        best_eval_WA = val_log['WA']
+                        best_eval_metrix = val_log[select_metrix]
                         patience = opts.patience
                         LOGGER.info('Save model at {} global step'.format(global_step))
                         model_saver.save(model, global_step)
@@ -242,7 +264,7 @@ def main(opts):
     pbar.close()
     LOGGER.info(f"finished {opts.num_train_steps} steps in {time()- start} seconds!")
     ### final use the best model tested on validation set.
-    LOGGER.info('Val: Best eval steps {} found with WA {}'.format(best_eval_step,  best_eval_WA))
+    LOGGER.info('Val: Best eval steps {} found with {} {}'.format(best_eval_step, select_metrix, best_eval_metrix))
     LOGGER.info('Val: {}'.format(steps2val_results[best_eval_step]))
     LOGGER.info('Test: {}'.format(steps2test_results[best_eval_step]))
     steps2val_results['beststep'] = best_eval_step
@@ -373,6 +395,8 @@ if __name__ == "__main__":
                         help="number of data workers")
     parser.add_argument('--pin_mem', action='store_true',
                         help="pin memory")
+    parser.add_argument('--corpus_name',  default='iemocap', type=str,
+                        help="downstream task name")
 
     # can use config files
     parser.add_argument('--config', help='JSON config files')
@@ -393,6 +417,15 @@ if __name__ == "__main__":
     IMG_DIM = args.IMG_DIM
     Speech_DIM = args.Speech_DIM
 
+    if args.corpus_name == 'meld':
+        select_metrix = 'WF1'
+    elif args.corpus_name == 'msp':
+        select_metrix = 'WA'
+    elif args.corpus_name == 'iemocap':
+        select_metrix = 'UA'
+    LOGGER.info(f'[INFO] Corpus {args.corpus_name} and select metrix {select_metrix}')
+    LOGGER.info(f'[INFO] output {args.output_dir}')
+
     # options safe guard
     if args.conf_th == -1:
         assert args.max_bb + args.max_txt_len + 2 <= 512
diff --git a/code/uniter3m/train_emo_iemocap.sh b/code/uniter3m/train_emo_iemocap.sh
index 498cfa6..575a647 100644
--- a/code/uniter3m/train_emo_iemocap.sh
+++ b/code/uniter3m/train_emo_iemocap.sh
@@ -56,29 +56,203 @@ corpus_name='iemocap'
 #         done
 # done
 
-for norm_type in selfnorm; 
+# for lr in 1e-4;
+# do
+#         for cvNo in `seq 1 10`;
+#         do
+#         num_train_steps=1200
+#         valid_steps=100
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_nomultitask.json \
+#                 --cls_num 4 --use_visual --use_speech \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu.json \
+#                 --checkpoint  /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_5tasks_vstype1_lr5e5_bs1024_faceth0.5/ckpt/model_step_20000.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps ${valid_steps} --patience 5 \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/IEMOCAP/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_wav2vec_5tasks-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+# for lr in 2e-5 5e-5
+# do
+#         for cvNo in $(seq 1 10)
+#         do
+#         num_train_steps=2500
+#         valid_steps=250
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_nomultitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 4 --use_speech \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu.json \
+#                 --checkpoint   /data7/emobert/resources/pretrained/uniter-base-uncased-init.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/IEMOCAP/finetune/nomask-directtrain-speech_wav2vec-berttype_text-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+# for lr in 5e-5
+# do
+#         for cvNo in $(seq 1 5)
+#         do
+#         num_train_steps=2500
+#         valid_steps=250
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_nomultitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 4 --use_speech \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vecasr-base-2gpu.json \
+#                 --checkpoint   /data7/emobert/resources/pretrained/uniter-base-uncased-init.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5 \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/IEMOCAP/finetune/nomask-directtrain-speech_wav2vecasr-berttype_text-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+# for lr in 2e-5 5e-5
+# do
+#         for cvNo in $(seq 1 10)
+#         do
+#         num_train_steps=1500
+#         valid_steps=100
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_nomultitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 4 --use_speech \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu.json \
+#                 --checkpoint  /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-movies_v1v2v3_uniter3m_speech_wav2vec-berttype-4tasks_maxbb36_faceth0.0_trnval/${cvNo}/ckpt/model_step_2000.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/IEMOCAP/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_speech_wav2vec-berttype_text-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+# for lr in 2e-5 5e-5
+# do
+#         for cvNo in $(seq 1 12)
+#         do
+#         num_train_steps=1500
+#         valid_steps=100
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_multitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 4 --use_visual \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu-emo.json \
+#                 --checkpoint  /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_faceth0.5_melm.5/ckpt/model_step_20000.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/IEMOCAP/finetune/nomask-movies_v1v2v3_uniter3m_4tasks_melm-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+# for lr in 5e-5
+# do
+#         for cvNo in $(seq 1 10)
+#         do
+#         num_train_steps=1500
+#         valid_steps=100
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_multitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 4 --use_visual \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu-emo_sentiword.json \
+#                 --checkpoint  /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_faceth0.5_melm.5_sentiword/ckpt/model_step_20000.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/IEMOCAP/finetune/nomask-movies_v1v2v3_uniter3m_4tasks_melm_sentiword-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+# for lr in 2e-5 5e-5
+# do
+#         for cvNo in $(seq 1 10)
+#         do
+#         num_train_steps=1000
+#         valid_steps=100
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_multitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 4 --use_visual \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu-emo.json \
+#                 --checkpoint /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_melm.5-faceth0.0_trnval/${cvNo}/ckpt/model_step_2000.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/IEMOCAP/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_4tasks_melm-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+for lr in 5e-5
 do
-        for lr in 1e-5;
+        for cvNo in $(seq 2 5)
         do
-                for cvNo in `seq 1 10`;
-                do
-                num_train_steps=1500
-                valid_steps=150
-                train_batch_size=32
-                inf_batch_size=32
-                frozens=0
-                CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
-                        --cvNo ${cvNo} --model_config config/uniter-base-emoword_nomultitask.json \
-                        --cls_num 4 --use_visual --use_speech \
-                        --config config/train-emo-${corpus_name}-openface_${norm_type}-base-2gpu.json \
-                        --checkpoint  /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-nomask_movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024-4tasks_maxbb36_faceth0.0_trnval/${cvNo}/ckpt/model_step_1500.pt \
-                        --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
-                        --learning_rate ${lr} --lr_sched_type 'linear' \
-                        --conf_th 0.0 --max_bb 36 --min_bb 10 \
-                        --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
-                        --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
-                        --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
-                        --output_dir /data7/emobert/exp/evaluation/IEMOCAP/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024-4tasks_maxbb36_faceth0.0_trnval-lr${lr}_train${num_train_steps}_${norm_type}_trnval
-                done
+        num_train_steps=1000
+        valid_steps=100
+        train_batch_size=32
+        inf_batch_size=32
+        frozens=0
+        CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+                --cvNo ${cvNo} --model_config config/uniter-base-emoword_multitask.json \
+                --corpus_name ${corpus_name} --cls_num 4 --use_visual \
+                --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu-emo_sentiword.json \
+                --checkpoint /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_melm.5_sentiword-faceth0.0_trnval/${cvNo}/ckpt/model_step_2000.pt \
+                --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+                --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+                --IMG_DIM 342 --Speech_DIM 768 \
+                --conf_th 0.0 --max_bb 36 --min_bb 10 \
+                --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+                --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+                --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+                --output_dir /data7/emobert/exp/evaluation/IEMOCAP/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_4tasks_melm_sentiword-lr${lr}_train${num_train_steps}_trnval
         done
-done
\ No newline at end of file
+done
+
diff --git a/code/uniter3m/train_emo_meld.sh b/code/uniter3m/train_emo_meld.sh
index 3176309..8805477 100644
--- a/code/uniter3m/train_emo_meld.sh
+++ b/code/uniter3m/train_emo_meld.sh
@@ -51,26 +51,161 @@ corpus_name='meld'
 #         done
 # done
 
-for norm_type in selfnorm; 
+# for norm_type in selfnorm; 
+# do
+#         for lr in 1e-5;
+#         do
+#                 num_train_steps=2000
+#                 valid_steps=200
+#                 train_batch_size=32
+#                 inf_batch_size=32
+#                 frozens=0
+#                 CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                         --cvNo 1 --model_config config/uniter-base-emoword_nomultitask.json \
+#                         --cls_num 7 --use_visual --use_speech \
+#                         --config config/train-emo-${corpus_name}-openface_${norm_type}-base-2gpu.json \
+#                         --checkpoint /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-nomask_movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024-4tasks_maxbb36_faceth0.5_trnval/1/ckpt/model_step_2500.pt \
+#                         --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                         --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps ${valid_steps} \
+#                         --conf_th 0.5 --max_bb 36 --min_bb 10 \
+#                         --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                         --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                         --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                         --output_dir /data7/emobert/exp/evaluation/MELD/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024-4tasks_maxbb36_faceth0.5_trnval-lr${lr}_train${num_train_steps}_${norm_type}_trnval
+#         done
+# done
+
+# for lr in 1e-5 2e-5 5e-5;
+# do
+#         num_train_steps=3000
+#         valid_steps=300
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo 1 --model_config config/uniter-base-emoword_nomultitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 7 --use_speech \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu.json \
+#                 --checkpoint   /data7/emobert/resources/pretrained/uniter-base-uncased-init.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 300 \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/MELD/finetune/nomask-directtrain-uniter3m_speech_wav2vec-lr${lr}_train${num_train_steps}_trnval
+# done
+
+# for lr in 1e-5 2e-5 5e-5;
+# do
+#         num_train_steps=3000
+#         valid_steps=300
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo 1 --model_config config/uniter-base-emoword_nomultitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 7 --use_speech \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vecasr-base-2gpu.json \
+#                 --checkpoint   /data7/emobert/resources/pretrained/uniter-base-uncased-init.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 300 \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/MELD/finetune/nomask-directtrain-uniter3m_speech_wav2vecasr-lr${lr}_train${num_train_steps}_trnval
+# done
+
+# for lr in 2e-5;
+# do
+#         num_train_steps=2000
+#         valid_steps=200
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo 1 --model_config config/uniter-base-emoword_nomultitask.json \
+#                 --cls_num 7 --use_visual --use_speech \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu.json \
+#                 --checkpoint  /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_5tasks_vstype1_lr5e5_bs1024_faceth0.5/ckpt/model_step_20000.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps ${valid_steps} --patience 5 \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.5 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/MELD/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_wav2vec_5tasks-lr${lr}_train${num_train_steps}_trnval
+# done
+
+# for lr in 5e-6
+# do
+#         for cvNo in $(seq 1 1)
+#         do
+#         num_train_steps=1500
+#         valid_steps=100
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_nomultitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 7 --use_speech \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu.json \
+#                 --checkpoint  /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-movies_v1v2v3_uniter3m_speech_wav2vec-berttype-4tasks_maxbb36_faceth0.5_trnval/1/ckpt/model_step_2500.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/MELD/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_speech_wav2vec-berttype_text-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+# for lr in 1e-5 2e-5 5e-5
+# do
+#         num_train_steps=2000
+#         valid_steps=200
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo 1 --model_config config/uniter-base-emoword_multitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 7 --use_visual \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu-emo.json \
+#                 --checkpoint  /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_faceth0.5_melm.5/ckpt/model_step_20000.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.5 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/MELD/finetune/nomask-movies_v1v2v3_uniter3m_4tasks_melm-lr${lr}_train${num_train_steps}_trnval
+# done
+
+for lr in 1e-5 2e-5 5e-5
 do
-        for lr in 1e-5;
-        do
-                num_train_steps=2000
-                valid_steps=200
-                train_batch_size=32
-                inf_batch_size=32
-                frozens=0
-                CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
-                        --cvNo 1 --model_config config/uniter-base-emoword_nomultitask.json \
-                        --cls_num 7 --use_visual --use_speech \
-                        --config config/train-emo-${corpus_name}-openface_${norm_type}-base-2gpu.json \
-                        --checkpoint /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-nomask_movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024-4tasks_maxbb36_faceth0.5_trnval/1/ckpt/model_step_2500.pt \
-                        --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
-                        --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps ${valid_steps} \
-                        --conf_th 0.5 --max_bb 36 --min_bb 10 \
-                        --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
-                        --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
-                        --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
-                        --output_dir /data7/emobert/exp/evaluation/MELD/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024-4tasks_maxbb36_faceth0.5_trnval-lr${lr}_train${num_train_steps}_${norm_type}_trnval
-        done
+        num_train_steps=1500
+        valid_steps=150
+        train_batch_size=32
+        inf_batch_size=32
+        frozens=0
+        CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+                --cvNo 1 --model_config config/uniter-base-emoword_multitask.json \
+                --corpus_name ${corpus_name} --cls_num 7 --use_visual \
+                --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu-emo.json \
+                --checkpoint /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_melm.5-faceth0.0_trnval/1/ckpt/model_step_2500.pt \
+                --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+                --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+                --IMG_DIM 342 --Speech_DIM 768 \
+                --conf_th 0.5 --max_bb 36 --min_bb 10 \
+                --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+                --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+                --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+                --output_dir /data7/emobert/exp/evaluation/MELD/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_4tasks_melm-lr${lr}_train${num_train_steps}_trnval
 done
\ No newline at end of file
diff --git a/code/uniter3m/train_emo_msp.sh b/code/uniter3m/train_emo_msp.sh
index 4945be0..426ca3e 100644
--- a/code/uniter3m/train_emo_msp.sh
+++ b/code/uniter3m/train_emo_msp.sh
@@ -58,29 +58,210 @@ corpus_name='msp'
 # done
 
 
-for norm_type in selfnorm; 
+#### for 3 modalities 
+# for lr in 1e-4;
+# do
+#         for cvNo in `seq 1 12`;
+#         do
+#         num_train_steps=1000
+#         valid_steps=100
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_nomultitask.json \
+#                 --cls_num 4 --use_visual --use_speech \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu.json \
+#                 --checkpoint  /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_5tasks_vstype1_lr5e5_bs1024_faceth0.5/ckpt/model_step_20000.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps ${valid_steps} --patience 5 \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/MSP/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_wav2vec_5tasks-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+### for speech + text modalities
+# for lr in 5e-5; do
+#         for cvNo in `seq 1 12`; do
+#         num_train_steps=2000
+#         valid_steps=200
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_nomultitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 4 --use_speech \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu.json \
+#                 --checkpoint /data7/emobert/resources/pretrained/uniter-base-uncased-init.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/MSP/finetune/nomask-directtrain-speech_wav2vec-berttype_text-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+#  for lr in 5e-5; 
+#  do
+#         for cvNo in `seq 1 12`; 
+#         do
+#         num_train_steps=2000
+#         valid_steps=200
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_nomultitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 4 --use_speech \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vecasr-base-2gpu.json \
+#                 --checkpoint   /data7/emobert/resources/pretrained/uniter-base-uncased-init.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5 \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/MSP/finetune/nomask-directtrain-speech_wav2vecasr-berttype_text-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+### for speech + text modalities
+# for lr in 2e-5 5e-5
+# do
+#         for cvNo in $(seq 1 12)
+#         do
+#         num_train_steps=1500
+#         valid_steps=100
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_nomultitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 4 --use_speech \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu.json \
+#                 --checkpoint  /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-movies_v1v2v3_uniter3m_speech_wav2vec-berttype-4tasks_maxbb36_faceth0.0_trnval/${cvNo}/ckpt/model_step_1500.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/MSP/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_speech_wav2vec-berttype_text-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+
+
+### for visual + text + melm modalities
+# for lr in 2e-5 5e-5
+# do
+#         for cvNo in $(seq 1 12)
+#         do
+#         num_train_steps=1500
+#         valid_steps=100
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_multitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 4 --use_visual \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu-emo.json \
+#                 --checkpoint  /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_faceth0.5_melm.5/ckpt/model_step_20000.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/MSP/finetune/nomask-movies_v1v2v3_uniter3m_4tasks_melm-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+
+# for lr in 5e-5
+# do
+#         for cvNo in $(seq 1 12)
+#         do
+#         num_train_steps=1200
+#         valid_steps=100
+#         train_batch_size=32
+#         inf_batch_size=32
+#         frozens=0
+#         CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+#                 --cvNo ${cvNo} --model_config config/uniter-base-emoword_multitask.json \
+#                 --corpus_name ${corpus_name} --cls_num 4 --use_visual \
+#                 --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu-emo_sentiword.json \
+#                 --checkpoint  /data7/emobert/exp/pretrain/nomask_movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_faceth0.5_melm.5_sentiword/ckpt/model_step_20000.pt \
+#                 --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+#                 --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+#                 --IMG_DIM 342 --Speech_DIM 768 \
+#                 --conf_th 0.0 --max_bb 36 --min_bb 10 \
+#                 --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+#                 --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+#                 --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+#                 --output_dir /data7/emobert/exp/evaluation/MSP/finetune/nomask-movies_v1v2v3_uniter3m_4tasks_melm_sentiword-lr${lr}_train${num_train_steps}_trnval
+#         done
+# done
+
+for lr in 2e-5 5e-5
 do
-        for lr in 1e-5;
+        for cvNo in $(seq 1 12)
         do
-                for cvNo in `seq 7 12`;
-                do
-                num_train_steps=1200
-                valid_steps=120
-                train_batch_size=32
-                inf_batch_size=32
-                frozens=0
-                CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
-                        --cvNo ${cvNo} --model_config config/uniter-base-emoword_nomultitask.json \
-                        --cls_num 4 --use_visual --use_speech \
-                        --config config/train-emo-${corpus_name}-openface_${norm_type}-base-2gpu.json \
-                        --checkpoint  /data7/emobert/exp/task_pretrain/${corpus_name}_basedon-nomask_movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024-4tasks_maxbb36_faceth0.0_trnval/${cvNo}/ckpt/model_step_1000.pt \
-                        --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
-                        --learning_rate ${lr} --lr_sched_type 'linear' \
-                        --conf_th 0.0 --max_bb 36 --min_bb 10 \
-                        --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
-                        --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
-                        --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
-                        --output_dir /data7/emobert/exp/evaluation/MSP/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_4tasks_lr5e5_bs1024-4tasks_maxbb36_faceth0.0_trnval-lr${lr}_train${num_train_steps}_${norm_type}_trnval
-                done
+        num_train_steps=1000
+        valid_steps=100
+        train_batch_size=32
+        inf_batch_size=32
+        frozens=0
+        CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+                --cvNo ${cvNo} --model_config config/uniter-base-emoword_multitask.json \
+                --corpus_name ${corpus_name} --cls_num 4 --use_visual \
+                --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu-emo.json \
+                --checkpoint /data7/emobert/exp/task_pretrain/msp_basedon-movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_melm.5-faceth0.0_trnval/${cvNo}/ckpt/model_step_2000.pt \
+                --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+                --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+                --IMG_DIM 342 --Speech_DIM 768 \
+                --conf_th 0.0 --max_bb 36 --min_bb 10 \
+                --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+                --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+                --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+                --output_dir /data7/emobert/exp/evaluation/MSP/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_4tasks_melm-lr${lr}_train${num_train_steps}_trnval
         done
-done
\ No newline at end of file
+done
+
+
+for lr in 2e-5 5e-5
+do
+        for cvNo in $(seq 1 12)
+        do
+        num_train_steps=1000
+        valid_steps=100
+        train_batch_size=32
+        inf_batch_size=32
+        frozens=0
+        CUDA_VISIBLE_DEVICES=${gpu_id} horovodrun -np 1 python train_emo.py \
+                --cvNo ${cvNo} --model_config config/uniter-base-emoword_multitask.json \
+                --corpus_name ${corpus_name} --cls_num 4 --use_visual \
+                --config config/train-emo-${corpus_name}-openface_wav2vec-base-2gpu-emo_sentiword.json \
+                --checkpoint /data7/emobert/exp/task_pretrain/msp_basedon-movies_v1v2v3_uniter3m_visual_text_4tasks_lr5e5_melm.5_sentiword-faceth0.0_trnval/${cvNo}/ckpt/model_step_2000.pt \
+                --frozen_en_layers ${frozens} --cls_dropout ${dropout} --cls_type vqa --postfix none \
+                --learning_rate ${lr} --lr_sched_type 'linear' --warmup_steps 0 --patience 5  \
+                --IMG_DIM 342 --Speech_DIM 768 \
+                --conf_th 0.0 --max_bb 36 --min_bb 10 \
+                --speech_conf_th 1.0 --max_frames 64 --min_frames 10 \
+                --train_batch_size ${train_batch_size} --inf_batch_size ${inf_batch_size} \
+                --num_train_steps ${num_train_steps} --valid_steps ${valid_steps}  \
+                --output_dir /data7/emobert/exp/evaluation/MSP/finetune/nomask-taskpretrain-movies_v1v2v3_uniter3m_4tasks_melm_sentiword-lr${lr}_train${num_train_steps}_trnval
+        done
+done
+
+
